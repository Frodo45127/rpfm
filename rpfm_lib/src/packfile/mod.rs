//---------------------------------------------------------------------------//
// Copyright (c) 2017-2020 Ismael Gutiérrez González. All rights reserved.
//
// This file is part of the Rusted PackFile Manager (RPFM) project,
// which can be found here: https://github.com/Frodo45127/rpfm.
//
// This file is licensed under the MIT license, which can be found here:
// https://github.com/Frodo45127/rpfm/blob/master/LICENSE.
//---------------------------------------------------------------------------//

/*!
Module with all the code to interact with PackFiles.

This module contains all the code related with PackFiles. If you want to do anything with a PackFile,
this is the place you have to come.

Also, something to take into account. RPFM supports PackFile compression/decompression and decryption,
and that is handled automagically by RPFM. All the data you'll ever see will be decompressed/decrypted,
so you don't have to worry about that.
!*/

use bitflags::bitflags;
use csv::ReaderBuilder;
use itertools::{Itertools, Either};
use serde_derive::{Serialize, Deserialize};
use rayon::prelude::*;

use std::{fmt, fmt::Display};
use std::fs::{DirBuilder, File};
use std::io::{prelude::*, BufReader, BufWriter, SeekFrom, Read, Write};
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};

use rpfm_error::{Error, ErrorKind, Result};

use crate::GAME_SELECTED;
use crate::DEPENDENCY_DATABASE;
use crate::FAKE_DEPENDENCY_DATABASE;
use crate::SCHEMA;
use crate::SETTINGS;
use crate::SUPPORTED_GAMES;
use crate::common::{*, decoder::Decoder, encoder::Encoder};
use crate::packfile::compression::*;
use crate::packfile::crypto::*;
use crate::packfile::packedfile::*;
use crate::packedfile::{DecodedPackedFile, PackedFileType};
use crate::packedfile::table::DecodedData;
use crate::packedfile::table::db::DB;
use crate::packedfile::table::loc::{Loc, TSV_NAME_LOC};

mod compression;
mod crypto;
pub mod packedfile;

#[cfg(test)]
mod packfile_test;

/// These consts are used for dealing with Time-related operations.
const WINDOWS_TICK: i64 = 10_000_000;
const SEC_TO_UNIX_EPOCH: i64 = 11_644_473_600;

/// These are the different Preamble/Id the PackFiles can have.
const PFH5_PREAMBLE: &str = "PFH5"; // PFH5
const PFH4_PREAMBLE: &str = "PFH4"; // PFH4
const PFH3_PREAMBLE: &str = "PFH3"; // PFH3
const PFH2_PREAMBLE: &str = "PFH2"; // PFH2
const PFH0_PREAMBLE: &str = "PFH0"; // PFH0

/// This one is the path in a `PackFile` where all the maps generated by Terry end up.
const TERRY_MAP_PATH: [&str; 4] = ["terrain", "tiles", "battle", "_assembly_kit"];

/// This one is the name of the main BMD data file used by maps exported from Terry.
const DEFAULT_BMD_DATA: &str = "bmd_data.bin";

/// These three hints are neccesary for the map patching function.
const FORT_PERIMETER_HINT: &[u8; 18] = b"AIH_FORT_PERIMETER";
const DEFENSIVE_HILL_HINT: &[u8; 18] = b"AIH_DEFENSIVE_HILL";
const SIEGE_AREA_NODE_HINT: &[u8; 19] = b"AIH_SIEGE_AREA_NODE";

/// This is the list of ***Reserved PackedFile Names***. They're packedfile names used by RPFM for special porpouses.
pub const RESERVED_PACKED_FILE_NAMES: [&str; 3] = ["extra_packfile.rpfm_reserved", "settings.rpfm_reserved", "notes.rpfm_reserved"];

/// These are the types the PackFiles can have.
const FILE_TYPE_BOOT: u32 = 0;
const FILE_TYPE_RELEASE: u32 = 1;
const FILE_TYPE_PATCH: u32 = 2;
const FILE_TYPE_MOD: u32 = 3;
const FILE_TYPE_MOVIE: u32 = 4;
bitflags! {

    /// This represents the bitmasks a PackFile can have applied to his type.
    ///
    /// Keep in mind that this lib supports decoding PackFiles with any of these flags enabled,
    /// but it only supports enconding for the `HAS_INDEX_WITH_TIMESTAMPS` flag.
    pub struct PFHFlags: u32 {

        /// Used to specify that the header of the PackFile is extended by 20 bytes. Used in Arena.
        const HAS_EXTENDED_HEADER       = 0b0000_0001_0000_0000;

        /// Used to specify that the PackedFile Index is encrypted. Used in Arena.
        const HAS_ENCRYPTED_INDEX       = 0b0000_0000_1000_0000;

        /// Used to specify that the PackedFile Index contains a timestamp of every PackFile.
        const HAS_INDEX_WITH_TIMESTAMPS = 0b0000_0000_0100_0000;

        /// Used to specify that the PackedFile's data is encrypted. Seen in `music.pack` PackFiles and in Arena.
        const HAS_ENCRYPTED_DATA        = 0b0000_0000_0001_0000;
    }
}

//---------------------------------------------------------------------------//
//                              Enum & Structs
//---------------------------------------------------------------------------//

/// This `Struct` stores the data of the PackFile in memory, along with some extra data needed to manipulate the PackFile.
#[derive(Debug, Clone, PartialEq)]
pub struct PackFile {

    /// The path of the PackFile on disk, if exists. If not, then this should be empty.
    file_path: PathBuf,

    /// The version of the PackFile.
    pfh_version: PFHVersion,

    /// The type of the PackFile.
    pfh_file_type: PFHFileType,

    /// The bitmasks applied to the PackFile.
    bitmask: PFHFlags,

    /// The timestamp of the last time the PackFile was saved.
    timestamp: i64,

    /// The list of PackFiles this PackFile requires to be loaded before himself when starting the game.
    ///
    /// In other places, we refer to this as the `Dependency List`.
    pack_files: Vec<String>,

    /// The list of PackedFiles this PackFile contains.
    packed_files: Vec<PackedFile>,

    /// Notes added to the PackFile. Exclusive of this lib.
    notes: Option<String>,
}

/// This struct is a reduced version of the `PackFile` one, used to pass just the needed data to an UI.
///
/// Don't create this one manually. Get it `From` the `PackFile` one, and use it as you need it.
#[derive(Debug)]
pub struct PackFileInfo {

    /// The name of the PackFile's file, if exists. If not, then this should be empty.
    pub file_name: String,

    /// The path of the PackFile on disk, if exists. If not, then this should be empty.
    pub file_path: PathBuf,

    /// The version of the PackFile.
    pub pfh_version: PFHVersion,

    /// The type of the PackFile.
    pub pfh_file_type: PFHFileType,

    /// The bitmasks applied to the PackFile.
    pub bitmask: PFHFlags,

    /// The current state of the compression inside the PackFile.
    pub compression_state: CompressionState,

    /// The timestamp of the last time the PackFile was saved.
    pub timestamp: i64,
}

/// This struct represents the entire **Manifest.txt** from the /data folder.
///
/// Private for now, because I see no public use for this.
#[derive(Debug, Serialize, Deserialize)]
struct Manifest(Vec<ManifestEntry>);

/// This struct represents a Manifest Entry.
#[derive(Debug, Serialize, Deserialize)]
struct ManifestEntry {

    /// The path of the file, relative to /data.
    relative_path: String,

    /// The size in bytes of the file.
    size: u64,

    /// If the file comes with the base game (1), or with one of its dlc (0).
    belongs_to_base_game: u8,
}

/// This enum represents the **Version** of a PackFile.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum PFHVersion {

    /// Used in Warhammer 2, Three Kingdoms and Arena.
    PFH5,

    /// Used in Warhammer 1, Attila, Rome 2, and Thrones of Brittania.
    PFH4,

    /// Used in Shogun 2.
    PFH3,

    /// Also used in Shogun 2.
    PFH2,

    /// Used in Napoleon and Empire.
    PFH0
}

/// This enum represents the **Type** of a PackFile.
///
/// The types here are sorted in the same order they'll load when the game starts.
/// The number in their docs is their numeric value when read from a PackFile.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum PFHFileType {

    /// **(0)**: Used in CA PackFiles, not useful for modding.
    Boot,

    /// **(1)**: Used in CA PackFiles, not useful for modding.
    Release,

    /// **(2)**: Used in CA PackFiles, not useful for modding.
    Patch,

    /// **(3)**: Used for mods. PackFiles of this type are only loaded in the game if they are enabled in the Mod Manager/Launcher.
    Mod,

    /// **(4)** Used in CA PackFiles and for some special mods. Unlike `Mod` PackFiles, these ones always get loaded.
    Movie,

    /// Wildcard for any type that doesn't fit in any of the other categories. The type's value is stored in the Variant.
    Other(u32),
}

/// This enum represents the type of a path in a PackFile.
///
/// Keep in mind that, in the lib we don't have a reliable way to determine if a path is a file or a folder if their path conflicts.
/// For example, if we have the folder "x/y/z/" and the file "x/y/z", and we ask the lib what type of path it's, we'll default to a file.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum PathType {

    /// Used for PackedFile paths. Contains the path of the PackedFile.
    File(Vec<String>),

    /// Used for folder paths. Contains the path of the folder.
    Folder(Vec<String>),

    /// Used for the PackFile itself.
    PackFile,

    /// Used for any other situation. Usually, if this is used, there is a detection problem somewhere else.
    None,
}

/// This enum indicates the current state of the compression in the current PackFile.
///
/// Despite compression being per-packedfile, we only support applying it to the full PackFile for now.
/// Also, compression is only supported by `PFHVersion::PFH5` PackFiles.
#[derive(Debug, Clone, PartialEq)]
pub enum CompressionState {

    /// All the PackedFiles in the PackFile are compressed.
    Enabled,

    /// Some of the PackedFiles in the PackFile are compressed.
    Partial,

    /// None of the files in the PackFile are compressed.
    Disabled,
}

//---------------------------------------------------------------------------//
//                             Enum Implementations
//---------------------------------------------------------------------------//

/// Implementation of `PFHFileType`.
impl PFHFileType {

    /// This function returns the PackFile's **Type** in `u32` format. To know what value corresponds with what type, check their definition's comment.
    pub fn get_value(self) -> u32 {
        match self {
            PFHFileType::Boot => FILE_TYPE_BOOT,
            PFHFileType::Release => FILE_TYPE_RELEASE,
            PFHFileType::Patch => FILE_TYPE_PATCH,
            PFHFileType::Mod => FILE_TYPE_MOD,
            PFHFileType::Movie => FILE_TYPE_MOVIE,
            PFHFileType::Other(value) => value
        }
    }

    /// This function returns the PackFile's Type corresponding to the provided value.
    pub fn get_type(value: u32) -> Self {
        match value {
            FILE_TYPE_BOOT => PFHFileType::Boot,
            FILE_TYPE_RELEASE => PFHFileType::Release,
            FILE_TYPE_PATCH => PFHFileType::Patch,
            FILE_TYPE_MOD => PFHFileType::Mod,
            FILE_TYPE_MOVIE => PFHFileType::Movie,
            _ => PFHFileType::Other(value),
        }
    }
}

/// Display implementation of `PFHFileType`.
impl Display for PFHFileType {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match self {
            PFHFileType::Boot => write!(f, "Boot"),
            PFHFileType::Release => write!(f, "Release"),
            PFHFileType::Patch => write!(f, "Patch"),
            PFHFileType::Mod => write!(f, "Mod"),
            PFHFileType::Movie => write!(f, "Movie"),
            PFHFileType::Other(version) => write!(f, "Other: {}", version),
        }
    }
}

/// Implementation of `PFHVersion`.
impl PFHVersion {

    /// This function returns the PackFile's *Id/Preamble* (his 4 first bytes) as a `&str`.
    pub fn get_value(&self) -> &str {
        match *self {
            PFHVersion::PFH5 => PFH5_PREAMBLE,
            PFHVersion::PFH4 => PFH4_PREAMBLE,
            PFHVersion::PFH3 => PFH3_PREAMBLE,
            PFHVersion::PFH2 => PFH2_PREAMBLE,
            PFHVersion::PFH0 => PFH0_PREAMBLE,
        }
    }

    /// This function returns the PackFile's `PFHVersion` corresponding to the provided value, or an error if the provided value is not a valid `PFHVersion`.
    pub fn get_version(value: &str) -> Result<Self> {
        match value {
            PFH5_PREAMBLE => Ok(PFHVersion::PFH5),
            PFH4_PREAMBLE => Ok(PFHVersion::PFH4),
            PFH3_PREAMBLE => Ok(PFHVersion::PFH3),
            PFH2_PREAMBLE => Ok(PFHVersion::PFH2),
            PFH0_PREAMBLE => Ok(PFHVersion::PFH0),
            _ => Err(ErrorKind::PackFileIsNotAPackFile.into()),
        }
    }
}

/// Display implementation of `PFHVersion`.
impl Display for PFHVersion {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        match self {
            PFHVersion::PFH5 => write!(f, "PFH5"),
            PFHVersion::PFH4 => write!(f, "PFH4"),
            PFHVersion::PFH3 => write!(f, "PFH3"),
            PFHVersion::PFH2 => write!(f, "PFH2"),
            PFHVersion::PFH0 => write!(f, "PFH0"),
        }
    }
}

/// Implementation of `PathType`.
impl PathType {

    /// This function removes collisioned items from the provided list of `PathType`.
    ///
    /// This means, if you have an item of type `PackFile` it removes the rest of the items.
    /// If you have a file and a folder containing the file, it removes the file. And so on.
    ///
    /// NOTE: It DOES NOT remove duplicated PathTypes. This means, if you pass it a slice with the same PathType duplicated,
    /// it'll give it you back as it came. This is for removing collisioned PathTypes, not duplicated ones!!!!
    pub fn dedup(path_types: &[Self]) -> Vec<Self> {
        let mut path_types = path_types.to_vec();

        // As this operation can get very expensive very fast, we first check if we have a PackFile.
        // If so, we just leave the PackFile and remove everything else.
        let we_have_packfile = path_types.par_iter().find_any(|item| {
            if let PathType::PackFile = item { true } else { false }
        });

        match we_have_packfile {
            Some(path_type) => path_types = vec![path_type.clone(); 1],

            // If we don't have a PackFile, we check the rest of the items (files and folders).
            None => {
                let we_have_folder = path_types.par_iter().any(|item| {
                    if let PathType::Folder(_) = item { true } else { false }
                });

                // If we don't have a folder, we assume the entire set are unique files.
                // Otherwise, we proceed to do the expensive dedup operation.
                if we_have_folder {
                    let items_to_remove = path_types.par_iter().filter(|item_type_to_add| {
                        match item_type_to_add {

                            // If it's a file, we have to check if there is a folder containing it.
                            PathType::File(ref path_to_add) => {
                                path_types.par_iter().filter(|x| {
                                    if let PathType::File(_) = x { false } else { true }
                                }).any(|item_type| {

                                    // If the other one is a folder that contains it, dont add it.
                                    if let PathType::Folder(ref path) = item_type {
                                        path_to_add.starts_with(path)
                                    } else { false }
                                })
                            }

                            // If it's a folder, we have to check if there is already another folder containing it.
                            PathType::Folder(ref path_to_add) => {
                                path_types.par_iter().filter(|x| {
                                    if let PathType::File(_) = x { false } else { true }
                                }).any(|item_type| {

                                    // If the other one is a folder that contains it, dont add it.
                                    if let PathType::Folder(ref path) = item_type {
                                        path_to_add.starts_with(path) && path_to_add.len() > path.len()
                                    } else { false }
                                })
                            }

                            // If we receive one of these... better start praying.
                            _ => unimplemented!(),
                        }
                    }).cloned().collect::<Vec<PathType>>();

                    // Remove the duplicated items.
                    items_to_remove.iter().for_each(|x| {
                        let index = path_types.iter().position(|y| x == y).unwrap();
                        path_types.remove(index);
                    });
                }
            }
        }

        path_types
    }
}

//---------------------------------------------------------------------------//
//                           Structs Implementations
//---------------------------------------------------------------------------//

/// Implementation of `PackFile`.
impl PackFile {

    /// This function creates a new empty `PackFile`. This is used for creating a *dummy* PackFile we'll later populate.
    pub fn new() -> Self {
        Self {
            file_path: PathBuf::new(),
            pfh_version: PFHVersion::PFH5,
            pfh_file_type: PFHFileType::Mod,
            bitmask: PFHFlags::empty(),
            timestamp: 0,

            pack_files: vec![],
            packed_files: vec![],

            notes: None
        }
    }

    /// This function creates a new empty `PackFile` with a name and a specific `PFHVersion`.
    pub fn new_with_name(file_name: &str, pfh_version: PFHVersion) -> Self {
        Self {
            file_path: PathBuf::from(file_name),
            pfh_version,
            pfh_file_type: PFHFileType::Mod,
            bitmask: PFHFlags::empty(),
            timestamp: 0,

            pack_files: vec![],
            packed_files: vec![],

            notes: None,
        }
    }

    /// This function returns a list of reserved PackedFile names, used by RPFM for special porpouses.
    pub fn get_reserved_packed_file_names() -> Vec<Vec<String>> {
        RESERVED_PACKED_FILE_NAMES.iter().map(|x| vec![(*x).to_string()]).collect()
    }

    /// This function returns the path where maps end up after being processed by Terry and put in a `PackFile`.
    pub fn get_terry_map_path() -> Vec<String> {
        TERRY_MAP_PATH.iter().map(|x| (*x).to_string()).collect()
    }

    /// This function returns the `PackFile List` of the provided `PackFile`.
    pub fn get_packfiles_list(&self) -> &[String] {
        &self.pack_files
    }

    /// This function replaces the `PackFile List` of our `PackFile` with the provided one.
    pub fn set_packfiles_list(&mut self, pack_files: &[String]) {
        self.pack_files = pack_files.to_vec();
    }

    /// This function retuns the list of PackedFiles inside a `PackFile`.
    pub fn get_packedfiles_list(&self) -> Vec<Vec<String>> {
        self.packed_files.par_iter().map(|x| x.get_path().to_vec()).collect()
    }

    /// This function adds a `PackedFile` to an existing `PackFile`.
    ///
    /// This function returns the path of the `PackedFile` which got added succesfully. Also, if you set `overwrite` to `true`,
    /// in case of conflict, the `PackedFile` is overwritten. If set to false, it'll be renamed instead.
    ///
    /// This is a convenience function to add just one PackedFile to our PackFile.
    pub fn add_packed_file(&mut self, packed_file: &PackedFile, overwrite: bool) -> Result<Vec<String>> {
        self.add_packed_files(&[packed_file], overwrite).map(|x| x[0].to_vec())
    }

    /// This function adds one or more `PackedFiles` to an existing `PackFile`.
    ///
    /// This function returns the paths of the `PackedFiles` which got added succesfully, which should be all. Also, if you set `overwrite` to `true`,
    /// in case of conflict the destination `PackedFiles`, if exists, are overwritten. If set to false, they'll be renamed instead.
    ///
    /// NOTE: This assumes the paths of the list of PackedFiles you pass it are unique among themselfs. It'll do weird things otherwise.
    pub fn add_packed_files(&mut self, packed_files: &[&PackedFile], overwrite: bool) -> Result<Vec<Vec<String>>> {

        // If we hit a reserved name, stop. Don't add anything.
        let pack_file_name = self.get_file_name();
        let reserved_names = Self::get_reserved_packed_file_names();
        if packed_files.par_iter().any(|x| reserved_names.iter().any(|y| x.get_path() == &**y)) { return Err(ErrorKind::ReservedFiles.into()) }

        // Prepare the list of added paths and get all the PackedFiles with all the info needed for them to be added.
        let mut destination_paths = Vec::with_capacity(packed_files.len());
        let packed_files = packed_files.par_iter()
            .map(|x| (x.get_path(), *x,
                self.packed_files.par_iter().position_any(|y| x.get_path() == y.get_path())
            )).collect::<Vec<(&[String], &PackedFile, Option<usize>)>>();

        // Get all the PackedFiles that are not in conflict with the ones we already have in our PackFile,
        // prepare them to be added, then add them all at once.
        let mut packed_files_new = packed_files.par_iter()
            .filter(|(_, _, position)| position.is_none())
            .map(|(_, packed_file, _)| {
                let mut packed_file = (*packed_file).clone();
                packed_file.get_ref_mut_raw().set_packfile_name(&pack_file_name);
                packed_file
            })
            .collect::<Vec<PackedFile>>();

        // Prepare the paths of the no-conflict PackedFiles to be returned.
        destination_paths.append(&mut packed_files_new.par_iter()
            .map(|packed_file| packed_file.get_path().to_vec())
            .collect::<Vec<Vec<String>>>());
        self.packed_files.append(&mut packed_files_new);


        // Now we deal with the problematic ones. If we set them to overwrite the conflicting files...we just replace them.
        if overwrite {
            let packed_files_conflict = packed_files.par_iter()
                .filter(|(_, _, position)| position.is_some())
                .map(|(_, packed_file, position)| (*packed_file, position.unwrap()))
                .collect::<Vec<(&PackedFile, usize)>>();

            packed_files_conflict.iter().for_each(|(packed_file, position)| {
                self.packed_files[*position] = (*packed_file).clone();
            });

            destination_paths.append(&mut packed_files_conflict.par_iter()
                .map(|(packed_file, _)| packed_file.get_path().to_vec())
                .collect::<Vec<Vec<String>>>());
        }

        // If not, then we need to do some path testing to rename them to something that doesn't match any other PackedFile we have.
        else {
            let mut packed_files_conflict = packed_files.par_iter()
                .filter(|(_, _, position)| position.is_some())
                .map(|(path, packed_file, _)| {
                    let mut path = path.to_vec();
                    let mut packed_file = (*packed_file).clone();
                    packed_file.get_ref_mut_raw().set_packfile_name(&pack_file_name);

                    let name_current = path.last().unwrap().to_owned();
                    let name_splitted = name_current.split('.').collect::<Vec<&str>>();
                    let name = name_splitted[0];
                    let extension = if name_splitted.len() > 1 { name_splitted[1..].join(".") } else { "".to_owned() };
                    for number in 0.. {
                        let name = if extension.is_empty() { format!("{}_{}", name, number) } else { format!("{}_{}.{}", name, number, extension) };
                        *path.last_mut().unwrap() = name;
                        if !self.packedfile_exists(&path) && !reserved_names.contains(&path) {

                            // Ignorable result. This will never fail due to the replacing code before this.
                            let _ = packed_file.get_ref_mut_raw().set_path(&path);
                            break;
                        }
                    }
                    packed_file
                })
                .collect::<Vec<PackedFile>>();

            destination_paths.append(&mut packed_files_conflict.par_iter()
                .map(|packed_file| packed_file.get_path().to_vec())
                .collect::<Vec<Vec<String>>>());
            self.packed_files.append(&mut packed_files_conflict);
        }

        Ok(destination_paths)
    }

    /// This function is used to add a file from disk to a `PackFile`, turning it into a `PackedFile`.
    ///
    /// In case of conflict, if overwrite is set to true, the current `PackedFile` in the conflicting path
    /// will be overwritten with the new one. If set to false, the new `PackFile` will be called `xxxx_1.extension`.
    pub fn add_from_file(
        &mut self,
        path_as_file: &PathBuf,
        path_as_packed_file: Vec<String>,
        overwrite: bool,
    ) -> Result<Vec<String>> {
        let raw_data = RawPackedFile::read_from_path(path_as_file, path_as_packed_file)?;
        let packed_file = PackedFile::new_from_raw(&raw_data);
        self.add_packed_file(&packed_file, overwrite)
    }

    /// This function is used to add one or more files from disk to a `PackFile`, turning them into `PackedFiles`.
    ///
    /// In case of conflict, if overwrite is set to true, the current `PackedFile` in the conflicting path
    /// will be overwritten with the new one. If set to false, the new `PackFile` will be called `xxxx_1.extension`.
    pub fn add_from_files(
        &mut self,
        paths_as_file_and_packed_file: &[(PathBuf, Vec<String>)],
        overwrite: bool,
    ) -> Result<Vec<Vec<String>>> {
        let mut packed_files = vec![];
        for (path_as_file, path_as_packed_file) in paths_as_file_and_packed_file.iter() {
            let raw_data = RawPackedFile::read_from_path(path_as_file, path_as_packed_file.to_vec())?;
            packed_files.push(PackedFile::new_from_raw(&raw_data));
        }
        let ref_packed_files = packed_files.iter().map(|x| x).collect::<Vec<&PackedFile>>();
        self.add_packed_files(&ref_packed_files, overwrite)
    }

    /// This function is used to add multiple folders from disk to a `PackFile`, turning their files into `PackedFiles`.
    ///
    /// In case of conflict, if overwrite is set to true, the current `PackedFile` in the conflicting path
    /// will be overwritten with the new one. If set to false, the new `PackFile` will be called `xxxx_1.extension`.
    pub fn add_from_folders(
        &mut self,
        paths_as_folder_and_destination: &[(PathBuf, Vec<String>)],
        overwrite: bool,
    ) -> Result<Vec<Vec<String>>> {

        let mut packed_files_to_add = vec![];
        for (path, base_path) in paths_as_folder_and_destination {
            match get_files_from_subdir(path) {
                Ok(file_paths) => {
                    for file_path in &file_paths {

                        // The stupid C: letter in paths causes problems when we're on windows.
                        let drain_fix = if cfg!(target_os = "windows") { 1 } else { 0 };
                        let new_path_filtered = file_path.to_string_lossy()
                            .replace('\\', "/") // Fix for windows paths.
                            .split('/')
                            .collect::<Vec<&str>>()
                            .drain(path.components().count() - 1 - drain_fix..)
                            .map(|x| x.to_owned())
                            .collect::<Vec<String>>();
                        let mut new_path = base_path.to_vec();
                        new_path.extend_from_slice(&new_path_filtered);
                        let raw_data = RawPackedFile::read_from_path(file_path, new_path)?;
                        let packed_file = PackedFile::new_from_raw(&raw_data);
                        packed_files_to_add.push(packed_file);
                    }
                }
                Err(error) => return Err(error)
            }
        }

        self.add_packed_files(&packed_files_to_add.iter().map(|x|x).collect::<Vec<&PackedFile>>(), overwrite)
    }

    /// This function is used to add a `PackedFile` from one `PackFile` into another.
    ///
    /// It's a ***Copy from another PackFile*** kind of function. It returns the PathTypes
    /// of whatever got added to our `PackFile`.
    pub fn add_from_packfile(
        &mut self,
        source: &Self,
        path_types: &[PathType],
        overwrite: bool,
    ) -> Result<Vec<PathType>> {

        // Keep the PathTypes added so we can return them to the UI easely.
        let paths;
        let path_types = PathType::dedup(path_types);

        // As this can get very slow very quickly, we do here some... optimizations.
        // First, we get if there are PackFiles or folders in our list of PathTypes.
        let we_have_packfile = path_types.par_iter().any(|item| {
            if let PathType::PackFile = item { true } else { false }
        });

        let we_have_folder = path_types.par_iter().any(|item| {
            if let PathType::Folder(_) = item { true } else { false }
        });

        // Then, if we have a PackFile,... just import all PackedFiles.
        if we_have_packfile {
            let packed_files = source.get_ref_packed_files_all();
            paths = self.add_packed_files(&packed_files, overwrite)?;
        }

        // If we only have files, get all the files we have at once, then add them all together.
        else if !we_have_folder {
            let paths_files = path_types.par_iter().filter_map(|x| {
                if let PathType::File(path) = x { Some(&**path) } else { None }
            }).collect::<Vec<&[String]>>();
            let packed_files = source.get_ref_packed_files_by_paths(paths_files);
            paths = self.add_packed_files(&packed_files, overwrite)?;
        }

        // Otherwise, we have a mix of Files and Folders (or folders only).
        // In this case, we get all the individual files, then the ones inside folders.
        // Then we merge them, and add all of them together.
        else {
            let paths_files = path_types.par_iter().filter_map(|x| {
                if let PathType::File(path) = x { Some(&**path) } else { None }
            }).collect::<Vec<&[String]>>();
            let mut packed_files = source.get_ref_packed_files_by_paths(paths_files);

            packed_files.append(&mut path_types.par_iter().filter_map(|x| {
                if let PathType::Folder(path) = x { Some(&**path) } else { None }
            }).map(|path| source.get_ref_packed_files_by_path_start(path))
            .flatten()
            .collect::<Vec<&PackedFile>>());
            paths = self.add_packed_files(&packed_files, overwrite)?;
        }

        Ok(paths.par_iter().map(|x| PathType::File(x.to_vec())).collect::<Vec<PathType>>())
    }

    /// This function returns the name of the PackFile. If it's empty, it's an in-memory only PackFile.
    pub fn get_file_name(&self) -> String {
        match self.file_path.file_name() {
            Some(s) => s.to_string_lossy().to_string(),
            None => String::new()
        }
    }

    /// This function returns the path of the PackFile. If it's empty, it's an in-memory only PackFile.
    pub fn get_file_path(&self) -> &PathBuf {
        &self.file_path
    }

    /// This function changes the path of the PackFile.
    ///
    /// This can fail if you pass it an empty path.
    pub fn set_file_path(&mut self, path: &Path) -> Result<()> {
        if path.components().count() == 0 { return Err(ErrorKind::EmptyInput.into()) }
        self.file_path = path.to_path_buf();

        // We have to change the name of the PackFile in all his `PackedFiles` too.
        let file_name = self.get_file_name();
        self.packed_files.iter_mut().for_each(|x| x.get_ref_mut_raw().set_packfile_name(&file_name));
        Ok(())
    }

    /// This function returns the current compression state of the provided `PackFile`.
    ///
    /// To get more info about the different compression states, check the `CompressionState` enum.
    pub fn get_compression_state(&self) -> CompressionState {
        let mut has_files_compressed = false;
        let mut has_files_uncompressed = false;
        for packed_file in &self.packed_files {
            let is_compressed = packed_file.get_ref_raw().get_compression_state();
            if !has_files_compressed && is_compressed {
                has_files_compressed = true;
            }

            if !has_files_uncompressed && !is_compressed {
                has_files_uncompressed = true;
            }

            if has_files_uncompressed && has_files_compressed {
                break;
            }
        }
        if has_files_compressed && has_files_uncompressed { CompressionState::Partial }
        else if has_files_compressed { CompressionState::Enabled }
        else { CompressionState::Disabled }
    }

    /// This function returns if the `PackFile` is editable or not.
    ///
    /// By *if is editable or not* I mean *If you can save it or not*. The conditions under which a PackFile is not editable are:
    /// - All PackFiles with extended header or encrypted parts are not editable.
    /// - All PackFiles of type `Mod` or `Movie` are editable.
    /// - If you say CA PackFiles are not editable:
    ///   - All PackFiles of type `Boot`, `Release` or `Patch` are not editable.
    /// - If you say CA PackFiles are editable:
    ///   - All PackFiles of type `Boot`, `Release` or `Patch` are editable.
    pub fn is_editable(&self, is_editing_of_ca_packfiles_allowed: bool) -> bool {

        // If it's this very specific type, don't save under any circunstance.
        if let PFHFileType::Other(_) = self.pfh_file_type { false }

        // If ANY of these bitmask is detected in the PackFile, disable all saving.
        else if self.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) ||
            self.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) ||
            self.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) { false }
        else {
            self.pfh_file_type == PFHFileType::Mod ||
            self.pfh_file_type == PFHFileType::Movie ||
            (is_editing_of_ca_packfiles_allowed && self.pfh_file_type.get_value() <= 2)
        }
    }

    /// This function returns a copy to the `PackedFile` with the provided path, if exists.
    pub fn get_packed_file_by_path(&self, path: &[String]) -> Option<PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path() == path).cloned().find_any(|x| x.get_path() == path)
    }

    /// This function returns a reference to the `PackedFile` with the provided path, if exists.
    pub fn get_ref_packed_file_by_path(&self, path: &[String]) -> Option<&PackedFile> {
        self.packed_files.par_iter().find_any(|x| x.get_path() == path)
    }

    /// This function returns a mutable reference to the `PackedFile` with the provided path, if exists.
    pub fn get_ref_mut_packed_file_by_path(&mut self, path: &[String]) -> Option<&mut PackedFile> {
        self.packed_files.par_iter_mut().find_any(|x| x.get_path() == path)
    }

    /// This function returns a copy of all the `PackedFiles` in the provided paths.
    pub fn get_packed_files_by_paths(&self, paths: Vec<&[String]>) -> Vec<PackedFile> {
        self.packed_files.par_iter().filter(|x| paths.contains(&x.get_path())).cloned().collect()
    }

    /// This function returns a reference of all the `PackedFiles` in the provided paths.
    pub fn get_ref_packed_files_by_paths(&self, paths: Vec<&[String]>) -> Vec<&PackedFile> {
        self.packed_files.par_iter().filter(|x| paths.contains(&x.get_path())).collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` in the provided paths.
    pub fn get_ref_mut_packed_files_by_paths(&mut self, paths: Vec<&[String]>) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().filter(|x| paths.contains(&x.get_path())).collect()
    }

    /// This function returns a copy of all the `PackedFiles` starting with the provided path.
    pub fn get_packed_files_by_path_start(&self, path: &[String]) -> Vec<PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().starts_with(path) && !path.is_empty() && x.get_path().len() > path.len()).cloned().collect()
    }

    /// This function returns a reference of all the `PackedFiles` starting with the provided path.
    pub fn get_ref_packed_files_by_path_start(&self, path: &[String]) -> Vec<&PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().starts_with(path) && !path.is_empty() && x.get_path().len() > path.len()).collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` starting with the provided path.
    pub fn get_ref_mut_packed_files_by_path_start(&mut self, path: &[String]) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().filter(|x| x.get_path().starts_with(path) && !path.is_empty() && x.get_path().len() > path.len()).collect()
    }

    /// This function returns a copy of the paths of all the `PackedFiles` in the provided `PackFile` under the provided path.
    pub fn get_packed_files_paths_by_path_start(&self, path: &[String]) -> Vec<Vec<String>> {
        self.packed_files.par_iter().map(|x| x.get_path()).filter(|x| x.starts_with(path) && !path.is_empty() && x.len() > path.len()).map(|x| x.to_vec()).collect()
    }

    /// This function returns a reference of the paths of all the `PackedFiles` in the provided `PackFile` under the provided path.
    pub fn get_ref_packed_files_paths_by_path_start(&self, path: &[String]) -> Vec<&[String]> {
        self.packed_files.par_iter().map(|x| x.get_path()).filter(|x| x.starts_with(path) && !path.is_empty() && x.len() > path.len()).collect()
    }

    /// This function returns a copy of all the `PackedFiles` ending with the provided path.
    pub fn get_packed_files_by_path_end(&self, path: &[String]) -> Vec<PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().ends_with(path) && !path.is_empty()).cloned().collect()
    }

    /// This function returns a reference of all the `PackedFiles` ending with the provided path.
    pub fn get_ref_packed_files_by_path_end(&self, path: &[String]) -> Vec<&PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().ends_with(path) && !path.is_empty()).collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` ending with the provided path.
    pub fn get_ref_mut_packed_files_by_path_end(&mut self, path: &[String]) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().filter(|x| x.get_path().ends_with(path) && !path.is_empty()).collect()
    }

    /// This function returns a copy of all the `PackedFiles` ending with the provided extension.
    pub fn get_packed_files_by_extension(&self, extension: &str) -> Vec<PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().last().unwrap().ends_with(extension) && !extension.is_empty()).cloned().collect()
    }

    /// This function returns a reference of all the `PackedFiles` ending with the provided extension.
    pub fn get_ref_packed_files_by_extension(&self, extension: &str) -> Vec<&PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().last().unwrap().ends_with(extension) && !extension.is_empty()).collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` ending with the provided extension.
    pub fn get_ref_mut_packed_files_by_extension(&mut self, extension: &str) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().filter(|x| x.get_path().last().unwrap().ends_with(extension) && !extension.is_empty()).collect()
    }

    /// This function returns a copy of all the PackedFiles in the current PackFile of the provided type.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not garantee the provided PackedFiles are of the type. Just that they `match` the type.
    pub fn get_packed_files_by_type(&self, packed_file_type: PackedFileType, strict_match_mode: bool) -> Vec<PackedFile> {
        self.packed_files.par_iter()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_path());
                if strict_match_mode { y == packed_file_type } else { y.eq_non_strict(packed_file_type) }
            }).cloned().collect()
    }

    /// This function returns a reference of all the PackedFiles in the current PackFile of the provided type.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not garantee the provided PackedFiles are of the type. Just that they `match` the type.
    pub fn get_ref_packed_files_by_type(&self, packed_file_type: PackedFileType, strict_match_mode: bool) -> Vec<&PackedFile> {
        self.packed_files.par_iter()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_path());
                if strict_match_mode { y == packed_file_type } else { y.eq_non_strict(packed_file_type) }
            }).collect()
    }

    /// This function returns a mutable reference of all the PackedFiles in the current PackFile of the provided type.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not garantee the provided PackedFiles are of the type. Just that they `match` the type.
    pub fn get_ref_mut_packed_files_by_type(&mut self, packed_file_type: PackedFileType, strict_match_mode: bool) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_path());
                if strict_match_mode { y == packed_file_type } else { y.eq_non_strict(packed_file_type) }
            }).collect()
    }

    /// This function returns a copy of all the PackedFiles in the current PackFile of the provided types.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not garantee the provided PackedFiles are of the type. Just that they `match` one of the types.
    pub fn get_packed_files_by_types(&mut self, packed_file_types: &[PackedFileType], strict_match_mode: bool) -> Vec<PackedFile> {
        self.packed_files.par_iter()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_path());
                if strict_match_mode { packed_file_types.contains(&y) } else { y.eq_non_strict_slice(packed_file_types) }
            }).cloned().collect()
    }

    /// This function returns a reference of all the PackedFiles in the current PackFile of the provided types.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not garantee the provided PackedFiles are of the type. Just that they `match` one of the types.
    pub fn get_ref_packed_files_by_types(&mut self, packed_file_types: &[PackedFileType], strict_match_mode: bool) -> Vec<&PackedFile> {
        self.packed_files.par_iter()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_path());
                if strict_match_mode { packed_file_types.contains(&y) } else { y.eq_non_strict_slice(packed_file_types) }
            }).collect()
    }

    /// This function returns a mutable reference of all the PackedFiles in the current PackFile of the provided types.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not garantee the provided PackedFiles are of the type. Just that they `match` one of the types.
    pub fn get_ref_mut_packed_files_by_types(&mut self, packed_file_types: &[PackedFileType], strict_match_mode: bool) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_path());
                if strict_match_mode { packed_file_types.contains(&y) } else { y.eq_non_strict_slice(packed_file_types) }
            }).collect()
    }

    /// This function returns a copy of all `PackedFiles` in the provided `PackFile`.
    pub fn get_packed_files_all(&self) -> Vec<PackedFile> {
        self.packed_files.clone()
    }

    /// This function returns a reference of all the `PackedFiles` in the provided `PackFile`.
    pub fn get_ref_packed_files_all(&self) -> Vec<&PackedFile> {
        self.packed_files.par_iter().collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` in the provided `PackFile`.
    pub fn get_ref_mut_packed_files_all(&mut self) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().collect()
    }

    /// This function returns a copy of the paths of all the `PackedFiles` in the provided `PackFile`.
    pub fn get_packed_files_all_paths(&self) -> Vec<Vec<String>> {
        self.packed_files.par_iter().map(|x| x.get_path().to_vec()).collect()
    }

    /// This function returns a reference of the paths of all the `PackedFiles` in the provided `PackFile`.
    pub fn get_ref_packed_files_all_paths(&self) -> Vec<&[String]> {
        self.packed_files.par_iter().map(|x| x.get_path()).collect()
    }

    /// This function returns a copy of all the `PackedFileInfo` corresponding to the provided `PackFile`.
    pub fn get_packed_files_all_info(&self) -> Vec<PackedFileInfo> {
        self.packed_files.par_iter().map(From::from).collect()
    }

    /// This function returns a copy of the `PackedFileInfo` of the `Packedfile` in the provided path.
    pub fn get_packed_file_info_by_path(&self, path: &[String]) -> Option<PackedFileInfo> {
        self.packed_files.par_iter().find_first(|x| x.get_path() == path).map(From::from)
    }

    /// This function removes, if exists, a `PackedFile` with the provided path from the `PackFile`.
    pub fn remove_packed_file_by_path(&mut self, path: &[String]) {
        if let Some(position) = self.packed_files.par_iter().position_any(|x| x.get_path() == path) {
            self.packed_files.remove(position);
        }
    }

    /// This function removes, if exists, all `PackedFile` starting with the provided path from the `PackFile`.
    pub fn remove_packed_files_by_path_start(&mut self, path: &[String]) {
        let positions: Vec<usize> = self.packed_files.iter()
            .enumerate()
            .filter(|x| x.1.get_path().starts_with(path) && !path.is_empty() && x.1.get_path().len() > path.len())
            .map(|x| x.0)
            .collect();
        for position in positions.iter().rev() {
            self.packed_files.remove(*position);
        }
    }

    /// This function removes, if exists, all `PackedFile` ending with the provided path from the `PackFile`.
    pub fn remove_packed_files_by_path_end(&mut self, path: &[String]) {
        let positions: Vec<usize> = self.packed_files.iter()
            .enumerate()
            .filter(|x| x.1.get_path().ends_with(path) && !path.is_empty())
            .map(|x| x.0)
            .collect();
        for position in positions.iter().rev() {
            self.packed_files.remove(*position);
        }
    }

    /// This function removes, if exists, all `PackedFile` of the provided types from the `PackFile`.
    pub fn remove_packed_files_by_type(&mut self, item_types: &[PathType]) -> Vec<PathType> {

        // We need to "clean" the selected path list to ensure we don't pass stuff already deleted.
        let item_types_clean = PathType::dedup(item_types);

        // Now we do some bitwise magic to get what type of selection combination we have.
        let mut contents: u8 = 0;
        for item_type in &item_types_clean {
            match item_type {
                PathType::File(_) => contents |= 1,
                PathType::Folder(_) => contents |= 2,
                PathType::PackFile => contents |= 4,
                PathType::None => contents |= 8,
            }
        }

        // Then we act, depending on the combination of items.
        match contents {

            // Any combination of files and folders.
            1 | 2 | 3 => {
                for item_type in &item_types_clean {
                    match item_type {
                        PathType::File(path) => self.remove_packed_file_by_path(path),
                        PathType::Folder(path) => self.remove_packed_files_by_path_start(path),
                        _ => unreachable!(),
                    }
                }
            },

            // If the `PackFile` is selected, just delete everything.
            4 | 5 | 6 | 7 => self.remove_all_packedfiles(),

            // No paths selected, none selected, invalid path selected, or invalid value.
            0 | 8..=255 => {},
        }

        // Return the list of deleted items so the caller can have a clean list to know what was really removed from the `PackFile`.
        item_types_clean
    }

    /// This function extracts, if exists, a `PackedFile` with the provided path from the `PackFile`.
    ///
    /// The destination path is always `destination_path/packfile_name/path_to_packedfile/packed_file`.
    pub fn extract_packed_file_by_path(&mut self, path: &[String], destination_path: &Path) -> Result<()> {
        match self.get_ref_mut_packed_file_by_path(path) {
            Some(ref mut packed_file) => {

                // Save it, in case it's cached.
                packed_file.encode()?;

                // We get his internal path without his name.
                let mut internal_path = packed_file.get_path().to_vec();
                let file_name = internal_path.pop().unwrap();

                // Then, we join his internal path with his destination path, so we have his almost-full path (his final path without his name).
                // This way we can create the entire folder structure up to the file itself.
                let mut current_path = destination_path.to_path_buf().join(internal_path.iter().collect::<PathBuf>());
                DirBuilder::new().recursive(true).create(&current_path)?;

                // Finish the path and try to save the file to disk.
                current_path.push(&file_name);
                let mut file = BufWriter::new(File::create(&current_path)?);
                if file.write_all(&packed_file.get_ref_raw().get_data()?).is_err() {
                    return Err(ErrorKind::ExtractError(path.to_vec()).into());
                }
                Ok(())
            }
            None => Err(ErrorKind::PackedFileNotFound.into())
        }
    }

    /// This function extract, if exists, all `PackedFile` of the provided types from the `PackFile` to disk.
    ///
    /// As this can fail for some files, and work for others, we return `Ok(amount_files_extracted)` only if all files were extracted correctly.
    /// If any of them failed, we return `Error` with a list of the paths that failed to get extracted.
    pub fn extract_packed_files_by_type(
        &mut self,
        item_types: &[PathType],
        extracted_path: &PathBuf,
    ) -> Result<u32> {

        // These variables are here to keep track of what we have extracted and what files failed.
        let mut files_extracted = 0;
        let mut error_files = vec![];

        // We need to "clean" the selected path list to ensure we don't pass stuff already extracted.
        let item_types_clean = PathType::dedup(item_types);

        // Now we do some bitwise magic to get what type of selection combination we have.
        let mut contents: u8 = 0;
        for item_type in &item_types_clean {
            match item_type {
                PathType::File(_) => contents |= 1,
                PathType::Folder(_) => contents |= 2,
                PathType::PackFile => contents |= 4,
                PathType::None => contents |= 8,
            }
        }

        // Then we act, depending on the combination of items.
        match contents {

            // Any combination of files and folders.
            1 | 2 | 3 => {

                // For folders we check each PackedFile to see if it starts with the folder's path (it's in the folder).
                // There should be no duplicates here thanks to the filters from before.
                for item_type in &item_types_clean {
                    match item_type {

                        // For individual `PackedFiles`, we extract them one by one.
                        PathType::File(path) => {
                            match self.extract_packed_file_by_path(path, extracted_path) {
                                Ok(_) => files_extracted += 1,
                                Err(_) => error_files.push(format!("{:?}", path)),
                            }
                        },

                        PathType::Folder(path) => {
                            for packed_file in self.get_ref_mut_packed_files_by_path_start(path) {
                                match packed_file.extract_packed_file(extracted_path) {
                                    Ok(_) => files_extracted += 1,
                                    Err(_) => error_files.push(format!("{:?}", path)),
                                }
                            }
                        },

                        _ => unreachable!(),
                    }
                }
            },

            // If the `PackFile` is selected, just extract it and everything will get extracted with it.
            4 | 5 | 6 | 7 => {

                // For each PackedFile we have, just extracted in the folder we got, under the PackFile's folder.
                for packed_file in self.get_ref_mut_packed_files_all() {
                    match packed_file.extract_packed_file(extracted_path) {
                        Ok(_) => files_extracted += 1,
                        Err(_) => error_files.push(format!("{:?}", packed_file.get_path())),
                    }
                }
            },

            // No paths selected, none selected, invalid path selected, or invalid value.
            0 | 8..=255 => return Err(ErrorKind::NonExistantFile.into()),
        }

        // If there is any error in the list, report it.
        if !error_files.is_empty() {
            let error_files_string = error_files.iter().map(|x| format!("<li>{}</li>", x)).collect::<Vec<String>>();
            return Err(ErrorKind::ExtractError(error_files_string).into())
        }

        // If we reach this, return the amount of extracted files.
        Ok(files_extracted)
    }

    /// This function enables/disables compression in all `PackedFiles` inside the `PackFile`. Partial compression is not supported.
    pub fn toggle_compression(&mut self, enable: bool) {
        self.packed_files.par_iter_mut().for_each(|x| x.get_ref_mut_raw().set_should_be_compressed(enable));
    }

    /// This function returns the notes contained within the provided `PackFile`.
    pub fn get_notes(&self) -> &Option<String> {
        &self.notes
    }

    /// This function saves your notes within the provided `PackFile`.
    pub fn set_notes(&mut self, notes: &Option<String>) {
        self.notes = notes.clone();
    }

    /// This function returns the timestamp of the provided `PackFile`.
    pub fn get_timestamp(&self) -> i64 {
        self.timestamp
    }

    /// This function sets the timestamp of the provided `PackFile`.
    pub fn set_timestamp(&mut self, timestamp: i64) {
        self.timestamp = timestamp;
    }

    /// This function returns the `PFHVersion` of the provided `PackFile`.
    pub fn get_pfh_version(&self) -> PFHVersion {
        self.pfh_version
    }

    /// This function sets the `PFHVersion` of the provided `PackFile`.
    pub fn set_pfh_version(&mut self, pfh_version: PFHVersion) {
        self.pfh_version = pfh_version;
    }

    /// This function returns the `PFHFileType` of the provided `PackFile`.
    pub fn get_pfh_file_type(&self) -> PFHFileType {
        self.pfh_file_type
    }

    /// This function sets the `PFHFileType` of the provided `PackFile`.
    pub fn set_pfh_file_type(&mut self, pfh_file_type: PFHFileType) {
        self.pfh_file_type = pfh_file_type;
    }

    /// This function returns the `Bitmask` of the provided `PackFile`.
    pub fn get_bitmask(&self) -> PFHFlags {
        self.bitmask
    }

    /// This function returns a reference to the `Bitmask` of the provided `PackFile`.
    pub fn get_ref_bitmask(&self) -> &PFHFlags {
        &self.bitmask
    }

    /// This function returns a mutable reference to the `Bitmask` of the provided `PackFile`.
    pub fn get_ref_mut_bitmask(&mut self) -> &mut PFHFlags {
        &mut self.bitmask
    }

    /// This function sets the `Bitmask` of the provided `PackFile`.
    pub fn set_bitmask(&mut self, bitmask: PFHFlags) {
        self.bitmask = bitmask;
    }

    /// This function remove all `PackedFiles` from a `PackFile`.
    pub fn remove_all_packedfiles(&mut self) {
        self.packed_files = vec![];
    }

    /// This function checks if a `PackedFile` with a certain path exists in a `PackFile`.
    pub fn packedfile_exists(&self, path: &[String]) -> bool {
        self.packed_files.par_iter().any(|x| x.get_path() == path)
    }

    /// This function checks if a folder with `PackedFiles` in it exists in a `PackFile`.
    pub fn folder_exists(&self, path: &[String]) -> bool {
        self.packed_files.par_iter().any(|x| x.get_path().starts_with(path) && !path.is_empty() && x.get_path().len() > path.len())
    }

    /// This function takes an slice of PathTypes and turns it into a vector of individual PackedFile's paths.
    ///
    /// This is intended to be done after a dedup. Otherwise, you'll probably get duplicated paths at the end.
    pub fn get_paths_from_path_types(&self, path_types: &[PathType]) -> Vec<Vec<String>> {

        // If we have a PackFile, just get the paths of all PackedFiles in the PackFile.
        let we_have_packfile = path_types.par_iter().any(|item| {
            if let PathType::PackFile = item { true } else { false }
        });

        if we_have_packfile {
            return self.get_packed_files_all_paths();
        }

        // If we don't have folders, the rest are files, so we just get the path stored in each PathType.
        let we_have_folder = path_types.par_iter().any(|item| {
            if let PathType::Folder(_) = item { true } else { false }
        });

        if !we_have_folder {
            return path_types.par_iter().filter_map(|x| if let PathType::File(path) = x { Some(path.to_vec()) } else { None }).collect();
        }

        // If we have a mix of files and folders... then we get the paths according to the type.
        let mut paths: Vec<Vec<String>> = path_types.par_iter()
            .filter_map(|path_type| if let PathType::Folder(path) = path_type { Some(self.get_packed_files_paths_by_path_start(&path)) } else { None })
            .flatten()
            .collect::<Vec<Vec<String>>>();

        paths.append(&mut path_types.par_iter()
            .filter_map(|x| if let PathType::File(path) = x { Some(path.to_vec()) } else { None })
            .collect::<Vec<Vec<String>>>());

        paths
    }

    /// This function allows you to change the path of a `PackedFile` inside a `PackFile`.
    ///
    /// By default this append a `_number` to the file name in case of collision. If you want it to overwrite instead,
    /// pass `overwrite` as `true`. This can fail if you pass it an empty or reserved path, so make sure you check the result.
    ///
    /// We return the final destination path of the PackedFile, if it worked, or an error.
    pub fn move_packedfile(
        &mut self,
        source_path: &[String],
        destination_path: &[String],
        overwrite: bool,
    ) -> Result<Vec<String>> {

        // First, ensure we can move between the paths.
        let reserved_names = Self::get_reserved_packed_file_names();
        if destination_path.is_empty() { return Err(ErrorKind::EmptyInput.into()) }
        if source_path == destination_path { return Err(ErrorKind::PathsAreEqual.into()) }
        if reserved_names.contains(&destination_path.to_vec()) { return Err(ErrorKind::ReservedFiles.into()) }

        // We may need to modify his destination path if we're not overwriting so...
        let mut destination_path = destination_path.to_vec();

        // First, we check if BOTH, the source and destination, exist.
        let source_exists = self.packedfile_exists(source_path);
        let destination_exists = self.packedfile_exists(&destination_path);

        // If both exists, we do some name resolving:
        // - If we want to overwrite the destination file, we simply remove it.
        // - If not, we check until we find a free path using "_X". This also takes into account extensions, so "m.loc" will become "m_1.loc".
        if source_exists && destination_exists {
            if overwrite { self.remove_packed_file_by_path(&destination_path); }
            else {
                let name_current = destination_path.last().unwrap().to_owned();
                let name_splitted = name_current.split('.').collect::<Vec<&str>>();
                let name = name_splitted[0];
                let extension = if name_splitted.len() > 1 { name_splitted[1..].join(".") } else { "".to_owned() };
                for number in 0.. {
                    let name = if extension.is_empty() { format!("{}_{}", name, number) } else { format!("{}_{}.{}", name, number, extension) };
                    *destination_path.last_mut().unwrap() = name;
                    if !self.packedfile_exists(&destination_path) && !reserved_names.contains(&destination_path) {
                        break;
                    }
                }
            }
        }

        // Then just change the path of the `PackedFile` if exists. Return error if it doesn't.
        match self.get_ref_mut_packed_file_by_path(source_path) {
            Some(packed_file) => {
                packed_file.get_ref_mut_raw().set_path(&destination_path)?;
                Ok(destination_path)
            },
            None => Err(ErrorKind::PackedFileNotFound.into())
        }
    }

    /// This function allows you to change the name of a folder inside a `PackFile`.
    ///
    /// By default this append a `_number` to the file names in case of collision. If you want it to overwrite instead,
    /// pass `overwrite` as `true`. This can fail if you pass it an empty or reserved path, so make sure you check the result.
    ///
    /// We return the list of source/final paths of each moved PackedFile, if it worked, or an error.
    pub fn move_folder(
        &mut self,
        source_path: &[String],
        destination_path: &[String],
        overwrite: bool,
    ) -> Result<Vec<(Vec<String>, Vec<String>)>> {

        // First, ensure we can move between the paths.
        if source_path.is_empty() || destination_path.is_empty() { return Err(ErrorKind::EmptyInput.into()) }
        if source_path == destination_path { return Err(ErrorKind::PathsAreEqual.into()) }

        // Next... just get all the PackedFiles to move, and move them one by one.
        let mut successes = vec![];
        for packed_file_current_path in self.get_ref_packed_files_by_path_start(source_path).iter().map(|x| x.get_path().to_vec()).collect::<Vec<Vec<String>>>() {
            let mut new_path = packed_file_current_path.to_vec();
            new_path.splice(..source_path.len(), destination_path.iter().cloned());
            if let Ok(new_path) = self.move_packedfile(&packed_file_current_path, &new_path, overwrite) {
                successes.push((packed_file_current_path, new_path))
            }
        }

        Ok(successes)
    }

    /// This function is used to rename one or more `PackedFile`/Folder inside a `PackFile`.
    ///
    /// It returns the list of "Original Path/New Path" of each renamed PackedFile.
    ///
    /// This doesn't stop on failure. Instead, if a rename fails, it skips that PackedFile from the rename process.
    ///
    /// If `overwrite` is set to `true`, in case of destination `PackedFile` already existing, it'll be overwritten.
    /// If set to `false`, the file will be renamed to 'xxx_1', or the first number available. Extensions are taken
    /// into account when doing this, so 'x.loc' will become 'x_1.loc'.
    pub fn rename_packedfiles(
        &mut self,
        renaming_data: &[(PathType, String)],
        overwrite: bool
    ) -> Vec<(PathType, Vec<String>)> {

        let mut successes = vec![];
        for (item_type, new_name) in renaming_data {

            // Skip items with empty new names.
            if new_name.is_empty() { continue; }

            // We only allow to rename files and folders.
            match item_type {
                PathType::File(ref path) => {
                    let mut new_path = path.to_vec();
                    *new_path.last_mut().unwrap() = new_name.to_owned();
                    if let Ok(destination_path) = self.move_packedfile(path, &new_path, overwrite) {
                        successes.push((item_type.clone(), destination_path));
                    }
                }

                PathType::Folder(ref path) => {
                    let mut new_path = path.to_vec();
                    *new_path.last_mut().unwrap() = new_name.to_owned();
                    if let Ok(result) = self.move_folder(path, &new_path, overwrite) {
                        result.iter().map(|(x, y)| (PathType::File(x.to_vec()), y.to_vec())).for_each(|x| successes.push(x));
                    }
                }

                // PackFiles and errors are skipped.
                PathType::PackFile | PathType::None => continue,
            }
        }

        // Return the list of successes.
        successes
    }

    /// This function checks all the DB Tables of the provided PackFile for dependency errors.
    ///
    /// TODO: Make this not throw warnings on references that point to a **localised** column.
    /// TODO2: Make this return the name of the columns instead of their index.
    pub fn check_table_integrity(&mut self) -> Result<()> {

        let schema = &*SCHEMA.read().unwrap();
        match schema {
            Some(ref schema) => {

                let mut broken_tables = vec![];
                let mut real_dep_db = DEPENDENCY_DATABASE.lock().unwrap();
                let fake_dep_db = FAKE_DEPENDENCY_DATABASE.read().unwrap();

                // Due to how mutability works, we have first to get the data of every table,
                // then iterate them and decode them. The errors here can be silenced safetly.
                self.get_ref_mut_packed_files_by_types(&[PackedFileType::DB, PackedFileType::Loc], false).par_iter_mut().for_each(|packed_file| {
                    let _ = packed_file.decode_no_locks(schema);
                });

                for packed_file in self.get_packed_files_by_path_start(&["db".to_owned()]) {
                    if let DecodedPackedFile::DB(table) = packed_file.get_ref_decoded() {
                        let dependency_data = DB::get_dependency_data(self, schema, &table.get_definition(), &mut real_dep_db, &fake_dep_db);

                        // If we got some dependency data (the referenced tables actually exists), check every
                        // referenced field of every referenced column for errors.
                        if !dependency_data.is_empty() {
                            let mut broken_columns = vec![];
                            for row in table.get_table_data() {
                                for (column, dependency_data) in &dependency_data {
                                    let field_data = match row[*column as usize] {
                                        DecodedData::Boolean(ref entry) => entry.to_string(),
                                        DecodedData::Float(ref entry) => entry.to_string(),
                                        DecodedData::Integer(ref entry) => entry.to_string(),
                                        DecodedData::LongInteger(ref entry) => entry.to_string(),
                                        DecodedData::StringU8(ref entry) |
                                        DecodedData::StringU16(ref entry) |
                                        DecodedData::OptionalStringU8(ref entry) |
                                        DecodedData::OptionalStringU16(ref entry) => entry.to_owned(),
                                        _ => "NoData".to_owned()
                                    };

                                    let dependency_data = dependency_data.iter().map(|x| x.0).collect::<Vec<&String>>();
                                    if &field_data != "NoData" && !field_data.is_empty() && !dependency_data.contains(&&field_data) {
                                        broken_columns.push(*column);
                                    }
                                }
                            }

                            // If we got missing refs, sort the columns, dedup them and turn them into a nice string for the error message.
                            // Columns + 1 is so we don't start counting on zero. Easier for the user to see.
                            if !broken_columns.is_empty() {
                                let path = packed_file.get_path();
                                broken_columns.sort();
                                broken_columns.dedup();
                                let mut broken_columns = broken_columns.iter().map(|x| format!("{},", *x + 1)).collect::<String>();
                                broken_columns.pop();
                                broken_tables.push(format!("Table: {}/{}, Column/s: {}", &path[1], &path[2], broken_columns));
                            }
                        }
                    }
                }

                // If all tables are Ok, return it. Otherwise, return an error with the list of broken tables.
                if broken_tables.is_empty() { Ok(()) }
                else { Err(ErrorKind::DBMissingReferences(broken_tables).into()) }
            }
            None => Err(ErrorKind::SchemaNotFound.into())
        }
    }

    /// This function merges (if possible) the provided DB and LOC tables into one with the provided name.
    ///
    /// NOTE: The merged table will be created in the folder of the first provided file.
    pub fn merge_tables(
        &mut self,
        paths: &[Vec<String>],
        name: &str,
        delete_source_paths: bool,
    ) -> Result<Vec<String>> {

        // Get the schema, as we'll need it unlocked to decode all the files fast.
        let schema = SCHEMA.read().unwrap();
        let schema = if let Some(ref schema) = *schema { schema } else { return Err(ErrorKind::SchemaNotFound.into()) };

        let mut db_files = vec![];
        let mut loc_files = vec![];

        // Decode the files and put them in their respective list.
        for path in paths {
            if let Some(packed_file) = self.get_ref_mut_packed_file_by_path(path) {
                match packed_file.decode_return_ref_no_locks(&schema)? {
                    DecodedPackedFile::DB(table) => db_files.push(table.clone()),
                    DecodedPackedFile::Loc(table) => loc_files.push(table.clone()),
                    _ => return Err(ErrorKind::InvalidFilesForMerging.into())
                }
            }
        }

        // If we have no tables, or we have both, db and loc, return an error. If we have only tables, but different tables, we also return an error.
        if (!db_files.is_empty() && !loc_files.is_empty()) || (db_files.is_empty() && loc_files.is_empty()) ||
        (!db_files.is_empty() && !db_files.iter().all(|x| x.name == db_files[0].name)) { return Err(ErrorKind::InvalidFilesForMerging.into()) }

        // If we have db tables, get their newest definition, update all the tables to that definition if needed,
        // and then merge all their data in one table.
        let merged_table = if !db_files.is_empty() {
            let db_files = if db_files.iter().all(|x| x.get_definition().version == db_files[0].get_definition().version) { db_files }
            else {
                let definition = db_files.iter().map(|x| x.get_definition()).max_by_key(|x| x.version).unwrap();
                for table in &mut db_files { table.set_definition(&definition); }
                db_files
            };
            let mut new_table = DB::new(&db_files[0].name, &db_files[0].get_definition());
            let mut entries = vec![];
            db_files.iter().for_each(|x| entries.extend_from_slice(x.get_ref_table_data()));
            new_table.set_table_data(&entries)?;
            DecodedPackedFile::DB(new_table)
        }

        // Same thing for locs.
        else if !loc_files.is_empty() {
            let loc_files = if loc_files.iter().all(|x| x.get_definition().version == loc_files[0].get_definition().version) { loc_files }
            else {
                let definition = loc_files.iter().map(|x| x.get_definition()).max_by_key(|x| x.version).unwrap();
                for table in &mut loc_files { table.set_definition(&definition); }
                loc_files
            };
            let mut new_table = Loc::new(&loc_files[0].get_definition());
            let mut entries = vec![];
            loc_files.iter().for_each(|x| entries.extend_from_slice(x.get_ref_table_data()));
            new_table.set_table_data(&entries)?;
            DecodedPackedFile::Loc(new_table)
        } else { unimplemented!() };

        // And then, we save the newly created table to a `PackedFile`.
        let mut path = paths[0].to_vec();
        path.pop();
        path.push(name.to_owned());
        let packed_file = PackedFile::new_from_decoded(&merged_table, path);

        // If we want to remove the source files, this is the moment.
        if delete_source_paths { paths.iter().for_each(|x| self.remove_packed_file_by_path(x)); }

        // Prepare the paths to return.
        self.add_packed_file(&packed_file, true)
    }

    /// This function is used to optimize a `PackFile` by removing extra useless data from it.
    ///
    /// Currently, this function removes:
    /// - Unchanged data from DB tables (except if the table has the same name as his vanilla counterpart and certain setting is enabled).
    /// - Unchanged data from Loc tables (except if the table has the same name as his vanilla counterpart and certain setting is enabled).
    /// - Empty DB tables (except if the table has the same name as his vanilla counterpart and certain setting is enabled).
    /// - Empty Loc tables (except if the table has the same name as his vanilla counterpart and certain setting is enabled).
    /// - XML files in map folders.
    pub fn optimize(&mut self) -> Vec<Vec<String>> {

        // List of PackedFiles to delete.
        let mut files_to_delete: Vec<Vec<String>> = vec![];
        let mut dependencies = DEPENDENCY_DATABASE.lock().unwrap();

        // We get the entire list of paths from the dependency database, so we can check if each `PackedFile is trying to overwrite a vanilla one or not.
        let database_path_list = dependencies.iter().map(|x| x.get_path().to_vec()).collect::<Vec<Vec<String>>>();

        // Get a list of every Loc and DB PackedFiles in our dependency's files. For performance reasons, we decode every one of them here.
        // Otherwise, they may have to be decoded multiple times, making this function take ages to finish.
        let (game_dbs, game_locs): (Vec<&DB>, Vec<&Loc>) = dependencies.iter_mut()
            .filter(|x| (x.get_path().len() == 3 && x.get_path()[0] == "db") || (x.get_path().last().unwrap().ends_with(".loc")))
            .map(|x| x.decode_return_ref())
            .filter(|x| x.is_ok())
            .map(|x| x.unwrap())
            .partition_map(|x| match x {
                DecodedPackedFile::DB(db) => Either::Left(db),
                DecodedPackedFile::Loc(loc) => Either::Right(loc),
                _ => unreachable!()
            });

        // We do this in two passes. First, we optimize the data inside the `PackedFiles`. Then, we do a *cleaning* pass, removing empty or useless `PackedFiles`.
        for packed_file in self.get_ref_mut_packed_files_all() {
            let path = packed_file.get_path().to_vec();

            // Unless we specifically wanted to, ignore the same-name-as-vanilla files,
            // as those are probably intended to overwrite vanilla files, not to be optimized.
            if database_path_list.contains(&path) && !SETTINGS.read().unwrap().settings_bool["optimize_not_renamed_packedfiles"] { continue; }

            // If it's a DB table, try to optimize it.
            if path.len() == 3 && path[0] == "db" && !game_dbs.is_empty() {

                // Try to decode our table.
                match packed_file.decode_return_ref_mut() {
                    Ok(data) => if let DecodedPackedFile::DB(db) = data {
                        let is_empty = db.optimize_table(&game_dbs);
                        if is_empty { files_to_delete.push(path.to_vec()); }
                    },
                    Err(_) => continue,
                };
            }

            // If it's a Loc table, try to optimize it.
            else if path.last().unwrap().ends_with(".loc") && !game_locs.is_empty() {
                match packed_file.decode_return_ref_mut() {
                    Ok(data) => if let DecodedPackedFile::Loc(loc) = data {
                        let is_empty = loc.optimize_table(&game_locs);
                        if is_empty { files_to_delete.push(path.to_vec()); }
                    },
                    Err(_) => continue,
                };
            }

            // If it's an xml in a map folder, remove it.
            else if !path.is_empty() && path.starts_with(&Self::get_terry_map_path()) && path.last().unwrap().ends_with(".xml") {
                files_to_delete.push(path.to_vec());
            }
        }

        // Delete all the files marked for deletion.
        files_to_delete.iter().for_each(|x| self.remove_packed_file_by_path(x));

        // Return the deleted files, so the caller can know what got removed.
        files_to_delete
    }

    /// This function is used to patch Warhammer Siege map packs so their AI actually works.
    ///
    /// This also removes the useles xml files left by Terry in the `PackFile`.
    pub fn patch_siege_ai(&mut self) -> Result<(String, Vec<Vec<String>>)> {

        // If there are no files, directly return an error.
        if self.packed_files.is_empty() {
            return Err(ErrorKind::PatchSiegeAIEmptyPackFile.into())
        }

        let mut files_patched = 0;
        let mut files_to_delete: Vec<Vec<String>> = vec![];
        let mut multiple_defensive_hill_hints = false;

        // We only need to change stuff inside the map folder, so we only check the maps in that folder.
        for packed_file in self.get_ref_mut_packed_files_by_path_start(&Self::get_terry_map_path()) {
            let path = packed_file.get_path();
            let name = path.last().unwrap().clone();

            // The files we need to process are `bmd_data.bin` and all the `catchment_` files the map has.
            if name == DEFAULT_BMD_DATA || (name.starts_with("catchment_") && name.ends_with(".bin")) {
                let data = packed_file.get_ref_mut_raw().get_ref_mut_data_and_keep_it()?;

                // The patching process it's simple. First, we check if there is SiegeAI stuff in the file by checking if there is an Area Node.
                // If we find one, we check if there is a defensive hill hint in the same file, and patch it if there is one.
                if data.windows(19).any(|window: &[u8]|window == SIEGE_AREA_NODE_HINT) {
                    if let Some(index) = data.windows(18).position(|window: &[u8]|window == DEFENSIVE_HILL_HINT) {
                        data.splice(index..index + 18, FORT_PERIMETER_HINT.iter().cloned());
                        files_patched += 1;
                    }

                    // If there is more than one defensive hill in one file, is a valid file, but we want to warn the user about it.
                    if data.windows(18).any(|window: &[u8]|window == DEFENSIVE_HILL_HINT) {
                        multiple_defensive_hill_hints = true;
                    }
                }
            }

            // All xml in this folder are useles, so we mark them all for deletion.
            else if name.ends_with(".xml") {
                files_to_delete.push(packed_file.get_path().to_vec());
            }
        }

        // If there are files to delete, we delete them.
        files_to_delete.iter().for_each(|x| self.remove_packed_file_by_path(x));

        // If we didn't found any file to patch or delete, return an error.
        if files_patched == 0 && files_to_delete.is_empty() { Err(ErrorKind::PatchSiegeAINoPatchableFiles.into()) }

        // TODO: make this more.... `fluent`.
        // If we found files to delete, but not to patch, return a message reporting it.
        else if files_patched == 0 {
            Ok((format!("No file suitable for patching has been found.\n{} files deleted.", files_to_delete.len()), files_to_delete))
        }

        // If we found multiple defensive hill hints... it's ok, but we return a warning.
        else if multiple_defensive_hill_hints {

            // The message is different depending on the amount of files deleted.
            if files_to_delete.is_empty() {
                Ok((format!("{} files patched.\nNo file suitable for deleting has been found.\
                \n\n\
                WARNING: Multiple Defensive Hints have been found and we only patched the first one.\
                 If you are using SiegeAI, you should only have one Defensive Hill in the map (the \
                 one acting as the perimeter of your fort/city/castle). Due to SiegeAI being present, \
                 in the map, normal Defensive Hills will not work anyways, and the only thing they do \
                 is interfere with the patching process. So, if your map doesn't work properly after \
                 patching, delete all the extra Defensive Hill Hints. They are the culprit.",
                 files_patched), files_to_delete))
            }
            else {
                Ok((format!("{} files patched.\n{} files deleted.\
                \n\n\
                WARNING: Multiple Defensive Hints have been found and we only patched the first one.\
                 If you are using SiegeAI, you should only have one Defensive Hill in the map (the \
                 one acting as the perimeter of your fort/city/castle). Due to SiegeAI being present, \
                 in the map, normal Defensive Hills will not work anyways, and the only thing they do \
                 is interfere with the patching process. So, if your map doesn't work properly after \
                 patching, delete all the extra Defensive Hill Hints. They are the culprit.",
                files_patched, files_to_delete.len()), files_to_delete))
            }
        }

        // If no files to delete were found, but we got files patched, report it.
        else if files_to_delete.is_empty() {
            Ok((format!("{} files patched.\nNo file suitable for deleting has been found.", files_patched), files_to_delete))
        }

        // And finally, if we got some files patched and some deleted, report it too.
        else {
            Ok((format!("{} files patched.\n{} files deleted.", files_patched, files_to_delete.len()), files_to_delete))
        }
    }


    /// This function is used to Mass-Import TSV files into a PackFile.
    pub fn mass_import_tsv(
        &mut self,
        tsv_paths: &[PathBuf],
        name: Option<String>,
        overwrite: bool
    ) -> Result<(Vec<Vec<String>>, Vec<Vec<String>>)> {

        // Create the following lists:
        // - PackedFiles to add.
        // - PackedFiles to remove.
        // - Paths with errors.
        let mut packed_files: Vec<PackedFile> = vec![];
        let mut packed_files_to_remove = vec![];
        let mut error_files = vec![];

        // If there is not a schema, don't do anything.
        if let Some(ref schema) = *SCHEMA.read().unwrap() {
            for path in tsv_paths {

                // The first row has the PackedFile Type (or name, in case of tables) and version.
                // The second row contains the column names, and it can be ignored.
                let mut tsv = String::new();
                BufReader::new(File::open(&path)?).read_to_string(&mut tsv)?;

                // We get his first line, if it has it. Otherwise, we return an error in this file.
                if let Some(line) = tsv.lines().next() {

                    // Split the first line by \t so we can get the info of the table.
                    // We expect to have 2 items here. If we have more or less, stop.
                    let tsv_info = line.split('\t').collect::<Vec<&str>>();
                    if tsv_info.len() == 2 {

                        // Get the type and the version of the table.
                        let table_type = tsv_info[0];
                        let table_version = match tsv_info[1].parse::<i32>() {
                            Ok(version) => version,
                            Err(_) => {
                                error_files.push(path.to_string_lossy().to_string());
                                continue
                            }
                        };

                        // Get the definition, depending on the table type and version.
                        // If the name is not specific for a type of file, we trat it as a DB Table.
                        match table_type {
                            TSV_NAME_LOC => {
                                let definition = schema.get_ref_versioned_file_loc()?.get_version(table_version)?;
                                if let Ok(table) = Loc::import_tsv(&definition, &path, &table_type) {

                                    // Depending on the name received, call it one thing or another.
                                    let name = match name {
                                        Some(ref name) => name.to_string(),
                                        None => path.file_stem().unwrap().to_str().unwrap().to_string(),
                                    };

                                    let mut path = vec!["text".to_owned(), "db".to_owned(), format!("{}.loc", name)];

                                    // If that path already exists in the list of new PackedFiles to add, change it using the index.
                                    if !overwrite {
                                        let mut index = 1;
                                        while packed_files.iter().any(|x| x.get_path() == &*path) {
                                            path[2] = format!("{}_{}.loc", name, index);
                                            index += 1;
                                        }
                                    }

                                    // If that path already exist in the PackFile, add it to the "remove" list.
                                    if self.packedfile_exists(&path) { packed_files_to_remove.push(path.to_vec()) }

                                    // Create and add the new PackedFile to the list of PackedFiles to add.
                                    let mut packed_file = PackedFile::new(path, self.get_file_name());
                                    packed_file.set_decoded(&DecodedPackedFile::Loc(table));
                                    packed_files.push(packed_file);
                                }
                                else { error_files.push(path.to_string_lossy().to_string()); }
                            }
                            _ => {
                                let definition = schema.get_ref_versioned_file_db(&table_type)?.get_version(table_version)?;
                                if let Ok(table) = DB::import_tsv(&definition, &path, &table_type) {

                                    // Depending on the name received, call it one thing or another.
                                    let name = match name {
                                        Some(ref name) => name.to_string(),
                                        None => path.file_stem().unwrap().to_str().unwrap().to_string(),
                                    };

                                    let mut path = vec!["db".to_owned(), table_type.to_owned(), name.to_owned()];

                                    // If that path already exists in the list of new PackedFiles to add, change it using the index.
                                    if !overwrite {
                                        let mut index = 1;
                                        while packed_files.iter().any(|x| x.get_path() == &*path) {
                                            path[2] = format!("{}_{}", name, index);
                                            index += 1;
                                        }
                                    }

                                    // If that path already exists in the PackFile, add it to the "remove" list.
                                    if self.packedfile_exists(&path) { packed_files_to_remove.push(path.to_vec()) }

                                    // Create and add the new PackedFile to the list of PackedFiles to add.
                                    let mut packed_file = PackedFile::new(path, self.get_file_name());
                                    packed_file.set_decoded(&DecodedPackedFile::DB(table));
                                    packed_files.push(packed_file);
                                }
                                else { error_files.push(path.to_string_lossy().to_string()); }
                            }
                        }
                    }
                    else { error_files.push(path.to_string_lossy().to_string()); }
                }
                else { error_files.push(path.to_string_lossy().to_string()); }
            }

            // If any of the files returned error, return error.
            if !error_files.is_empty() {
                let error_files_string = error_files.iter().map(|x| format!("<li>{}</li>", x)).collect::<String>();
                return Err(ErrorKind::MassImport(error_files_string).into())
            }

            // Get the "TreePath" of the new PackFiles to return them.
            let tree_path = packed_files.iter().map(|x| x.get_path().to_vec()).collect::<Vec<Vec<String>>>();

            // Remove all the "conflicting" PackedFiles from the PackFile, before adding the new ones.
            for packed_file_to_remove in &packed_files_to_remove {
                self.remove_packed_file_by_path(packed_file_to_remove);
            }

            // We add all the files to the PackFile, and return success.
            let packed_files_to_add = packed_files.iter().collect::<Vec<&PackedFile>>();
            self.add_packed_files(&packed_files_to_add, true)?;
            Ok((packed_files_to_remove, tree_path))
        }
        else {
            Err(ErrorKind::SchemaNotFound.into())
        }
    }

    /// This function is used to Mass-Export TSV files from a PackFile.
    ///
    /// NOTE: this will OVERWRITE any existing file that has a name conflict with the TSV files provided.
    pub fn mass_export_tsv(&mut self, path_types: &[PathType], export_path: &PathBuf) -> Result<String> {

        // Lists of PackedFiles that couldn't be exported for one thing or another and exported PackedFile names,
        // so we make sure we don't overwrite those with the following ones.
        let mut error_list = vec![];
        let mut exported_files = vec![];

        // We need the schema to export. If there is no schema, return an error.
        match *SCHEMA.read().unwrap() {
            Some(ref schema) => {

                // Keep the PathTypes added so we can return them to the UI easely.
                let path_types = PathType::dedup(path_types);
                let paths = self.get_paths_from_path_types(&path_types);
                let paths_ref = paths.par_iter().map(|x| (*x).as_ref()).collect::<Vec<&[String]>>();

                let mut packed_files = self.get_ref_mut_packed_files_by_paths(paths_ref);

                // Decode the entire set of PackedFiles we want to export.
                packed_files.iter_mut().for_each(|packed_file| {
                    let path = packed_file.get_path().to_vec();
                    match packed_file.decode_return_ref_no_locks(schema) {
                        Ok(data) => match data {
                            DecodedPackedFile::DB(data) => {

                                // His name will be "db_name_file_name.tsv". If that's taken, we'll add an index until we find one available.
                                let mut name = format!("{}_{}.tsv", path[1], path.last().unwrap().to_owned());
                                let mut export_path = export_path.to_path_buf();

                                // Checks to avoid overwriting exported files go here, in an infinite loop of life and death.
                                let mut index = 1;
                                while exported_files.contains(&name) {
                                    name = format!("{}_{}_{}.tsv", path[1], path.last().unwrap().to_owned(), index);
                                    index += 1;
                                }

                                export_path.push(name.to_owned());
                                match data.export_tsv(&export_path, &path[1]) {
                                    Ok(_) => exported_files.push(name),
                                    Err(error) => error_list.push((packed_file.get_path().join("\\"), error)),
                                }

                            }
                            DecodedPackedFile::Loc(data) => {

                                // His name will be "db_name_file_name.tsv". If that's taken, we'll add an index until we find one available.
                                let mut name = format!("{}.tsv", path.last().unwrap().to_owned());
                                let mut export_path = export_path.to_path_buf();

                                // Checks to avoid overwriting exported files go here, in an infinite loop of life and death.
                                let mut index = 1;
                                while exported_files.contains(&name) {
                                    name = format!("{}_{}.tsv", path.last().unwrap().to_owned(), index);
                                    index += 1;
                                }

                                export_path.push(name.to_owned());
                                match data.export_tsv(&export_path, &TSV_NAME_LOC) {
                                    Ok(_) => exported_files.push(name),
                                    Err(error) => error_list.push((packed_file.get_path().join("\\"), error)),
                                }

                            }

                            // Ignore any other PackedFiles.
                            _ => {}
                        }
                        Err(error) => error_list.push((packed_file.get_path().join("\\"), error)),
                    }
                });
            }
            None => return Err(Error::from(ErrorKind::SchemaNotFound)),
        }

        // If there has been errors, return ok with the list of errors.
        if !error_list.is_empty() {
            let error_files_string = error_list.iter().map(|x| format!("<li>{}</li>", x.0)).collect::<String>();
            Ok(format!("<p>All exportable files have been exported, except the following ones:</p><ul>{}</ul>", error_files_string))
        }

        // Otherwise, just return success and an empty error list.
        else { Ok("<p>All exportable files have been exported.</p>".to_owned()) }
    }

    /// This function loads to memory the vanilla (made by CA) dependencies of a `PackFile`.
    fn load_vanilla_dependency_packfiles(packed_files: &mut Vec<PackedFile>) {

        // Get all the paths we need.
        let main_db_pack_paths = get_game_selected_db_pack_path();
        let main_loc_pack_paths = get_game_selected_loc_pack_path();

        // Get all the DB Tables from the main DB `PackFiles`, if it's configured.
        if let Some(paths) = main_db_pack_paths {
            if let Ok(pack_file) = PackFile::open_packfiles(&paths, true, false, false) {
                for packed_file in pack_file.get_ref_packed_files_by_path_start(&["db".to_owned()]) {

                    // Clone the PackedFile, and add it to the list.
                    let mut packed_file = packed_file.clone();
                    if packed_file.get_ref_mut_raw().load_data().is_ok() {
                        packed_files.push(packed_file);
                    }
                }
            }
        }

        // Get all the Loc PackedFiles from the main Loc `PackFiles`, if it's configured.
        if let Some(paths) = main_loc_pack_paths {
             if let Ok(pack_file) = PackFile::open_packfiles(&paths, true, false, false) {
                for packed_file in pack_file.get_ref_packed_files_by_path_end(&[".loc".to_owned()]) {

                    // Clone the PackedFile, and add it to the list.
                    let mut packed_file = packed_file.clone();
                    if packed_file.get_ref_mut_raw().load_data().is_ok() {
                        packed_files.push(packed_file);
                    }
                }
            }
        }
    }

    /// This function loads a `PackFile` as dependency, loading all his dependencies in the process.
    fn load_single_dependency_packfile(
        packed_files: &mut Vec<PackedFile>,
        packfile_name: &str,
        already_loaded_dependencies: &mut Vec<String>,
        data_paths: &Option<Vec<PathBuf>>,
        contents_paths: &Option<Vec<PathBuf>>,
    ) {

        // First we load the content `PackFiles`.
        if let Some(ref paths) = contents_paths {
            if let Some(path) = paths.iter().find(|x| x.file_name().unwrap().to_string_lossy() == packfile_name) {
                if let Ok(pack_file) = PackFile::open_packfiles(&[path.to_path_buf()], true, false, false) {

                    // Add the current `PackFile` to the done list, so we don't get into cyclic dependencies.
                    already_loaded_dependencies.push(packfile_name.to_owned());
                    pack_file.get_packfiles_list().iter().for_each(|x| Self::load_single_dependency_packfile(packed_files, x, already_loaded_dependencies, data_paths, contents_paths));
                    for packed_file in pack_file.get_ref_packed_files_by_path_start(&["db".to_owned()]) {

                        // Clone the PackedFile, and add it to the list.
                        let mut packed_file = packed_file.clone();
                        if packed_file.get_ref_mut_raw().load_data().is_ok() {
                            packed_files.push(packed_file);
                        }
                    }

                    for packed_file in pack_file.get_ref_packed_files_by_path_end(&["loc".to_owned()]) {

                        // Clone the PackedFile, and add it to the list.
                        let mut packed_file = packed_file.clone();
                        if packed_file.get_ref_mut_raw().load_data().is_ok() {
                            packed_files.push(packed_file);
                        }
                    }
                }
            }
        }

        // Then we load the data `PackFiles`.
        if let Some(ref paths) = data_paths {
            if let Some(path) = paths.iter().find(|x| x.file_name().unwrap().to_string_lossy() == packfile_name) {
                if let Ok(pack_file) = PackFile::open_packfiles(&[path.to_path_buf()], true, false, false) {

                    // Add the current `PackFile` to the done list, so we don't get into cyclic dependencies.
                    already_loaded_dependencies.push(packfile_name.to_owned());
                    pack_file.get_packfiles_list().iter().for_each(|x| Self::load_single_dependency_packfile(packed_files, x, already_loaded_dependencies, data_paths, contents_paths));
                    for packed_file in pack_file.get_ref_packed_files_by_path_start(&["db".to_owned()]) {

                        // Clone the PackedFile, and add it to the list.
                        let mut packed_file = packed_file.clone();
                        if packed_file.get_ref_mut_raw().load_data().is_ok() {
                            packed_files.push(packed_file);
                        }
                    }

                    for packed_file in pack_file.get_ref_packed_files_by_path_end(&["loc".to_owned()]) {

                        // Clone the PackedFile, and add it to the list.
                        let mut packed_file = packed_file.clone();
                        if packed_file.get_ref_mut_raw().load_data().is_ok() {
                            packed_files.push(packed_file);
                        }
                    }
                }
            }
        }
    }

    /// This function loads to memory the custom (made by modders) dependencies of a `PackFile`.
    ///
    /// To avoid entering into an infinite loop while calling this recursively, we have to pass the
    /// list of loaded `PackFiles` each time we execute this.
    fn load_custom_dependency_packfiles(
        packed_files: &mut Vec<PackedFile>,
        pack_file_names: &[String],
    ) {

        let data_packs_paths = get_game_selected_data_packfiles_paths();
        let content_packs_paths = get_game_selected_content_packfiles_paths();
        let mut loaded_packfiles = vec![];

        pack_file_names.iter().for_each(|x| Self::load_single_dependency_packfile(packed_files, x, &mut loaded_packfiles, &data_packs_paths, &content_packs_paths));
    }

    /// This function loads to memory the dependencies of a `PackFile`. Well.... most of them.
    ///
    /// This function loads to memory all DB and Loc `PackedFiles` from vanilla `PackFiles` and
    /// from any `PackFile` the provided `PackFile` has as a dependency.
    pub fn load_all_dependency_packfiles(dependencies: &[String]) -> Vec<PackedFile> {

        // Create the empty list.
        let mut packed_files = vec![];

        Self::load_vanilla_dependency_packfiles(&mut packed_files);
        Self::load_custom_dependency_packfiles(&mut packed_files, dependencies);

        packed_files
    }

    /// This function allows you to open all CA PackFiles as one for the currently selected Game.
    ///
    /// This function tries to get the list of CA PackFile of the currently selected game from the manifest.txt on /data,
    /// then it tries to open them all as one. Simple and effective.
    pub fn open_all_ca_packfiles() -> Result<Self> {
        let data_path = get_game_selected_data_path().ok_or_else(|| ErrorKind::GameSelectedPathNotCorrectlyConfigured)?;
        let manifest = Manifest::read_from_game_selected()?;
        let pack_file_names = manifest.0.iter().filter_map(|x| if x.relative_path.ends_with(".pack") { Some(x.relative_path.to_owned()) } else { None }).collect::<Vec<String>>();
        let pack_file_paths = pack_file_names.iter().map(|x| {
            let mut pack_file_path = data_path.to_path_buf();
            pack_file_path.push(x);
            pack_file_path
        }).collect::<Vec<PathBuf>>();
        Self::open_packfiles(&pack_file_paths, true, true, true)
    }

    /// This function allows you to open one or more `PackFiles`.
    ///
    /// The way it works:
    /// - If you open just one `PackFile`, it just calls the `PackFile::read()` function on it.
    /// - If you open multiple `PackFiles`, it merges them into one, taking care of conflicts the same way the game does.
    ///
    /// You can also make it ignore mod PackFiles, so it only open `PackFiles` released by CA, and can choose to lock it,
    /// so the user cannot save it (avoiding the *"I tried to save and got an out-of-memory error!!!"* problem).
    pub fn open_packfiles(
        packs_paths: &[PathBuf],
        use_lazy_loading: bool,
        ignore_mods: bool,
        lock_packfile: bool
    ) -> Result<Self> {

        // If we just have one `PackFile`, just read it. No fancy logic needed. If you're an asshole and tried to break this
        // by passing it no paths, enjoy the error.
        if packs_paths.is_empty() { return Err(ErrorKind::PackFileNoPathProvided.into()) }
        if packs_paths.len() == 1 { Self::read(&packs_paths[0], use_lazy_loading) }

        // Otherwise, read all of them into a *fake* `PackFile` and take care of the duplicated files like the game will do.
        else {

            // We have to ensure the paths are sorted and valid. Otherwise, this can go to hell.
            let mut packs_paths = packs_paths.iter().filter(|x| x.is_file()).collect::<Vec<&PathBuf>>();
            packs_paths.sort_by_key(|x| x.file_name().unwrap().to_string_lossy().to_string());

            let pfh_version = SUPPORTED_GAMES.get(&**GAME_SELECTED.read().unwrap()).unwrap().pfh_version[0];
            let pfh_name = if ignore_mods { GAME_SELECTED.read().unwrap().to_owned() } else { String::from("merged_mod.pack")};
            let mut pack_file = Self::new_with_name(&pfh_name, pfh_version);

            // Read all the `PackFiles`, one by one, and separate their files by `PFHFileType`.
            let mut boot_files = vec![];
            let mut release_files = vec![];
            let mut patch_files = vec![];
            let mut mod_files = vec![];
            let mut movie_files = vec![];
            for path in packs_paths {
                match Self::read(&path, use_lazy_loading) {
                    Ok(pack) => match pack.get_pfh_file_type() {
                        PFHFileType::Boot => boot_files.append(&mut pack.get_packed_files_all()),
                        PFHFileType::Release => release_files.append(&mut pack.get_packed_files_all()),
                        PFHFileType::Patch => patch_files.append(&mut pack.get_packed_files_all()),
                        PFHFileType::Mod => mod_files.append(&mut pack.get_packed_files_all()),
                        PFHFileType::Movie => movie_files.append(&mut pack.get_packed_files_all()),

                        // If we find an unknown one, return an error.
                        PFHFileType::Other(_) => return Err(ErrorKind::PackFileTypeUknown.into()),
                    },
                    Err(error) => return Err(error)
                }
            }

            // The priority in case of collision is:
            // - Same Type: First to come is the valid one.
            // - Different Type: Last to come is the valid one.
            boot_files.sort_by_key(|x| x.get_path().to_vec());
            boot_files.dedup_by_key(|x| x.get_path().to_vec());

            release_files.sort_by_key(|x| x.get_path().to_vec());
            release_files.dedup_by_key(|x| x.get_path().to_vec());

            patch_files.sort_by_key(|x| x.get_path().to_vec());
            patch_files.dedup_by_key(|x| x.get_path().to_vec());

            mod_files.sort_by_key(|x| x.get_path().to_vec());
            mod_files.dedup_by_key(|x| x.get_path().to_vec());

            movie_files.sort_by_key(|x| x.get_path().to_vec());
            movie_files.dedup_by_key(|x| x.get_path().to_vec());

            pack_file.add_packed_files(&(boot_files.iter().map(|x| x).collect::<Vec<&PackedFile>>()), true)?;
            pack_file.add_packed_files(&(release_files.iter().map(|x| x).collect::<Vec<&PackedFile>>()), true)?;
            pack_file.add_packed_files(&(patch_files.iter().map(|x| x).collect::<Vec<&PackedFile>>()), true)?;
            pack_file.add_packed_files(&(mod_files.iter().map(|x| x).collect::<Vec<&PackedFile>>()), true)?;
            pack_file.add_packed_files(&(movie_files.iter().map(|x| x).collect::<Vec<&PackedFile>>()), true)?;

            // Set it as type "Other(200)", so we can easely identify it as fake in other places.
            // Used to lock the CA Files.
            if lock_packfile {
                pack_file.set_pfh_file_type(PFHFileType::Other(200));
            }

            // Return the new PackedFiles list.
            Ok(pack_file)
        }
    }

    /// This function reads the content of a PackFile into a `PackFile` struct.
    pub fn read(
        file_path: &PathBuf,
        use_lazy_loading: bool
    ) -> Result<Self> {

        // Check if what we received is even a `PackFile`.
        if !file_path.file_name().unwrap().to_string_lossy().to_string().ends_with(".pack") { return Err(ErrorKind::OpenPackFileInvalidExtension.into()) }

        // Prepare the PackFile to be read and the virtual PackFile to be written.
        let mut pack_file = BufReader::new(File::open(&file_path)?);
        let pack_file_name = file_path.file_name().unwrap().to_string_lossy().to_string();
        let mut pack_file_decoded = Self::new();

        // First, we do some quick checkings to ensure it's a valid PackFile.
        // 24 is the bare minimum that we need to check how a PackFile should be internally, so any file with less than that is not a valid PackFile.
        let pack_file_len = pack_file.get_ref().metadata()?.len();
        if pack_file_len < 24 { return Err(ErrorKind::PackFileHeaderNotComplete.into()) }

        // Create a little buffer to read the basic data from the header of the PackFile.
        let mut buffer = vec![0; 24];
        pack_file.read_exact(&mut buffer)?;

        // Start populating our decoded PackFile struct.
        pack_file_decoded.file_path = file_path.to_path_buf();
        pack_file_decoded.pfh_version = PFHVersion::get_version(&buffer.decode_string_u8(0, 4)?)?;
        pack_file_decoded.pfh_file_type = PFHFileType::get_type(buffer.decode_integer_u32(4)? & 15);
        pack_file_decoded.bitmask = PFHFlags::from_bits_truncate(buffer.decode_integer_u32(4)? & !15);

        // Read the data about the indexes to use it later.
        let pack_file_count = buffer.decode_integer_u32(8)?;
        let pack_file_index_size = buffer.decode_integer_u32(12)?;
        let packed_file_count = buffer.decode_integer_u32(16)?;
        let packed_file_index_size = buffer.decode_integer_u32(20)?;

        // Depending on the data we got, prepare to read the header and ensure we have all the bytes we need.
        match pack_file_decoded.pfh_version {
            PFHVersion::PFH5 | PFHVersion::PFH4 => {
                if (pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) && pack_file_len < 48) ||
                    (!pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) && pack_file_len < 28) { return Err(ErrorKind::PackFileHeaderNotComplete.into()) }

                if pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) { buffer = vec![0; 48]; }
                else { buffer = vec![0; 28]; }
            }

            PFHVersion::PFH3 | PFHVersion::PFH2 => buffer = vec![0; 32],
            PFHVersion::PFH0 => buffer = vec![0; 24],
        }

        // Restore the cursor of the BufReader to 0, so we can read the full header in one go. The first 24 bytes are
        // already decoded but, for the sake of clarity in the positions of the rest of the header stuff, we do this.
        pack_file.seek(SeekFrom::Start(0))?;
        pack_file.read_exact(&mut buffer)?;

        // The creation time is a bit of an asshole. Depending on the PackFile Version/Id/Preamble, it uses a type, another or it doesn't exists.
        // Keep in mind that we store his raw value. If you want his legible value, you have to convert it yourself. PFH0 doesn't have it.
        pack_file_decoded.timestamp = match pack_file_decoded.pfh_version {
            PFHVersion::PFH5 | PFHVersion::PFH4 => i64::from(buffer.decode_integer_u32(24)?),
            PFHVersion::PFH3 | PFHVersion::PFH2 => (buffer.decode_integer_i64(24)? / WINDOWS_TICK) - SEC_TO_UNIX_EPOCH,
            PFHVersion::PFH0 => 0
        };

        // Ensure the PackFile has all the data needed for the index. If the PackFile's data is encrypted
        // and the PackFile is PFH5, due to how the encryption works, the data should start in a multiple of 8.
        let mut data_position = u64::from(buffer.len() as u32 + pack_file_index_size + packed_file_index_size);
        if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) &&
            pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) &&
            pack_file_decoded.pfh_version == PFHVersion::PFH5 {
            data_position = if (data_position % 8) > 0 { data_position + 8 - (data_position % 8) } else { data_position };
        }
        if pack_file_len < data_position { return Err(ErrorKind::PackFileIndexesNotComplete.into()) }

        // Create the buffers for the indexes data.
        let mut pack_file_index = vec![0; pack_file_index_size as usize];
        let mut packed_file_index = vec![0; packed_file_index_size as usize];

        // Get the data from both indexes to their buffers.
        pack_file.read_exact(&mut pack_file_index)?;
        pack_file.read_exact(&mut packed_file_index)?;

        // Read the PackFile Index.
        let mut pack_file_index_position: usize = 0;

        // First, we decode every entry in the PackFile index and store it. It's encoded in StringU8 terminated in 00,
        // so we just read them char by char until hitting 0, then decode the next one and so on.
        // NOTE: This doesn't deal with encryption, as we haven't seen any encrypted PackFile with data in this index.
        for _ in 0..pack_file_count {
            let pack_file_name = pack_file_index.decode_packedfile_string_u8_0terminated(pack_file_index_position, &mut pack_file_index_position)?;
            pack_file_decoded.pack_files.push(pack_file_name);
        }

        // Depending on the version of the PackFile and his bitmask, the PackedFile index has one format or another.
        let packed_file_index_path_offset = match pack_file_decoded.pfh_version {
            PFHVersion::PFH5 => {

                // If it has the extended header bit, is an Arena PackFile. These ones use a normal PFH4 index format for some reason.
                if pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) {
                    if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 8 } else { 4 }
                }

                // Otherwise, it's a Warhammer 2 PackFile. These ones have 4 bytes for the size, 4 for the timestamp and 1 for the compression.
                else if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 9 } else { 5 }
            }

            // If it has the last modified date of the PackedFiles, we default to 8. Otherwise, we default to 4.
            PFHVersion::PFH4 => if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 8 } else { 4 }

            // These are like PFH4, but the timestamp has 8 bytes instead of 4.
            PFHVersion::PFH3 | PFHVersion::PFH2 => if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 12 } else { 4 }

            // There isn't seem to be a bitmask in ANY PFH0 PackFile, so we will assume they didn't even use it back then.
            PFHVersion::PFH0 => 4
        };

        // Prepare the needed stuff to read the PackedFiles.
        let mut index_position: usize = 0;
        let pack_file = Arc::new(Mutex::new(pack_file));
        for packed_files_to_decode in (0..packed_file_count).rev() {

            // Get his size. If it's encrypted, decrypt it first.
            let size = if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) {
                let encrypted_size = packed_file_index.decode_integer_u32(index_position)?;
                decrypt_index_item_file_length(encrypted_size, packed_files_to_decode as u32)
            } else {
                packed_file_index.decode_integer_u32(index_position)?
            };

            // If we have the last modified date of the PackedFiles in the Index, get it. Otherwise, default to 0,
            // so we have something to write in case we want to enable them for our PackFile.
            let timestamp = if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) {
                match pack_file_decoded.pfh_version {
                    PFHVersion::PFH5 | PFHVersion::PFH4 => {
                        let timestamp = i64::from(packed_file_index.decode_integer_u32(index_position + 4)?);
                        if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) {
                            i64::from(decrypt_index_item_file_length(timestamp as u32, packed_files_to_decode as u32))
                        } else { timestamp }
                    }

                    // We haven't found a single encrypted PFH3/PFH0 PackFile to test, so always assume these are unencrypted. Also, PFH0 doesn't seem to have a timestamp.
                    PFHVersion::PFH3 | PFHVersion::PFH2 => (packed_file_index.decode_integer_i64(index_position + 4)? / WINDOWS_TICK) - SEC_TO_UNIX_EPOCH,
                    PFHVersion::PFH0 => 0,
                }
            } else { 0 };

            // Update his offset, and get his compression data if it has it.
            index_position += packed_file_index_path_offset;
            let is_compressed = if let PFHVersion::PFH5 = pack_file_decoded.pfh_version {
                if let Ok(true) = packed_file_index.decode_bool(index_position - 1) { true }
                else { false }
            } else { false };

            // Get his path. Like the PackFile index, it's a StringU8 terminated in 00. We get it and split it in folders for easy use.
            let path = if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) {
                decrypt_index_item_filename(&packed_file_index[index_position..], size as u8, &mut index_position)
            }
            else { packed_file_index.decode_packedfile_string_u8_0terminated(index_position, &mut index_position)? };
            let path = path.split('\\').map(|x| x.to_owned()).collect::<Vec<String>>();

            // Once we are done, we create the and add it to the PackedFile list.
            let raw_data = RawPackedFile::read_from_data(
                path,
                pack_file_name.to_string(),
                timestamp,
                is_compressed,
                if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) { Some(pack_file_decoded.pfh_version) } else { None },
                PackedFileData::OnDisk(
                    pack_file.clone(),
                    data_position,
                    size,
                    is_compressed,
                    if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) { Some(pack_file_decoded.pfh_version) } else { None },
                )
            );

            let packed_file = PackedFile::new_from_raw(&raw_data);

            // If this is a notes PackedFile, save the notes and forget about the PackedFile. Otherwise, save the PackedFile.
            if packed_file.get_path() == ["notes.rpfm_reserved"] {
                if let Ok(data) = packed_file.get_ref_raw().get_data() {
                    if let Ok(data) = data.decode_string_u8(0, data.len()) {
                        pack_file_decoded.notes = Some(data);
                    }
                }
            }
            else {
                pack_file_decoded.packed_files.push(packed_file);
            }

            // Then we move our data position. For encrypted files in PFH5 PackFiles (only ARENA) we have to start the next one in a multiple of 8.
            if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) &&
                pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) &&
                pack_file_decoded.pfh_version == PFHVersion::PFH5 {
                let padding = 8 - (size % 8);
                let padded_size = if padding < 8 { size + padding } else { size };
                data_position += u64::from(padded_size);
            }
            else { data_position += u64::from(size); }
        }

        // If at this point we have not reached the end of the PackFile, there is something wrong with it.
        // NOTE: Arena PackFiles have extra data at the end. If we detect one of those PackFiles, take that into account.
        if pack_file_decoded.pfh_version == PFHVersion::PFH5 && pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) {
            if data_position + 256 != pack_file_len { return Err(ErrorKind::PackFileSizeIsNotWhatWeExpect(pack_file_len, data_position).into()) }
        }
        else if data_position != pack_file_len { return Err(ErrorKind::PackFileSizeIsNotWhatWeExpect(pack_file_len, data_position).into()) }

        // If we disabled lazy-loading, load every PackedFile to memory.
        if !use_lazy_loading { for packed_file in &mut pack_file_decoded.packed_files { packed_file.get_ref_mut_raw().load_data()?; }}

        // Return our PackFile.
        Ok(pack_file_decoded)
    }

    /// This function tries to save a `PackFile` to a file in the filesystem.
    ///
    /// If no path is passed, the `PackFile` will be saved in his current path.
    /// If a path is passed as `new_path` the `PackFile` will be saved in that path.
    pub fn save(&mut self, new_path: Option<PathBuf>) -> Result<()> {

        // If any of the problematic masks in the header is set or is one of CA's, return an error.
        if !self.is_editable(*SETTINGS.read().unwrap().settings_bool.get("allow_editing_of_ca_packfiles").unwrap()) { return Err(ErrorKind::PackFileIsNonEditable.into()) }

        // If we receive a new path, update it. Otherwise, ensure the file actually exists on disk.
        if let Some(path) = new_path { self.set_file_path(&path)?; }
        else if !self.get_file_path().is_file() { return Err(ErrorKind::PackFileIsNotAFile.into()) }

        // Before everything else, add the file for the notes if we have them. We'll remove it later, after the file has been saved.
        if let Some(note) = &self.notes {
            let mut data = vec![];
            data.encode_string_u8(&note);
            let raw_data = RawPackedFile::read_from_vec(vec!["notes.rpfm_reserved".to_owned()], self.get_file_name(), 0, false, data);
            let packed_file = PackedFile::new_from_raw(&raw_data);
            self.packed_files.push(packed_file);
        }

        // For some bizarre reason, if the PackedFiles are not alphabetically sorted they may or may not crash the game for particular people.
        // So, to fix it, we have to sort all the PackedFiles here by path.
        // NOTE: This sorting has to be CASE INSENSITIVE. This means for "ac", "Ab" and "aa" it'll be "aa", "Ab", "ac".
        self.packed_files.sort_unstable_by(|a, b| a.get_path().join("\\").to_lowercase().cmp(&b.get_path().join("\\").to_lowercase()));

        // We ensure that all the data is loaded and in his right form (compressed/encrypted) before attempting to save.
        // We need to do this here because we need later on their compressed size.
        for packed_file in &mut self.packed_files {

            // If we decoded it, re-encode it. Otherwise, just load it.
            packed_file.encode()?;

            // Remember: first compress (only PFH5), then encrypt.
            let (path, data, is_compressed, is_encrypted, should_be_compressed, should_be_encrypted) = packed_file.get_ref_mut_raw().get_data_and_info_from_memory()?;

            // If, in any moment, we enabled/disabled the PackFile compression, compress/decompress the PackedFile. EXCEPT FOR TABLES. NEVER COMPRESS TABLES.
            match PackedFileType::get_packed_file_type(path) {
                PackedFileType::DB | PackedFileType::Loc => *should_be_compressed = false,
                _ => {}
            }

            if *should_be_compressed && !*is_compressed {
                *data = compress_data(&data)?;
                *is_compressed = true;
            }
            else if !*should_be_compressed && *is_compressed {
                *data = decompress_data(&data)?;
                *is_compressed = false;
            }

            // Encryption is not yet supported. Unencrypt everything.
            if is_encrypted.is_some() {
                *data = decrypt_packed_file(&data);
                *is_encrypted = None;
                *should_be_encrypted = None;
            }
        }

        // First we encode the indexes and the data (just in case we compressed it).
        let mut pack_file_index = vec![];
        let mut packed_file_index = vec![];

        for pack_file in &self.pack_files {
            pack_file_index.extend_from_slice(pack_file.as_bytes());
            pack_file_index.push(0);
        }

        for packed_file in &self.packed_files {
            packed_file_index.encode_integer_u32(packed_file.get_ref_raw().get_size());

            // Depending on the version of the PackFile and his bitmask, the PackedFile index has one format or another.
            // In PFH5 case, we don't support saving encrypted PackFiles for Arena. So we'll default to Warhammer 2 format.
            match self.pfh_version {
                PFHVersion::PFH5 => {
                    if self.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { packed_file_index.encode_integer_u32(packed_file.get_ref_raw().get_timestamp() as u32); }
                    if packed_file.get_ref_raw().get_should_be_compressed() { packed_file_index.push(1); } else { packed_file_index.push(0); }
                }
                PFHVersion::PFH4 => {
                    if self.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { packed_file_index.encode_integer_u32(packed_file.get_ref_raw().get_timestamp() as u32); }
                }
                PFHVersion::PFH3 | PFHVersion::PFH2 => {
                    if self.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { packed_file_index.encode_integer_i64(packed_file.get_ref_raw().get_timestamp()); }
                }

                // This one doesn't have timestamps, so we just skip this step.
                PFHVersion::PFH0 => {}
            }

            packed_file_index.append(&mut packed_file.get_path().join("\\").as_bytes().to_vec());
            packed_file_index.push(0);
        }

        // Create the file to save to, and save the header and the indexes.
        let mut file = BufWriter::new(File::create(&self.file_path)?);

        // Write the entire header.
        let mut header = vec![];
        header.encode_string_u8(&self.pfh_version.get_value());
        header.encode_integer_u32(self.bitmask.bits | self.pfh_file_type.get_value());
        header.encode_integer_u32(self.pack_files.len() as u32);
        header.encode_integer_u32(pack_file_index.len() as u32);
        header.encode_integer_u32(self.packed_files.len() as u32);
        header.encode_integer_u32(packed_file_index.len() as u32);

        // Update the creation time, then save it. PFH0 files don't have timestamp in the headers.
        self.timestamp = get_current_time();
        match self.pfh_version {
            PFHVersion::PFH5 | PFHVersion::PFH4 => header.encode_integer_u32(self.timestamp as u32),
            PFHVersion::PFH3 | PFHVersion::PFH2 => header.encode_integer_i64((self.timestamp + SEC_TO_UNIX_EPOCH) * WINDOWS_TICK),
            PFHVersion::PFH0 => {}
        };

        // Write the indexes and the data of the PackedFiles. No need to keep the data, as it has been preloaded before.
        file.write_all(&header)?;
        file.write_all(&pack_file_index)?;
        file.write_all(&packed_file_index)?;
        for packed_file in &self.packed_files {
            let data = packed_file.get_ref_raw().get_raw_data()?;
            file.write_all(&data)?;
        }

        // Remove again the notes PackedFile, as that one is stored separated from the rest.
        self.remove_packed_file_by_path(&["notes.rpfm_reserved".to_owned()]);

        // If nothing has failed, return success.
        Ok(())
    }
}

/// Implementaion of trait `Default` for `PackFile`.
impl Default for PackFile {

    /// This function creates a new empty `PackFile`.
    ///
    /// In reality, this just calls the `new()` function. It's just here for completeness.
    fn default() -> Self {
        Self::new()
    }
}

/// Implementation to create a `PackFileInfo` from a `PackFile`.
impl From<&PackFile> for PackFileInfo {
    fn from(packfile: &PackFile) -> Self {
        Self {
            file_name: packfile.get_file_name(),
            file_path: packfile.file_path.to_path_buf(),
            pfh_version: packfile.pfh_version,
            pfh_file_type: packfile.pfh_file_type,
            bitmask: packfile.bitmask,
            timestamp: packfile.timestamp,
            compression_state: packfile.get_compression_state(),
        }
    }
}

/// Implementation of `Manifest`.
impl Manifest {

    /// This function returns a parsed version of the `manifest.txt` of the Game Selected, if exists and is parseable.
    pub fn read_from_game_selected() -> Result<Self> {
        let mut manifest_path = get_game_selected_data_path().ok_or_else(|| ErrorKind::GameSelectedPathNotCorrectlyConfigured)?;
        manifest_path.push("manifest.txt");

        let mut reader = ReaderBuilder::new()
            .delimiter(b'\t')
            .quoting(false)
            .has_headers(false)
            .from_path(&manifest_path)?;

        // If we succesfully load the TSV file into a reader, check the first two lines to ensure
        // it's a valid TSV for our specific table.
        let entries = reader.deserialize().filter_map(|x| x.ok()).collect::<Vec<ManifestEntry>>();
        let manifest = Self(entries);
        Ok(manifest)
    }
}

//---------------------------------------------------------------------------//
// Copyright (c) 2017-2022 Ismael Gutiérrez González. All rights reserved.
//
// This file is part of the Rusted PackFile Manager (RPFM) project,
// which can be found here: https://github.com/Frodo45127/rpfm.
//
// This file is licensed under the MIT license, which can be found here:
// https://github.com/Frodo45127/rpfm/blob/master/LICENSE.
//---------------------------------------------------------------------------//

/*!
Module with all the code to interact with PackFiles.

This module contains all the code related with PackFiles. If you want to do anything with a PackFile,
this is the place you have to come.

Also, something to take into account. RPFM supports PackFile compression/decompression and decryption,
and that is handled automagically by RPFM. All the data you'll ever see will be decompressed/decrypted,
so you don't have to worry about that.
!*/

use bitflags::bitflags;
use csv::ReaderBuilder;
//use itertools::{Itertools, Either};
use serde_derive::{Serialize, Deserialize};
//use serde_json::{from_slice, to_string_pretty};
use rayon::prelude::*;
//use unicase::UniCase;

use std::collections::{BTreeMap, HashMap, HashSet};
use std::{fmt, fmt::Display};
use std::fs::{DirBuilder, File};
use std::io::{prelude::*, BufReader, BufWriter, SeekFrom, Read, Write};
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};

use getset::*;
use crate::{compression::*, binary::{decoder::Decoder, encoder::Encoder}, schema::Schema, utils::*};
use crate::games::pfh_version::PFHVersion;
use crate::games::pfh_file_type::PFHFileType;

use crate::files::RFile;

use crate::error::{RLibError, Result};
use crate::files::FileType;
use crate::files::Decodeable;
use crate::files::Container;
//use crate::files::table::DecodedData;
//use crate::files::db::DB;
//use crate::files::loc::{Loc, TSV_NAME_LOC};
use crate::files::text::TextType;

//use crate::GAME_SELECTED;
//use crate::SCHEMA;
//use crate::SETTINGS;
//use crate::dependencies::Dependencies;


//pub mod packedfile;

#[cfg(test)]
mod packfile_test;

/// These are the different Preamble/Id the PackFiles can have.
const MFH_PREAMBLE: &str = "MFH"; // Weird format of some packs downloaded from Steam.

/// This one is the path in a `PackFile` where all the maps generated by Terry end up.
const TERRY_MAP_PATH: [&str; 4] = ["terrain", "tiles", "battle", "_assembly_kit"];

/// This one is the name of the main BMD data file used by maps exported from Terry.
const DEFAULT_BMD_DATA: &str = "bmd_data.bin";

/// These three hints are necessary for the map patching function.
const FORT_PERIMETER_HINT: &[u8; 18] = b"AIH_FORT_PERIMETER";
const DEFENSIVE_HILL_HINT: &[u8; 18] = b"AIH_DEFENSIVE_HILL";
const SIEGE_AREA_NODE_HINT: &[u8; 19] = b"AIH_SIEGE_AREA_NODE";

pub const RESERVED_NAME_DEPENDENCIES_MANAGER: &str = "dependencies_manager.rpfm_reserved";
pub const RESERVED_NAME_EXTRA_PACKFILE: &str = "extra_packfile.rpfm_reserved";
pub const RESERVED_NAME_SETTINGS: &str = "settings.rpfm_reserved";
pub const RESERVED_NAME_NOTES: &str = "notes.rpfm_reserved";

/// This is the list of ***Reserved PackedFile Names***. They're packedfile names used by RPFM for special purposes.
pub const RESERVED_PACKED_FILE_NAMES: [&str; 3] = [RESERVED_NAME_EXTRA_PACKFILE, RESERVED_NAME_SETTINGS, RESERVED_NAME_NOTES];

const SUBHEADER_MARK: u32 = 0x12345678;
const SUBHEADER_VERSION: u32 = 1;

const AUTHORING_TOOL_CA: &str = "CA_TOOL";
const AUTHORING_TOOL_RPFM: &str = "RPFM";
const AUTHORING_TOOL_SIZE: u32 = 8;

bitflags! {

    /// This represents the bitmasks a PackFile can have applied to his type.
    ///
    /// Keep in mind that this lib supports decoding PackFiles with any of these flags enabled,
    /// but it only supports enconding for the `HAS_INDEX_WITH_TIMESTAMPS` flag.
    pub struct PFHFlags: u32 {

        /// Used to specify that the header of the PackFile is extended by 20 bytes. Used in Arena.
        const HAS_EXTENDED_HEADER       = 0b0000_0001_0000_0000;

        /// Used to specify that the PackedFile Index is encrypted. Used in Arena.
        const HAS_ENCRYPTED_INDEX       = 0b0000_0000_1000_0000;

        /// Used to specify that the PackedFile Index contains a timestamp of every PackFile.
        const HAS_INDEX_WITH_TIMESTAMPS = 0b0000_0000_0100_0000;

        /// Used to specify that the PackedFile's data is encrypted. Seen in `music.pack` PackFiles and in Arena.
        const HAS_ENCRYPTED_DATA        = 0b0000_0000_0001_0000;
    }
}

//---------------------------------------------------------------------------//
//                              Enum & Structs
//---------------------------------------------------------------------------//

/// This `Struct` stores the data of the PackFile in memory, along with some extra data needed to manipulate the PackFile.
#[derive(Debug, Clone, PartialEq)]
pub struct Pack<T: Decodeable> {

    /// The path of the PackFile on disk, if exists. If not, then this should be empty.
    file_path: PathBuf,

    header: PackHeader,

    /// The list of PackFiles this PackFile requires to be loaded before himself when starting the game.
    ///
    /// In other places, we refer to this as the `Dependency List`.
    dependencies: Vec<String>,

    /// The list of PackedFiles this PackFile contains.
    files: HashMap<String, RFile<T>>,

    /// Notes added to the PackFile. Exclusive of this lib.
    notes: Option<String>,

    /// Settings stored in the PackFile itself, to be able to share them between installations.
    settings: PackFileSettings,
}

#[derive(Debug, Clone, PartialEq)]
pub struct PackHeader {

    /// The version of the PackFile.
    pfh_version: PFHVersion,

    /// The type of the PackFile.
    pfh_file_type: PFHFileType,

    /// The bitmasks applied to the PackFile.
    bitmask: PFHFlags,

    /// The timestamp of the last time the PackFile was saved.
    timestamp: i64,

    /// Game version this mod is intended for. This usually triggers the "outdated mod" warning in the launcher if it doesn't match the current exe version.
    game_version: u32,

    /// Build number of the game.
    build_number: u32,

    /// Tool that created the PackFile. Max 8 characters, 00-padded.
    authoring_tool: String,

    /// Extra subheader data, in case it's used in the future.
    extra_subheader_data: Vec<u8>,
}

/// This struct is a reduced version of the `PackFile` one, used to pass just the needed data to an UI.
///
/// Don't create this one manually. Get it `From` the `PackFile` one, and use it as you need it.
#[derive(Clone, Debug, Default)]
pub struct PackFileInfo {

    /// The name of the PackFile's file, if exists. If not, then this should be empty.
    pub file_name: String,

    /// The path of the PackFile on disk, if exists. If not, then this should be empty.
    pub file_path: PathBuf,

    /// The version of the PackFile.
    pub pfh_version: PFHVersion,

    /// The type of the PackFile.
    pub pfh_file_type: PFHFileType,

    /// The bitmasks applied to the PackFile.
    pub bitmask: PFHFlags,

    /// The current state of the compression inside the PackFile.
    pub compression_state: bool,

    /// The timestamp of the last time the PackFile was saved.
    pub timestamp: i64,
}

/// This struct hold PackFile-specific settings.
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub struct PackFileSettings {

    /// For multi-line text.
    pub settings_text: BTreeMap<String, String>,

    /// For single-line text.
    pub settings_string: BTreeMap<String, String>,

    /// For bool values.
    pub settings_bool: BTreeMap<String, bool>,

    /// For integer values.
    pub settings_number: BTreeMap<String, i32>,
}

//---------------------------------------------------------------------------//
//                           Structs Implementations
//---------------------------------------------------------------------------//

impl<T: Decodeable> Container<T> for Pack<T> {
    fn files(&self) -> &HashMap<std::string::String, RFile<T>> {
        &self.files
    }

    fn files_mut(&mut self) -> &mut HashMap<std::string::String, RFile<T>> {
        &mut self.files
    }
}

impl<T: Decodeable> Decodeable for Pack<T> {
    fn file_type(&self) -> FileType {
        FileType::Pack
    }

    fn decode(data: &[u8], extra_data: Option<(&Schema, &str, bool)>) -> Result<Self> where Self: Sized {

        todo!()
    }
}
/*
impl<T: Decodeable> Pack<T> {
    pub fn read(
        file_path: &Path,
        use_lazy_loading: bool
    ) -> Result<Self> {

        // Check if what we received is even a `PackFile`.
        if !file_path.file_name().unwrap().to_string_lossy().to_string().ends_with(".pack") { return Err(ErrorKind::OpenPackFileInvalidExtension.into()) }

        // Prepare the PackFile to be read and the virtual PackFile to be written.
        let mut pack_file = BufReader::new(File::open(&file_path)?);
        let pack_file_name = file_path.file_name().unwrap().to_string_lossy().to_string();
        let mut pack_file_decoded = Self::new();

        // First, we do some quick checkings to ensure it's a valid PackFile.
        // 24 is the bare minimum that we need to check how a PackFile should be internally, so any file with less than that is not a valid PackFile.
        let pack_file_len = pack_file.get_ref().metadata()?.len();
        if pack_file_len < 24 { return Err(ErrorKind::PackFileHeaderNotComplete.into()) }

        // Check if it has the weird steam-only header, and skip it if found.
        let mut buffer = vec![0; 8];
        pack_file.read_exact(&mut buffer)?;
        let start_index = if buffer.decode_string_u8(0, 3)? == MFH_PREAMBLE { 8 } else { 0 };

        // Create a little buffer to read the basic data from the header of the PackFile.
        let mut buffer = vec![0; 24];
        pack_file.seek(SeekFrom::Start(start_index))?;
        pack_file.read_exact(&mut buffer)?;

        // Start populating our decoded PackFile struct.
        pack_file_decoded.file_path = file_path.to_path_buf();
        pack_file_decoded.pfh_version = PFHVersion::get_version(&buffer.decode_string_u8(0, 4)?)?;
        pack_file_decoded.pfh_file_type = PFHFileType::get_type(buffer.decode_integer_u32(4)? & 15);
        pack_file_decoded.bitmask = PFHFlags::from_bits_truncate(buffer.decode_integer_u32(4)? & !15);

        // Read the data about the indexes to use it later.
        let pack_file_count = buffer.decode_integer_u32(8)?;
        let pack_file_index_size = buffer.decode_integer_u32(12)?;
        let packed_file_count = buffer.decode_integer_u32(16)?;
        let packed_file_index_size = buffer.decode_integer_u32(20)?;

        // Depending on the data we got, prepare to read the header and ensure we have all the bytes we need.
        match pack_file_decoded.pfh_version {

            // PFH6 contains a subheader with some extra data we want to keep.
            PFHVersion::PFH6 => buffer = vec![0; 308],

            PFHVersion::PFH5 | PFHVersion::PFH4 => {
                if (pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) && pack_file_len < 48) ||
                    (!pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) && pack_file_len < 28) { return Err(ErrorKind::PackFileHeaderNotComplete.into()) }

                if pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) { buffer = vec![0; 48]; }
                else { buffer = vec![0; 28]; }
            }

            PFHVersion::PFH3 | PFHVersion::PFH2 => buffer = vec![0; 32],
            PFHVersion::PFH0 => buffer = vec![0; 24],
        }

        // Restore the cursor of the BufReader to 0, so we can read the full header in one go. The first 24 bytes are
        // already decoded but, for the sake of clarity in the positions of the rest of the header stuff, we do this.
        pack_file.seek(SeekFrom::Start(start_index))?;
        pack_file.read_exact(&mut buffer)?;

        // The creation time is a bit of an asshole. Depending on the PackFile Version/Id/Preamble, it uses a type, another or it doesn't exists.
        // Keep in mind that we store his raw value. If you want his legible value, you have to convert it yourself. PFH0 doesn't have it.
        pack_file_decoded.timestamp = match pack_file_decoded.pfh_version {
            PFHVersion::PFH6 | PFHVersion::PFH5 | PFHVersion::PFH4 => i64::from(buffer.decode_integer_u32(24)?),
            PFHVersion::PFH3 | PFHVersion::PFH2 => (buffer.decode_integer_i64(24)? / WINDOWS_TICK) - SEC_TO_UNIX_EPOCH,
            PFHVersion::PFH0 => 0
        };

        if let PFHVersion::PFH6 = pack_file_decoded.pfh_version {
            pack_file_decoded.game_version = buffer.decode_integer_u32(36)?;
            pack_file_decoded.build_number = buffer.decode_integer_u32(40)?;
            pack_file_decoded.authoring_tool = buffer.decode_string_u8_0padded(44, AUTHORING_TOOL_SIZE as usize)?.0;
            pack_file_decoded.extra_subheader_data = buffer[52..].to_vec();
        }

        // Ensure the PackFile has all the data needed for the index. If the PackFile's data is encrypted
        // and the PackFile is PFH5, due to how the encryption works, the data should start in a multiple of 8.
        let mut data_position = u64::from(start_index as u32 + buffer.len() as u32 + pack_file_index_size + packed_file_index_size);
        if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) &&
            pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) &&
            pack_file_decoded.pfh_version == PFHVersion::PFH5 {
            data_position = if (data_position % 8) > 0 { data_position + 8 - (data_position % 8) } else { data_position };
        }
        if pack_file_len < data_position { return Err(ErrorKind::PackFileIndexesNotComplete.into()) }

        // Create the buffers for the indexes data.
        let mut pack_file_index = vec![0; pack_file_index_size as usize];
        let mut packed_file_index = vec![0; packed_file_index_size as usize];

        // Get the data from both indexes to their buffers.
        pack_file.read_exact(&mut pack_file_index)?;
        pack_file.read_exact(&mut packed_file_index)?;

        // Read the PackFile Index.
        let mut pack_file_index_position: usize = 0;

        // First, we decode every entry in the PackFile index and store it. It's encoded in StringU8 terminated in 00,
        // so we just read them char by char until hitting 0, then decode the next one and so on.
        // NOTE: This doesn't deal with encryption, as we haven't seen any encrypted PackFile with data in this index.
        for _ in 0..pack_file_count {
            let pack_file_name = pack_file_index.decode_packedfile_string_u8_0terminated(pack_file_index_position, &mut pack_file_index_position)?;
            pack_file_decoded.pack_files.push(pack_file_name);
        }

        // Depending on the version of the PackFile and his bitmask, the PackedFile index has one format or another.
        let packed_file_index_path_offset = match pack_file_decoded.pfh_version {
            PFHVersion::PFH6 | PFHVersion::PFH5 => {

                // If it has the extended header bit, is an Arena PackFile. These ones use a normal PFH4 index format for some reason.
                if pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) {
                    if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 8 } else { 4 }
                }

                // Otherwise, it's a Warhammer 2 PackFile. These ones have 4 bytes for the size, 4 for the timestamp and 1 for the compression.
                else if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 9 } else { 5 }
            }

            // If it has the last modified date of the PackedFiles, we default to 8. Otherwise, we default to 4.
            PFHVersion::PFH4 => if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 8 } else { 4 }

            // These are like PFH4, but the timestamp has 8 bytes instead of 4.
            PFHVersion::PFH3 | PFHVersion::PFH2 => if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 12 } else { 4 }

            // There isn't seem to be a bitmask in ANY PFH0 PackFile, so we will assume they didn't even use it back then.
            PFHVersion::PFH0 => 4
        };

        // Prepare the needed stuff to read the PackedFiles.
        let mut index_position: usize = 0;
        let pack_file = Arc::new(Mutex::new(pack_file));
        for packed_files_to_decode in (0..packed_file_count).rev() {

            // Get his size. If it's encrypted, decrypt it first.
            let size = if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) {
                let encrypted_size = packed_file_index.decode_integer_u32(index_position)?;
                decrypt_index_item_file_length(encrypted_size, packed_files_to_decode as u32)
            } else {
                packed_file_index.decode_integer_u32(index_position)?
            };

            // If we have the last modified date of the PackedFiles in the Index, get it. Otherwise, default to 0,
            // so we have something to write in case we want to enable them for our PackFile.
            let timestamp = if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) {
                match pack_file_decoded.pfh_version {
                    PFHVersion::PFH6 | PFHVersion::PFH5 | PFHVersion::PFH4 => {
                        let timestamp = i64::from(packed_file_index.decode_integer_u32(index_position + 4)?);
                        if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) {
                            i64::from(decrypt_index_item_file_length(timestamp as u32, packed_files_to_decode as u32))
                        } else { timestamp }
                    }

                    // We haven't found a single encrypted PFH3/PFH0 PackFile to test, so always assume these are unencrypted. Also, PFH0 doesn't seem to have a timestamp.
                    PFHVersion::PFH3 | PFHVersion::PFH2 => (packed_file_index.decode_integer_i64(index_position + 4)? / WINDOWS_TICK) - SEC_TO_UNIX_EPOCH,
                    PFHVersion::PFH0 => 0,
                }
            } else { 0 };

            // Update his offset, and get his compression data if it has it.
            index_position += packed_file_index_path_offset;
            let is_compressed = if let PFHVersion::PFH5 = pack_file_decoded.pfh_version {
                matches!(packed_file_index.decode_bool(index_position - 1), Ok(true))
            } else { false };

            // Get his path. Like the PackFile index, it's a StringU8 terminated in 00. We get it and split it in folders for easy use.
            let path = if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) {
                decrypt_index_item_filename(&packed_file_index[index_position..], size as u8, &mut index_position)
            }
            else { packed_file_index.decode_packedfile_string_u8_0terminated(index_position, &mut index_position)? };
            let path = path.split('\\').map(|x| x.to_owned()).collect::<Vec<String>>();

            // Once we are done, we create the PackedFile and add it to the PackedFile list.
            let raw_data = RawPackedFile::read_from_data(
                path,
                pack_file_name.to_string(),
                timestamp,
                is_compressed,
                if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) { Some(pack_file_decoded.pfh_version) } else { None },
                PackedFileData::OnDisk(RawOnDisk::new(
                    pack_file.clone(),
                    data_position,
                    size,
                    is_compressed,
                    if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) { Some(pack_file_decoded.pfh_version) } else { None },
                ))
            );

            let mut packed_file = PackedFile::new_from_raw(&raw_data);

            // If this is a notes PackedFile, save the notes and forget about the PackedFile. Otherwise, save the PackedFile.
            if packed_file.get_path() == [RESERVED_NAME_NOTES] {
                if let Ok(data) = packed_file.get_raw_data_and_keep_it() {
                    if let Ok(data) = data.decode_string_u8(0, data.len()) {
                        pack_file_decoded.notes = Some(data);
                    }
                }
            }

            else if packed_file.get_path() == [RESERVED_NAME_SETTINGS] {
                if let Ok(data) = packed_file.get_raw_data_and_keep_it() {
                    pack_file_decoded.settings = if let Ok(settings) = PackFileSettings::load(&data) {
                        settings
                    } else {
                        PackFileSettings::default()
                    };
                }
            }
            else {
                pack_file_decoded.packed_files.push(packed_file);
            }

            // Then we move our data position. For encrypted files in PFH5 PackFiles (only ARENA) we have to start the next one in a multiple of 8.
            if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) &&
                pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) &&
                pack_file_decoded.pfh_version == PFHVersion::PFH5 {
                let padding = 8 - (size % 8);
                let padded_size = if padding < 8 { size + padding } else { size };
                data_position += u64::from(padded_size);
            }
            else { data_position += u64::from(size); }
        }

        // If at this point we have not reached the end of the PackFile, there is something wrong with it.
        // NOTE: Arena PackFiles have extra data at the end. If we detect one of those PackFiles, take that into account.
        if pack_file_decoded.pfh_version == PFHVersion::PFH5 && pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) {
            if data_position + 256 != pack_file_len { return Err(ErrorKind::PackFileSizeIsNotWhatWeExpect(pack_file_len, data_position).into()) }
        }
        else if data_position != pack_file_len { return Err(ErrorKind::PackFileSizeIsNotWhatWeExpect(pack_file_len, data_position).into()) }

        // If we disabled lazy-loading, load every PackedFile to memory.
        if !use_lazy_loading { for packed_file in &mut pack_file_decoded.packed_files { packed_file.get_ref_mut_raw().load_data()?; }}

        // Return our PackFile.
        Ok(pack_file_decoded)
    }
}*/
//---------------------------------------------------------------------------//
//                             Enum Implementations
//---------------------------------------------------------------------------//
/*

/// Implementation of `PathType`.
impl PathType {

    /// This function removes collided items from the provided list of `PathType`.
    ///
    /// This means, if you have an item of type `PackFile` it removes the rest of the items.
    /// If you have a file and a folder containing the file, it removes the file. And so on.
    ///
    /// NOTE: It DOES NOT remove duplicated PathTypes. This means, if you pass it a slice with the same PathType duplicated,
    /// it'll give it you back as it came. This is for removing collided PathTypes, not duplicated ones!!!!
    pub fn dedup(path_types: &[Self]) -> Vec<Self> {
        let mut path_types = path_types.to_vec();

        // As this operation can get very expensive very fast, we first check if we have a PackFile.
        // If so, we just leave the PackFile and remove everything else.
        let we_have_packfile = path_types.par_iter().find_any(|item| {
            matches!(item, PathType::PackFile)
        });

        match we_have_packfile {
            Some(path_type) => path_types = vec![path_type.clone(); 1],

            // If we don't have a PackFile, we check the rest of the items (files and folders).
            None => {
                let we_have_folder = path_types.par_iter().any(|item| {
                    matches!(item, PathType::Folder(_))
                });

                // If we don't have a folder, we assume the entire set are unique files.
                // Otherwise, we proceed to do the expensive dedup operation.
                if we_have_folder {
                    let items_to_remove = path_types.par_iter().filter(|item_type_to_add| {
                        match item_type_to_add {

                            // If it's a file, we have to check if there is a folder containing it.
                            PathType::File(ref path_to_add) => {
                                path_types.par_iter().filter(|x| {
                                    !matches!(x, PathType::File(_))
                                }).any(|item_type| {

                                    // If the other one is a folder that contains it, dont add it.
                                    if let PathType::Folder(ref path) = item_type {
                                        path_to_add.starts_with(path)
                                    } else { false }
                                })
                            }

                            // If it's a folder, we have to check if there is already another folder containing it.
                            PathType::Folder(ref path_to_add) => {
                                path_types.par_iter().filter(|x| {
                                    !matches!(x, PathType::File(_))
                                }).any(|item_type| {

                                    // If the other one is a folder that contains it, dont add it.
                                    if let PathType::Folder(ref path) = item_type {
                                        path_to_add.starts_with(path) && path_to_add.len() > path.len()
                                    } else { false }
                                })
                            }

                            // If we receive one of these... better start praying.
                            _ => unimplemented!(),
                        }
                    }).cloned().collect::<Vec<PathType>>();

                    // Remove the duplicated items.
                    items_to_remove.iter().for_each(|x| {
                        let index = path_types.iter().position(|y| x == y).unwrap();
                        path_types.remove(index);
                    });
                }
            }
        }

        path_types
    }
}

//---------------------------------------------------------------------------//
//                           Structs Implementations
//---------------------------------------------------------------------------//

/// Implementation of `PackFile`.
impl PackFile {

    /// This function creates a new empty `PackFile`. This is used for creating a *dummy* PackFile we'll later populate.
    pub fn new() -> Self {
        Self {
            file_path: PathBuf::new(),
            pfh_version: PFHVersion::PFH5,
            pfh_file_type: PFHFileType::Mod,
            bitmask: PFHFlags::empty(),
            timestamp: 0,
            game_version: 0,
            build_number: 0,
            authoring_tool: AUTHORING_TOOL_RPFM.to_owned(),
            extra_subheader_data: vec![0; 256],

            pack_files: vec![],
            packed_files: vec![],

            notes: None,
            settings: PackFileSettings::default(),
        }
    }

    /// This function creates a new empty `PackFile` with a name and a specific `PFHVersion`.
    pub fn new_with_name(file_name: &str, pfh_version: PFHVersion) -> Self {
        Self {
            file_path: PathBuf::from(file_name),
            pfh_version,
            pfh_file_type: PFHFileType::Mod,
            bitmask: PFHFlags::empty(),
            timestamp: 0,
            game_version: 0,
            build_number: 0,
            authoring_tool: AUTHORING_TOOL_RPFM.to_owned(),
            extra_subheader_data: vec![0; 256],

            pack_files: vec![],
            packed_files: vec![],

            notes: None,
            settings: PackFileSettings::default(),
        }
    }

    /// This function returns a list of reserved PackedFile names, used by RPFM for special purposes.
    pub fn get_reserved_packed_file_names() -> Vec<Vec<String>> {
        RESERVED_PACKED_FILE_NAMES.iter().map(|x| vec![(*x).to_string()]).collect()
    }

    /// This function returns the path where maps end up after being processed by Terry and put in a `PackFile`.
    pub fn get_terry_map_path() -> Vec<String> {
        TERRY_MAP_PATH.iter().map(|x| (*x).to_string()).collect()
    }

    /// This function returns the game version of this PackFile.
    pub fn get_game_version(&self) -> u32 {
        self.game_version
    }

    /// This function allows you to change the game version of this PackFile.
    pub fn set_game_version(&mut self, version: u32) {
        self.game_version = version;
    }

    /// This function returns the authoring tool used to make this PackFile initially.
    pub fn get_authoring_tool(&self) -> &str {
        &self.authoring_tool
    }

    /// This function allows you to change the authoring tool used to make this PackFile.
    ///
    /// This has a character limit, and will fail if you pass a string longer than that.
    pub fn set_authoring_tool(&mut self, authoring_tool: &str) -> Result<()> {
        if authoring_tool.len() > AUTHORING_TOOL_SIZE as usize {
            return Err(ErrorKind::StringTooLong(AUTHORING_TOOL_SIZE).into());
        }
        self.authoring_tool = authoring_tool.to_owned();
        Ok(())
    }

    /// This function returns the `PackFile List` of the provided `PackFile`.
    pub fn get_packfiles_list(&self) -> &[String] {
        &self.pack_files
    }

    /// This function replaces the `PackFile List` of our `PackFile` with the provided one.
    pub fn set_packfiles_list(&mut self, pack_files: &[String]) {
        self.pack_files = pack_files.to_vec();
    }

    /// This function returns the list of PackedFiles inside a `PackFile`.
    pub fn get_packedfiles_list(&self) -> Vec<Vec<String>> {
        self.packed_files.par_iter().map(|x| x.get_path().to_vec()).collect()
    }

    /// This function adds a `PackedFile` to an existing `PackFile`.
    ///
    /// This function returns the path of the `PackedFile` which got added successfully. Also, if you set `overwrite` to `true`,
    /// in case of conflict, the `PackedFile` is overwritten. If set to false, it'll be renamed instead.
    ///
    /// This is a convenience function to add just one PackedFile to our PackFile.
    pub fn add_packed_file(&mut self, packed_file: &PackedFile, overwrite: bool) -> Result<Vec<String>> {
        self.add_packed_files(&[packed_file], overwrite, true).map(|x| x[0].to_vec())
    }

    /// This function adds one or more `PackedFiles` to an existing `PackFile`.
    ///
    /// This function returns the paths of the `PackedFiles` which got added successfully, which should be all. Also, if you set `overwrite` to `true`,
    /// in case of conflict the destination `PackedFiles`, if exists, are overwritten. If set to false, they'll be renamed instead.
    ///
    /// NOTE: This assumes the paths of the list of PackedFiles you pass it are unique among themselves. It'll do weird things otherwise.
    pub fn add_packed_files(&mut self, packed_files: &[&PackedFile], overwrite: bool, update_packfile_name: bool) -> Result<Vec<Vec<String>>> {

        // If we hit a reserved name, stop. Don't add anything.
        let pack_file_name = self.get_file_name();
        let reserved_names = Self::get_reserved_packed_file_names();
        if packed_files.par_iter().any(|x| reserved_names.iter().any(|y| x.get_path() == &**y)) { return Err(ErrorKind::ReservedFiles.into()) }

        // Prepare the list of added paths and get all the PackedFiles with all the info needed for them to be added.
        let mut destination_paths = Vec::with_capacity(packed_files.len());
        let packed_files = packed_files.par_iter()
            .map(|x| (x.get_path(), *x,
                self.packed_files.par_iter().position_any(|y| x.get_path() == y.get_path())
            )).collect::<Vec<(&[String], &PackedFile, Option<usize>)>>();

        // Get all the PackedFiles that are not in conflict with the ones we already have in our PackFile,
        // prepare them to be added, then add them all at once.
        let mut packed_files_new = packed_files.par_iter()
            .filter(|(_, _, position)| position.is_none())
            .map(|(_, packed_file, _)| {
                let mut packed_file = (*packed_file).clone();
                if update_packfile_name {
                    packed_file.get_ref_mut_raw().set_packfile_name(&pack_file_name);
                }
                packed_file
            })
            .collect::<Vec<PackedFile>>();

        // Prepare the paths of the no-conflict PackedFiles to be returned.
        destination_paths.append(&mut packed_files_new.par_iter()
            .map(|packed_file| packed_file.get_path().to_vec())
            .collect::<Vec<Vec<String>>>());
        self.packed_files.append(&mut packed_files_new);


        // Now we deal with the problematic ones. If we set them to overwrite the conflicting files...we just replace them.
        if overwrite {
            let packed_files_conflict = packed_files.par_iter()
                .filter(|(_, _, position)| position.is_some())
                .map(|(_, packed_file, position)| (*packed_file, position.unwrap()))
                .collect::<Vec<(&PackedFile, usize)>>();

            packed_files_conflict.iter().for_each(|(packed_file, position)| {
                self.packed_files[*position] = (*packed_file).clone();
            });

            destination_paths.append(&mut packed_files_conflict.par_iter()
                .map(|(packed_file, _)| packed_file.get_path().to_vec())
                .collect::<Vec<Vec<String>>>());
        }

        // If not, then we need to do some path testing to rename them to something that doesn't match any other PackedFile we have.
        else {
            let mut packed_files_conflict = packed_files.par_iter()
                .filter(|(_, _, position)| position.is_some())
                .map(|(path, packed_file, _)| {
                    let mut path = path.to_vec();
                    let mut packed_file = (*packed_file).clone();
                    if update_packfile_name {
                        packed_file.get_ref_mut_raw().set_packfile_name(&pack_file_name);
                    }

                    let name_current = path.last().unwrap().to_owned();
                    let name_split = name_current.split('.').collect::<Vec<&str>>();
                    let name = name_split[0];
                    let extension = if name_split.len() > 1 { name_split[1..].join(".") } else { "".to_owned() };
                    for number in 0.. {
                        let name = if extension.is_empty() { format!("{}_{}", name, number) } else { format!("{}_{}.{}", name, number, extension) };
                        *path.last_mut().unwrap() = name;
                        if !self.packedfile_exists(&path) && !reserved_names.contains(&path) {

                            // Ignorable result. This will never fail due to the replacing code before this.
                            let _ = packed_file.get_ref_mut_raw().set_path(&path);
                            break;
                        }
                    }
                    packed_file
                })
                .collect::<Vec<PackedFile>>();

            destination_paths.append(&mut packed_files_conflict.par_iter()
                .map(|packed_file| packed_file.get_path().to_vec())
                .collect::<Vec<Vec<String>>>());
            self.packed_files.append(&mut packed_files_conflict);
        }

        Ok(destination_paths)
    }

    /// This function is used to add a file from disk to a `PackFile`, turning it into a `PackedFile`.
    ///
    /// In case of conflict, if overwrite is set to true, the current `PackedFile` in the conflicting path
    /// will be overwritten with the new one. If set to false, the new `PackFile` will be called `xxxx_1.extension`.
    pub fn add_from_file(
        &mut self,
        path_as_file: &Path,
        mut path_as_packed_file: Vec<String>,
        overwrite: bool,
        import_tables_from_tsv: bool,
    ) -> Result<Vec<String>> {
        let raw_data = if import_tables_from_tsv {

            // If it's a tsv, try to import it as a table. If it fails... import it as a normal file.
            if let Some(extension) = path_as_file.extension() {
                if extension == "tsv" {
                    match &*SCHEMA.read().unwrap() {
                        Some(schema) => {

                            // Get the file type/table name.
                            let mut file = BufReader::new(File::open(path_as_file)?);
                            let mut first_row = String::new();
                            let mut second_row = String::new();
                            file.read_line(&mut first_row)?;
                            file.read_line(&mut second_row)?;

                            let second_row_split = second_row.split('\t').collect::<Vec<&str>>();
                            match second_row_split.get(0) {
                                Some(table_type) => {

                                    // If we have at least 2 fields, use the legacy behavior.
                                    let has_legacy_structure = if let Some(table_type) = second_row_split.get(1) { table_type != &"" } else { false };
                                    let mut table_type = if has_legacy_structure { table_type.to_string() } else { table_type.split(';').collect::<Vec<&str>>()[0].to_owned() };

                                    if table_type.starts_with("#") {
                                        table_type.remove(0);
                                    }

                                    // Act depending on the type.
                                    if &table_type == TSV_NAME_LOC {
                                        let (table, _) = Loc::import_tsv(schema, path_as_file)?;
                                        let raw_data = table.save()?;
                                        let mut packed_file_name = path_as_packed_file.last().unwrap().to_string();
                                        let packed_file_name_len = packed_file_name.chars().count();
                                        if packed_file_name_len >= 4 {
                                            packed_file_name.drain(packed_file_name_len - 4..packed_file_name_len);
                                        }
                                        *path_as_packed_file.last_mut().unwrap() = packed_file_name.to_owned() + ".loc";
                                        RawPackedFile::read_from_vec(path_as_packed_file, packed_file_name, 0, false, raw_data)
                                    } else {
                                        let (table, _) = DB::import_tsv(schema, path_as_file)?;
                                        let raw_data = table.save()?;
                                        let mut packed_file_name = path_as_packed_file.last().unwrap().to_string();
                                        let packed_file_name_len = packed_file_name.chars().count();
                                        if packed_file_name_len >= 4 {
                                            packed_file_name.drain(packed_file_name_len - 4..packed_file_name_len);
                                        }
                                        *path_as_packed_file.last_mut().unwrap() = packed_file_name.to_owned();
                                        RawPackedFile::read_from_vec(path_as_packed_file, packed_file_name, 0, false, raw_data)
                                    }
                                }
                                None => RawPackedFile::read_from_path(path_as_file, path_as_packed_file)?,
                            }
                        }
                        None => return Err(ErrorKind::SchemaNotFound.into())
                    }
                } else if extension == "json" || extension == "md" {
                    if let Some(file_name) = path_as_file.file_name() {
                        let settings_name = RESERVED_NAME_SETTINGS.to_owned() + ".json";
                        let notes_name = RESERVED_NAME_NOTES.to_owned() + ".md";
                        let file_name = file_name.to_string_lossy().to_string();
                        if file_name == settings_name {
                            let mut file = BufReader::new(File::open(path_as_file)?);
                            let mut data = vec![];
                            file.read_to_end(&mut data)?;
                            let settings = PackFileSettings::load(&data)?;
                            self.set_settings(&settings);
                            return Ok(vec![]);
                        } else if file_name == notes_name {
                            let mut file = BufReader::new(File::open(path_as_file)?);
                            let mut data = String::new();
                            file.read_to_string(&mut data)?;
                            self.set_notes(&Some(data));
                            return Ok(vec![]);
                        }
                        else {
                            RawPackedFile::read_from_path(path_as_file, path_as_packed_file)?
                        }
                    } else {
                        RawPackedFile::read_from_path(path_as_file, path_as_packed_file)?
                    }
                } else {
                    RawPackedFile::read_from_path(path_as_file, path_as_packed_file)?
                }
            }

            // If it's a normal file, import it as a normal file.
            else {
                RawPackedFile::read_from_path(path_as_file, path_as_packed_file)?
            }
        } else {
            RawPackedFile::read_from_path(path_as_file, path_as_packed_file)?
        };
        let packed_file = PackedFile::new_from_raw(&raw_data);
        self.add_packed_file(&packed_file, overwrite)
    }

    /// This function is used to add one or more files from disk to a `PackFile`, turning them into `PackedFiles`.
    ///
    /// In case of conflict, if overwrite is set to true, the current `PackedFile` in the conflicting path
    /// will be overwritten with the new one. If set to false, the new `PackFile` will be called `xxxx_1.extension`.
    pub fn add_from_files(
        &mut self,
        paths_as_file_and_packed_file: &[(PathBuf, Vec<String>)],
        overwrite: bool,
    ) -> Result<Vec<Vec<String>>> {
        let mut packed_files = vec![];
        for (path_as_file, path_as_packed_file) in paths_as_file_and_packed_file.iter() {
            let raw_data = RawPackedFile::read_from_path(path_as_file, path_as_packed_file.to_vec())?;
            packed_files.push(PackedFile::new_from_raw(&raw_data));
        }
        let ref_packed_files = packed_files.iter().collect::<Vec<&PackedFile>>();
        self.add_packed_files(&ref_packed_files, overwrite, true)
    }

    /// This function is used to add multiple folders from disk to a `PackFile`, turning their files into `PackedFiles`.
    ///
    /// In case of conflict, if overwrite is set to true, the current `PackedFile` in the conflicting path
    /// will be overwritten with the new one. If set to false, the new `PackFile` will be called `xxxx_1.extension`.
    pub fn add_from_folders(
        &mut self,
        paths_as_folder_and_destination: &[(PathBuf, Vec<String>)],
        paths_to_ignore: &Option<Vec<PathBuf>>,
        overwrite: bool,
        import_tables_from_tsv: bool,
    ) -> Result<Vec<Vec<String>>> {

        let mut packed_files_to_add = vec![];
        for (path, base_path) in paths_as_folder_and_destination {
            match get_files_from_subdir(path, true) {
                Ok(file_paths) => {
                    for file_path in &file_paths {

                        if let Some(paths_to_ignore) = paths_to_ignore {
                            if paths_to_ignore.iter().any(|x| file_path.starts_with(x)) {
                                continue;
                            }
                        }

                        // If we're going to import TSV, make sure to remove any collision between binary and TSV.
                        if import_tables_from_tsv {
                            if let Some(extension) = file_path.extension() {
                                if extension != "tsv" {
                                    let mut path = file_path.to_path_buf();
                                    path.set_extension("tsv");
                                    if file_paths.par_iter().any(|source| source == &path) {
                                        continue;
                                    }
                                }
                            } else {
                                let mut path = file_path.to_path_buf();
                                path.set_extension("tsv");
                                if file_paths.par_iter().any(|source| source == &path) {
                                    continue;
                                }
                            }
                        }

                        // The stupid C: letter in paths causes problems when we're on windows.
                        let drain_fix = if cfg!(target_os = "windows") { 1 } else { 0 };
                        let new_path_filtered = file_path.to_string_lossy()
                            .replace('\\', "/") // Fix for windows paths.
                            .split('/')
                            .collect::<Vec<&str>>()
                            .drain(path.components().count() - 1 - drain_fix..)
                            .map(|x| x.to_owned())
                            .collect::<Vec<String>>();
                        let mut new_path = base_path.to_vec();
                        new_path.extend_from_slice(&new_path_filtered);

                        let raw_data = if import_tables_from_tsv {

                            // If it's a tsv, try to import it as a table. If it fails... import it as a normal file.
                            if let Some(extension) = file_path.extension() {
                                if extension == "tsv" {
                                    match &*SCHEMA.read().unwrap() {
                                        Some(schema) => {

                                            // Get the file type/table name.
                                            let mut file = BufReader::new(File::open(file_path)?);
                                            let mut first_row = String::new();
                                            let mut second_row = String::new();
                                            file.read_line(&mut first_row)?;
                                            file.read_line(&mut second_row)?;

                                            let second_row_split = second_row.split('\t').collect::<Vec<&str>>();
                                            match second_row_split.get(0) {
                                                Some(table_type) => {

                                                    // If we have at least 2 fields, use the legacy behavior.
                                                    let has_legacy_structure = if let Some(table_type) = second_row_split.get(1) { table_type != &"" } else { false };
                                                    let mut table_type = if has_legacy_structure { table_type.to_string() } else { table_type.split(';').collect::<Vec<&str>>()[0].to_owned() };

                                                    if table_type.starts_with("#") {
                                                        table_type.remove(0);
                                                    }

                                                    // Act depending on the type.
                                                    if &table_type == TSV_NAME_LOC {
                                                        let (table, _) = Loc::import_tsv(schema, file_path)?;
                                                        let raw_data = table.save()?;
                                                        let mut packed_file_name = new_path.last().unwrap().to_string();
                                                        let packed_file_name_len = packed_file_name.chars().count();
                                                        if packed_file_name_len >= 4 {
                                                            packed_file_name.drain(packed_file_name_len - 4..packed_file_name_len);
                                                        }
                                                        *new_path.last_mut().unwrap() = packed_file_name.to_owned() + ".loc";
                                                        RawPackedFile::read_from_vec(new_path, packed_file_name, 0, false, raw_data)
                                                    } else {
                                                        let (table, _) = DB::import_tsv(schema, file_path)?;
                                                        let raw_data = table.save()?;
                                                        let mut packed_file_name = new_path.last().unwrap().to_string();
                                                        let packed_file_name_len = packed_file_name.chars().count();
                                                        if packed_file_name_len >= 4 {
                                                            packed_file_name.drain(packed_file_name_len - 4..packed_file_name_len);
                                                        }
                                                        *new_path.last_mut().unwrap() = packed_file_name.to_owned();
                                                        RawPackedFile::read_from_vec(new_path, packed_file_name, 0, false, raw_data)
                                                    }
                                                }
                                                None => RawPackedFile::read_from_path(file_path, new_path)?,
                                            }
                                        }
                                        None => return Err(ErrorKind::SchemaNotFound.into())
                                    }
                                } else if extension == "json" || extension == "md" {
                                    if let Some(file_name) = file_path.file_name() {
                                        let settings_name = RESERVED_NAME_SETTINGS.to_owned() + ".json";
                                        let notes_name = RESERVED_NAME_NOTES.to_owned() + ".md";
                                        let file_name = file_name.to_string_lossy().to_string();
                                        if file_name == settings_name {
                                            let mut file = BufReader::new(File::open(file_path)?);
                                            let mut data = vec![];
                                            file.read_to_end(&mut data)?;
                                            let settings = PackFileSettings::load(&data)?;
                                            self.set_settings(&settings);
                                            return Ok(vec![]);
                                        } else if file_name == notes_name {
                                            let mut file = BufReader::new(File::open(file_path)?);
                                            let mut data = String::new();
                                            file.read_to_string(&mut data)?;
                                            self.set_notes(&Some(data));
                                            return Ok(vec![]);
                                        }
                                        else {
                                            RawPackedFile::read_from_path(file_path, new_path)?
                                        }
                                    } else {
                                        RawPackedFile::read_from_path(file_path, new_path)?
                                    }
                                } else {
                                    RawPackedFile::read_from_path(file_path, new_path)?
                                }
                            }

                            // If it's a normal file, import it as a normal file.
                            else {
                                RawPackedFile::read_from_path(file_path, new_path)?
                            }
                        } else {
                                RawPackedFile::read_from_path(file_path, new_path)?
                        };

                        let packed_file = PackedFile::new_from_raw(&raw_data);
                        packed_files_to_add.push(packed_file);
                    }
                }
                Err(error) => return Err(From::from(error))
            }
        }

        self.add_packed_files(&packed_files_to_add.iter().collect::<Vec<&PackedFile>>(), overwrite, true)
    }

    /// This function is used to add a `PackedFile` from one `PackFile` into another.
    ///
    /// It's a ***Copy from another PackFile*** kind of function. It returns the PathTypes
    /// of whatever got added to our `PackFile`.
    pub fn add_from_packfile(
        &mut self,
        source: &Self,
        path_types: &[PathType],
        overwrite: bool,
    ) -> Result<Vec<PathType>> {

        // Keep the PathTypes added so we can return them to the UI easily.
        let paths;
        let path_types = PathType::dedup(path_types);

        // As this can get very slow very quickly, we do here some... optimizations.
        // First, we get if there are PackFiles or folders in our list of PathTypes.
        let we_have_packfile = path_types.par_iter().any(|item| {
            matches!(item, PathType::PackFile)
        });

        let we_have_folder = path_types.par_iter().any(|item| {
            matches!(item, PathType::Folder(_))
        });

        // Then, if we have a PackFile,... just import all PackedFiles.
        if we_have_packfile {
            let mut packed_files = source.get_packed_files_all();
            packed_files.par_iter_mut().try_for_each(|x| x.encode())?;
            let packed_files = packed_files.par_iter().map(|x|&*x).collect::<Vec<&PackedFile>>();
            paths = self.add_packed_files(&packed_files, overwrite, true)?;
        }

        // If we only have files, get all the files we have at once, then add them all together.
        else if !we_have_folder {
            let paths_files = path_types.par_iter().filter_map(|x| {
                if let PathType::File(path) = x { Some(&**path) } else { None }
            }).collect::<Vec<&[String]>>();
            let mut packed_files = source.get_packed_files_by_paths(paths_files);
            packed_files.par_iter_mut().try_for_each(|x| x.encode())?;
            let packed_files = packed_files.par_iter().map(|x|&*x).collect::<Vec<&PackedFile>>();
            paths = self.add_packed_files(&packed_files, overwrite, true)?;
        }

        // Otherwise, we have a mix of Files and Folders (or folders only).
        // In this case, we get all the individual files, then the ones inside folders.
        // Then we merge them, and add all of them together.
        else {
            let paths_files = path_types.par_iter().filter_map(|x| {
                if let PathType::File(path) = x { Some(&**path) } else { None }
            }).collect::<Vec<&[String]>>();
            let mut packed_files = source.get_packed_files_by_paths(paths_files);

            packed_files.append(&mut path_types.par_iter().filter_map(|x| {
                if let PathType::Folder(path) = x { Some(&**path) } else { None }
            }).map(|path| source.get_packed_files_by_path_start(path))
            .flatten()
            .collect::<Vec<PackedFile>>());

            packed_files.par_iter_mut().try_for_each(|x| x.encode())?;
            let packed_files = packed_files.par_iter().map(|x|&*x).collect::<Vec<&PackedFile>>();
            paths = self.add_packed_files(&packed_files, overwrite, true)?;
        }

        Ok(paths.par_iter().map(|x| PathType::File(x.to_vec())).collect::<Vec<PathType>>())
    }

    /// This function returns the name of the PackFile. If it's empty, it's an in-memory only PackFile.
    pub fn get_file_name(&self) -> String {
        match self.file_path.file_name() {
            Some(s) => s.to_string_lossy().to_string(),
            None => String::new()
        }
    }

    /// This function returns the path of the PackFile. If it's empty, it's an in-memory only PackFile.
    pub fn get_file_path(&self) -> &PathBuf {
        &self.file_path
    }

    /// This function changes the path of the PackFile.
    ///
    /// This can fail if you pass it an empty path.
    pub fn set_file_path(&mut self, path: &Path) -> Result<()> {
        if path.components().count() == 0 { return Err(ErrorKind::EmptyInput.into()) }
        self.file_path = path.to_path_buf();

        // We have to change the name of the PackFile in all his `PackedFiles` too.
        let file_name = self.get_file_name();
        self.packed_files.iter_mut().for_each(|x| x.get_ref_mut_raw().set_packfile_name(&file_name));
        Ok(())
    }

    /// This function returns the current compression state of the provided `PackFile`.
    ///
    /// To get more info about the different compression states, check the `CompressionState` enum.
    pub fn get_compression_state(&self) -> CompressionState {
        let mut has_files_compressed = false;
        let mut has_files_uncompressed = false;
        for packed_file in &self.packed_files {
            let is_compressed = packed_file.get_ref_raw().get_compression_state();
            if !has_files_compressed && is_compressed {
                has_files_compressed = true;
            }

            if !has_files_uncompressed && !is_compressed {
                has_files_uncompressed = true;
            }

            if has_files_uncompressed && has_files_compressed {
                break;
            }
        }
        if has_files_compressed && has_files_uncompressed { CompressionState::Partial }
        else if has_files_compressed { CompressionState::Enabled }
        else { CompressionState::Disabled }
    }

    /// This function returns if the `PackFile` is editable or not.
    ///
    /// By *if is editable or not* I mean *If you can save it or not*. The conditions under which a PackFile is not editable are:
    /// - All PackFiles with extended header or encrypted parts are not editable.
    /// - All PackFiles of type `Mod` or `Movie` are editable.
    /// - If you say CA PackFiles are not editable:
    ///   - All PackFiles of type `Boot`, `Release` or `Patch` are not editable.
    /// - If you say CA PackFiles are editable:
    ///   - All PackFiles of type `Boot`, `Release` or `Patch` are editable.
    pub fn is_editable(&self, is_editing_of_ca_packfiles_allowed: bool) -> bool {

        // If it's this very specific type, don't save under any circumstance.
        if let PFHFileType::Other(_) = self.pfh_file_type { false }

        // If ANY of these bitmask is detected in the PackFile, disable all saving.
        else if self.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) ||
            self.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) ||
            self.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) { false }
        else {
            self.pfh_file_type == PFHFileType::Mod ||
            self.pfh_file_type == PFHFileType::Movie ||
            (is_editing_of_ca_packfiles_allowed && self.pfh_file_type.get_value() <= 2)
        }
    }

    /// This function returns a copy to the `PackedFile` with the provided path, if exists.
    pub fn get_packed_file_by_path(&self, path: &[String]) -> Option<PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path() == path).cloned().find_any(|x| x.get_path() == path)
    }

    /// This function returns a reference to the `PackedFile` with the provided path, if exists.
    pub fn get_ref_packed_file_by_path(&self, path: &[String]) -> Option<&PackedFile> {
        self.packed_files.par_iter().find_any(|x| x.get_path() == path)
    }

    /// This function returns a mutable reference to the `PackedFile` with the provided path, if exists.
    pub fn get_ref_mut_packed_file_by_path(&mut self, path: &[String]) -> Option<&mut PackedFile> {
        self.packed_files.par_iter_mut().find_any(|x| x.get_path() == path)
    }

    /// This function returns a copy of all the `PackedFiles` in the provided paths.
    pub fn get_packed_files_by_paths(&self, paths: Vec<&[String]>) -> Vec<PackedFile> {
        self.packed_files.par_iter().filter(|x| paths.contains(&x.get_path())).cloned().collect()
    }

    /// This function returns a reference of all the `PackedFiles` in the provided paths.
    pub fn get_ref_packed_files_by_paths(&self, paths: Vec<&[String]>) -> Vec<&PackedFile> {
        self.packed_files.par_iter().filter(|x| paths.contains(&x.get_path())).collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` in the provided paths.
    pub fn get_ref_mut_packed_files_by_paths(&mut self, paths: Vec<&[String]>) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().filter(|x| paths.contains(&x.get_path())).collect()
    }

    /// This function returns a copy of all the `PackedFiles` in the provided paths, in a case insensitive manner.
    pub fn get_packed_files_by_paths_unicased(&self, paths: Vec<UniCase<String>>) -> Vec<PackedFile> {
        self.packed_files.par_iter().filter(|x| paths.contains(&UniCase::new(x.get_path().join("/")))).cloned().collect()
    }

    /// This function returns a reference of all the `PackedFiles` in the provided paths, in a case insensitive manner.
    pub fn get_ref_packed_files_by_paths_unicased(&self, paths: Vec<UniCase<String>>) -> Vec<&PackedFile> {
        self.packed_files.par_iter().filter(|x| paths.contains(&UniCase::new(x.get_path().join("/")))).collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` in the provided paths, in a case insensitive manner.
    pub fn get_ref_mut_packed_files_by_paths_unicased(&mut self, paths: Vec<UniCase<String>>) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().filter(|x| paths.contains(&UniCase::new(x.get_path().join("/")))).collect()
    }

    /// This function returns a copy of all the `PackedFiles` starting with the provided path.
    pub fn get_packed_files_by_path_start(&self, path: &[String]) -> Vec<PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().starts_with(path) && !path.is_empty() && x.get_path().len() > path.len()).cloned().collect()
    }

    /// This function returns a reference of all the `PackedFiles` starting with the provided path.
    pub fn get_ref_packed_files_by_path_start(&self, path: &[String]) -> Vec<&PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().starts_with(path) && !path.is_empty() && x.get_path().len() > path.len()).collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` starting with the provided path.
    pub fn get_ref_mut_packed_files_by_path_start(&mut self, path: &[String]) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().filter(|x| x.get_path().starts_with(path) && !path.is_empty() && x.get_path().len() > path.len()).collect()
    }

    /// This function returns a copy of all the `PackedFiles` starting with the provided path, in a case insensitive manner.
    pub fn get_packed_files_by_path_start_unicased(&self, path: UniCase<String>) -> Vec<PackedFile> {
        let path_provided_len = path.chars().count();
        self.packed_files.par_iter().filter(|x| {
            if !path.is_empty() && path.starts_with(&x.get_path()[0]) {
                let path_str = x.get_path().join("/");
                let path_len = path_str.chars().count();
                path_len > path_provided_len && UniCase::new(&path_str[..path_provided_len]) == path
            } else { false }
        }).cloned().collect()
    }

    /// This function returns a reference of all the `PackedFiles` starting with the provided path, in a case insensitive manner.
    pub fn get_ref_packed_files_by_path_start_unicased(&self, path: UniCase<String>) -> Vec<&PackedFile> {
        let path_provided_len = path.chars().count();
        self.packed_files.par_iter().filter(|x| {
            if !path.is_empty() && path.starts_with(&x.get_path()[0]) {
                let path_str = x.get_path().join("/");
                let path_len = path_str.chars().count();
                path_len > path_provided_len && UniCase::new(&path_str[..path_provided_len]) == path
            } else { false }
        }).collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` starting with the provided path, in a case insensitive manner.
    pub fn get_ref_mut_packed_files_by_path_start_unicased(&mut self, path: UniCase<String>) -> Vec<&mut PackedFile> {
        let path_provided_len = path.chars().count();
        self.packed_files.par_iter_mut().filter(|x| {
            if !path.is_empty() && path.starts_with(&x.get_path()[0]){
                let path_str = x.get_path().join("/");
                let path_len = path_str.chars().count();
                path_len > path_provided_len && UniCase::new(&path_str[..path_provided_len]) == path
            } else { false }
        }).collect()
    }

    /// This function returns a copy of the paths of all the `PackedFiles` in the provided `PackFile` under the provided path.
    pub fn get_packed_files_paths_by_path_start(&self, path: &[String]) -> Vec<Vec<String>> {
        self.packed_files.par_iter().map(|x| x.get_path()).filter(|x| x.starts_with(path) && !path.is_empty() && x.len() > path.len()).map(|x| x.to_vec()).collect()
    }

    /// This function returns a reference of the paths of all the `PackedFiles` in the provided `PackFile` under the provided path.
    pub fn get_ref_packed_files_paths_by_path_start(&self, path: &[String]) -> Vec<&[String]> {
        self.packed_files.par_iter().map(|x| x.get_path()).filter(|x| x.starts_with(path) && !path.is_empty() && x.len() > path.len()).collect()
    }

    /// This function returns a copy of all the `PackedFiles` ending with the provided path.
    pub fn get_packed_files_by_path_end(&self, path: &[String]) -> Vec<PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().ends_with(path) && !path.is_empty()).cloned().collect()
    }

    /// This function returns a reference of all the `PackedFiles` ending with the provided path.
    pub fn get_ref_packed_files_by_path_end(&self, path: &[String]) -> Vec<&PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().ends_with(path) && !path.is_empty()).collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` ending with the provided path.
    pub fn get_ref_mut_packed_files_by_path_end(&mut self, path: &[String]) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().filter(|x| x.get_path().ends_with(path) && !path.is_empty()).collect()
    }

    /// This function returns a copy of all the `PackedFiles` ending with the provided extension.
    pub fn get_packed_files_by_extension(&self, extension: &str) -> Vec<PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().last().unwrap().ends_with(extension) && !extension.is_empty()).cloned().collect()
    }

    /// This function returns a reference of all the `PackedFiles` ending with the provided extension.
    pub fn get_ref_packed_files_by_extension(&self, extension: &str) -> Vec<&PackedFile> {
        self.packed_files.par_iter().filter(|x| x.get_path().last().unwrap().ends_with(extension) && !extension.is_empty()).collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` ending with the provided extension.
    pub fn get_ref_mut_packed_files_by_extension(&mut self, extension: &str) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().filter(|x| x.get_path().last().unwrap().ends_with(extension) && !extension.is_empty()).collect()
    }

    /// This function returns a copy of all the PackedFiles in the current PackFile of the provided type.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not guarantee the provided PackedFiles are of the type. Just that they `match` the type.
    pub fn get_packed_files_by_type(&self, packed_file_type: PackedFileType, strict_match_mode: bool) -> Vec<PackedFile> {
        self.packed_files.par_iter()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_ref_raw(), false);
                if strict_match_mode { y == packed_file_type } else { y.eq_non_strict(packed_file_type) }
            }).cloned().collect()
    }

    /// This function returns a reference of all the PackedFiles in the current PackFile of the provided type.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not guarantee the provided PackedFiles are of the type. Just that they `match` the type.
    pub fn get_ref_packed_files_by_type(&self, packed_file_type: PackedFileType, strict_match_mode: bool) -> Vec<&PackedFile> {
        self.packed_files.par_iter()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_ref_raw(), false);
                if strict_match_mode { y == packed_file_type } else { y.eq_non_strict(packed_file_type) }
            }).collect()
    }

    /// This function returns a mutable reference of all the PackedFiles in the current PackFile of the provided type.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not guarantee the provided PackedFiles are of the type. Just that they `match` the type.
    pub fn get_ref_mut_packed_files_by_type(&mut self, packed_file_type: PackedFileType, strict_match_mode: bool) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_ref_raw(), false);
                if strict_match_mode { y == packed_file_type } else { y.eq_non_strict(packed_file_type) }
            }).collect()
    }

    /// This function returns a copy of all the PackedFiles in the current PackFile of the provided types.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not guarantee the provided PackedFiles are of the type. Just that they `match` one of the types.
    pub fn get_packed_files_by_types(&self, packed_file_types: &[PackedFileType], strict_match_mode: bool) -> Vec<PackedFile> {
        self.packed_files.par_iter()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_ref_raw(), false);
                if strict_match_mode { packed_file_types.contains(&y) } else { y.eq_non_strict_slice(packed_file_types) }
            }).cloned().collect()
    }

    /// This function returns a reference of all the PackedFiles in the current PackFile of the provided types.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not guarantee the provided PackedFiles are of the type. Just that they `match` one of the types.
    pub fn get_ref_packed_files_by_types(&self, packed_file_types: &[PackedFileType], strict_match_mode: bool) -> Vec<&PackedFile> {
        self.packed_files.par_iter()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_ref_raw(), false);
                if strict_match_mode { packed_file_types.contains(&y) } else { y.eq_non_strict_slice(packed_file_types) }
            }).collect()
    }

    /// This function returns a mutable reference of all the PackedFiles in the current PackFile of the provided types.
    ///
    /// If `strict_match_mode` is enabled, only the PackedFiles of the specified type and subtype will be returned.
    /// NOTE: This does not guarantee the provided PackedFiles are of the type. Just that they `match` one of the types.
    pub fn get_ref_mut_packed_files_by_types(&mut self, packed_file_types: &[PackedFileType], strict_match_mode: bool) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut()
            .filter(|x| {
                let y = PackedFileType::get_packed_file_type(x.get_ref_raw(), false);
                if strict_match_mode { packed_file_types.contains(&y) } else { y.eq_non_strict_slice(packed_file_types) }
            }).collect()
    }

    /// This function returns a copy of all `PackedFiles` in the provided `PackFile`.
    pub fn get_packed_files_all(&self) -> Vec<PackedFile> {
        self.packed_files.clone()
    }

    /// This function returns a reference of all the `PackedFiles` in the provided `PackFile`.
    pub fn get_ref_packed_files_all(&self) -> Vec<&PackedFile> {
        self.packed_files.par_iter().collect()
    }

    /// This function returns a mutable reference of all the `PackedFiles` in the provided `PackFile`.
    pub fn get_ref_mut_packed_files_all(&mut self) -> Vec<&mut PackedFile> {
        self.packed_files.par_iter_mut().collect()
    }

    /// This function returns a copy of the paths of all the `PackedFiles` in the provided `PackFile`.
    pub fn get_packed_files_all_paths(&self) -> Vec<Vec<String>> {
        self.packed_files.par_iter().map(|x| x.get_path().to_vec()).collect()
    }

    /// This function returns a reference of the paths of all the `PackedFiles` in the provided `PackFile`.
    pub fn get_ref_packed_files_all_paths(&self) -> Vec<&[String]> {
        self.packed_files.par_iter().map(|x| x.get_path()).collect()
    }

    /// This function returns a copy of the paths of all the `PackedFiles` in the provided `PackFile` as Strings.
    pub fn get_packed_files_all_paths_as_string(&self) -> HashSet<UniCase<String>> {
        self.packed_files.par_iter().map(|x| UniCase::new(x.get_path().join("/"))).collect()
    }

    /// This function returns a copy of the paths of all the folders in the provided `PackFile` as Strings.
    pub fn get_folder_all_paths_as_string(&self) -> HashSet<UniCase<String>> {
        let mut folder_paths = self.packed_files.par_iter().map(|x| {
            let path = x.get_path();
            let mut paths = Vec::with_capacity(path.len() - 1);

            for (index, folder) in path.iter().enumerate() {
                if index < path.len() - 1 && !folder.is_empty() {
                    paths.push(UniCase::new(path[0..=index].join("/")))
                }
            }

            paths
        }).flatten().collect::<Vec<UniCase<String>>>();

        folder_paths.sort();
        folder_paths.dedup();
        folder_paths.into_iter().collect::<HashSet<_>>()
    }

    /// This function returns a copy of all the `PackedFileInfo` corresponding to the provided `PackFile`.
    pub fn get_packed_files_all_info(&self) -> Vec<PackedFileInfo> {
        self.packed_files.par_iter().map(From::from).collect()
    }

    /// This function returns a copy of the `PackedFileInfo` of the `Packedfile` in the provided path.
    pub fn get_packed_file_info_by_path(&self, path: &[String]) -> Option<PackedFileInfo> {
        self.packed_files.par_iter().find_first(|x| x.get_path() == path).map(From::from)
    }

    /// This function returns a copy of all the PackedFiles in the provided PathTypes, in a case insensitive manner.
    pub fn get_packed_files_by_path_type_unicased(&self, path_types: &[PathType]) -> Vec<PackedFile> {

        // Keep the PathTypes added so we can return them to the UI easily.
        let path_types = PathType::dedup(path_types);

        // As this can get very slow very quickly, we do here some... optimizations.
        // First, we get if there are PackFiles or folders in our list of PathTypes.
        let we_have_packfile = path_types.par_iter().any(|item| {
            matches!(item, PathType::PackFile)
        });

        let we_have_folder = path_types.par_iter().any(|item| {
            matches!(item, PathType::Folder(_))
        });

        // Then, if we have a PackFile,... just import all PackedFiles.
        if we_have_packfile {
            self.get_packed_files_all()
        }

        // If we only have files, get all the files we have at once, then add them all together.
        else if !we_have_folder {
            let paths_files = path_types.par_iter().filter_map(|x| {
                if let PathType::File(path) = x { Some(UniCase::new(path.join("/"))) } else { None }
            }).collect::<Vec<UniCase<String>>>();
            self.get_packed_files_by_paths_unicased(paths_files)
        }

        // Otherwise, we have a mix of Files and Folders (or folders only).
        // In this case, we get all the individual files, then the ones inside folders.
        // Then we merge them, and add all of them together.
        else {
            let paths_files = path_types.par_iter().filter_map(|x| {
                if let PathType::File(path) = x { Some(UniCase::new(path.join("/")))  } else { None }
            }).collect::<Vec<UniCase<String>>>();
            let mut packed_files = self.get_packed_files_by_paths_unicased(paths_files);

            packed_files.append(&mut path_types.par_iter().filter_map(|x| {
                if let PathType::Folder(path) = x { Some(UniCase::new(path.join("/"))) } else { None }
            }).map(|path| self.get_packed_files_by_path_start_unicased(path))
            .flatten()
            .collect::<Vec<PackedFile>>());
            packed_files
        }
    }

    /// This function returns a reference of all the PackedFiles in the provided PathTypes, in a case insensitive manner.
    pub fn get_ref_packed_files_by_path_type_unicased(&self, path_types: &[PathType]) -> Vec<&PackedFile> {

        // Keep the PathTypes added so we can return them to the UI easily.
        let path_types = PathType::dedup(path_types);

        // As this can get very slow very quickly, we do here some... optimizations.
        // First, we get if there are PackFiles or folders in our list of PathTypes.
        let we_have_packfile = path_types.par_iter().any(|item| {
            matches!(item, PathType::PackFile)
        });

        let we_have_folder = path_types.par_iter().any(|item| {
            matches!(item, PathType::Folder(_))
        });

        // Then, if we have a PackFile,... just import all PackedFiles.
        if we_have_packfile {
            self.get_ref_packed_files_all()
        }

        // If we only have files, get all the files we have at once, then add them all together.
        else if !we_have_folder {
            let paths_files = path_types.par_iter().filter_map(|x| {
                if let PathType::File(path) = x { Some(UniCase::new(path.join("/"))) } else { None }
            }).collect::<Vec<UniCase<String>>>();
            self.get_ref_packed_files_by_paths_unicased(paths_files)
        }

        // Otherwise, we have a mix of Files and Folders (or folders only).
        // In this case, we get all the individual files, then the ones inside folders.
        // Then we merge them, and add all of them together.
        else {
            let paths_files = path_types.par_iter().filter_map(|x| {
                if let PathType::File(path) = x { Some(UniCase::new(path.join("/")))  } else { None }
            }).collect::<Vec<UniCase<String>>>();
            let mut packed_files = self.get_ref_packed_files_by_paths_unicased(paths_files);

            packed_files.append(&mut path_types.par_iter().filter_map(|x| {
                if let PathType::Folder(path) = x { Some(UniCase::new(path.join("/"))) } else { None }
            }).map(|path| self.get_ref_packed_files_by_path_start_unicased(path))
            .flatten()
            .collect::<Vec<&PackedFile>>());
            packed_files
        }
    }

    /// This function returns a copy of all the PackedFiles in the provided PathTypes.
    pub fn get_packed_files_by_path_type(&mut self, path_types: &[PathType]) -> Vec<PackedFile> {

        // Keep the PathTypes added so we can return them to the UI easily.
        let path_types = PathType::dedup(path_types);

        // As this can get very slow very quickly, we do here some... optimizations.
        // First, we get if there are PackFiles or folders in our list of PathTypes.
        let we_have_packfile = path_types.par_iter().any(|item| {
            matches!(item, PathType::PackFile)
        });

        let we_have_folder = path_types.par_iter().any(|item| {
            matches!(item, PathType::Folder(_))
        });

        // Then, if we have a PackFile,... just import all PackedFiles.
        if we_have_packfile {
            self.get_packed_files_all()
        }

        // If we only have files, get all the files we have at once, then add them all together.
        else if !we_have_folder {
            let paths_files = path_types.par_iter().filter_map(|x| {
                if let PathType::File(path) = x { Some(&**path) } else { None }
            }).collect::<Vec<&[String]>>();
            self.get_packed_files_by_paths(paths_files)
        }

        // Otherwise, we have a mix of Files and Folders (or folders only).
        // In this case, we get all the individual files, then the ones inside folders.
        // Then we merge them, and add all of them together.
        else {
            let paths_files = path_types.par_iter().filter_map(|x| {
                if let PathType::File(path) = x { Some(&**path) } else { None }
            }).collect::<Vec<&[String]>>();
            let mut packed_files = self.get_packed_files_by_paths(paths_files);

            packed_files.append(&mut path_types.par_iter().filter_map(|x| {
                if let PathType::Folder(path) = x { Some(&**path) } else { None }
            }).map(|path| self.get_packed_files_by_path_start(path))
            .flatten()
            .collect::<Vec<PackedFile>>());
            packed_files
        }
    }

    /// This function removes, if exists, a `PackedFile` with the provided path from the `PackFile`.
    pub fn remove_packed_file_by_path(&mut self, path: &[String]) {
        if let Some(position) = self.packed_files.par_iter().position_any(|x| x.get_path() == path) {
            self.packed_files.remove(position);
        }
    }

    /// This function removes, if exists, all `PackedFile` starting with the provided path from the `PackFile`.
    pub fn remove_packed_files_by_path_start(&mut self, path: &[String]) {
        let positions: Vec<usize> = self.packed_files.iter()
            .enumerate()
            .filter(|x| x.1.get_path().starts_with(path) && !path.is_empty() && x.1.get_path().len() > path.len())
            .map(|x| x.0)
            .collect();
        for position in positions.iter().rev() {
            self.packed_files.remove(*position);
        }
    }

    /// This function removes, if exists, all `PackedFile` ending with the provided path from the `PackFile`.
    pub fn remove_packed_files_by_path_end(&mut self, path: &[String]) {
        let positions: Vec<usize> = self.packed_files.iter()
            .enumerate()
            .filter(|x| x.1.get_path().ends_with(path) && !path.is_empty())
            .map(|x| x.0)
            .collect();
        for position in positions.iter().rev() {
            self.packed_files.remove(*position);
        }
    }

    /// This function removes, if exists, all `PackedFile` of the provided types from the `PackFile`.
    pub fn remove_packed_files_by_type(&mut self, item_types: &[PathType]) -> Vec<PathType> {

        // We need to "clean" the selected path list to ensure we don't pass stuff already deleted.
        let item_types_clean = PathType::dedup(item_types);

        // Now we do some bitwise magic to get what type of selection combination we have.
        let mut contents: u8 = 0;
        for item_type in &item_types_clean {
            match item_type {
                PathType::File(_) => contents |= 1,
                PathType::Folder(_) => contents |= 2,
                PathType::PackFile => contents |= 4,
                PathType::None => contents |= 8,
            }
        }

        // Then we act, depending on the combination of items.
        match contents {

            // Any combination of files and folders.
            1 | 2 | 3 => {
                for item_type in &item_types_clean {
                    match item_type {
                        PathType::File(path) => self.remove_packed_file_by_path(path),
                        PathType::Folder(path) => self.remove_packed_files_by_path_start(path),
                        _ => unreachable!(),
                    }
                }
            },

            // If the `PackFile` is selected, just delete everything.
            4 | 5 | 6 | 7 => self.remove_all_packedfiles(),

            // No paths selected, none selected, invalid path selected, or invalid value.
            0 | 8..=255 => {},
        }

        // Return the list of deleted items so the caller can have a clean list to know what was really removed from the `PackFile`.
        item_types_clean
    }

    /// This function extracts, if exists, a `PackedFile` with the provided path from the `PackFile`.
    ///
    /// The destination path is always `destination_path/packfile_name/path_to_packedfile/packed_file`.
    pub fn extract_packed_file_by_path(
        &mut self,
        path: &[String],
        destination_path: &Path,
        extract_table_as_tsv: bool
    ) -> Result<()> {
        match self.get_ref_mut_packed_file_by_path(path) {
            Some(ref mut packed_file) => packed_file.extract_packed_file(destination_path, extract_table_as_tsv),
            None => Err(ErrorKind::PackedFileNotFound.into())
        }
    }

    /// This function extract, if exists, all `PackedFile` of the provided types from the `PackFile` to disk.
    ///
    /// As this can fail for some files, and work for others, we return `Ok(amount_files_extracted)` only if all files were extracted correctly.
    /// If any of them failed, we return `Error` with a list of the paths that failed to get extracted.
    pub fn extract_packed_files_by_type(
        &mut self,
        item_types: &[PathType],
        extracted_path: &Path,
        extract_table_as_tsv: bool
    ) -> Result<u32> {

        // These variables are here to keep track of what we have extracted and what files failed.
        let mut files_extracted = 0;
        let mut error_files = vec![];

        // We need to "clean" the selected path list to ensure we don't pass stuff already extracted.
        let item_types_clean = PathType::dedup(item_types);

        // Now we do some bitwise magic to get what type of selection combination we have.
        let mut contents: u8 = 0;
        for item_type in &item_types_clean {
            match item_type {
                PathType::File(_) => contents |= 1,
                PathType::Folder(_) => contents |= 2,
                PathType::PackFile => contents |= 4,
                PathType::None => contents |= 8,
            }
        }

        // Then we act, depending on the combination of items.
        match contents {

            // Any combination of files and folders.
            1 | 2 | 3 => {

                // For folders we check each PackedFile to see if it starts with the folder's path (it's in the folder).
                // There should be no duplicates here thanks to the filters from before.
                for item_type in &item_types_clean {
                    match item_type {

                        // For individual `PackedFiles`, we extract them one by one.
                        PathType::File(path) => {
                            match self.extract_packed_file_by_path(path, extracted_path, extract_table_as_tsv) {
                                Ok(_) => files_extracted += 1,
                                Err(_) => error_files.push(format!("{:?}", path)),
                            }
                        },

                        PathType::Folder(path) => {
                            for packed_file in self.get_ref_mut_packed_files_by_path_start(path) {
                                match packed_file.extract_packed_file(extracted_path, extract_table_as_tsv) {
                                    Ok(_) => files_extracted += 1,
                                    Err(_) => error_files.push(format!("{:?}", path)),
                                }
                            }
                        },

                        _ => unreachable!(),
                    }
                }
            },

            // If the `PackFile` is selected, just extract it and everything will get extracted with it.
            4 | 5 | 6 | 7 => {

                // For each PackedFile we have, just extracted in the folder we got, under the PackFile's folder.
                let mut packed_files = self.get_ref_mut_packed_files_all();
                files_extracted = packed_files.len() as u32;

                error_files = packed_files.par_iter_mut().filter_map(|packed_file| {
                    if packed_file.extract_packed_file(extracted_path, extract_table_as_tsv).is_err() {
                        Some(format!("{:?}", packed_file.get_path()))
                    } else { None }
                }).collect();
                files_extracted -= error_files.len() as u32;

                // If we're extracting everything as TSV, it's a mymod export. Also extract notes and settings.
                if extract_table_as_tsv {

                    if let Some(note) = &self.notes {
                        let mut data = vec![];
                        data.encode_string_u8(note);
                        let path = extracted_path.join(RESERVED_NAME_NOTES.to_owned() + ".md");
                        let mut file = BufWriter::new(File::create(path)?);
                        file.write_all(&data)?;
                        file.flush()?;
                    }

                    // Saving PackFile settings.
                    let mut data = vec![];
                    data.write_all(to_string_pretty(&self.settings)?.as_bytes())?;
                    data.extend_from_slice(b"\n"); // Add newline to the end of the file
                    let path = extracted_path.join(RESERVED_NAME_SETTINGS.to_owned() + ".json");
                    let mut file = BufWriter::new(File::create(path)?);
                    file.write_all(&data)?;
                    file.flush()?;
                }
            },

            // No paths selected, none selected, invalid path selected, or invalid value.
            0 | 8..=255 => return Err(ErrorKind::NonExistentFile.into()),
        }

        // If there is any error in the list, report it.
        if !error_files.is_empty() {
            let error_files_string = error_files.iter().map(|x| format!("<li>{}</li>", x)).collect::<Vec<String>>();
            return Err(ErrorKind::ExtractError(error_files_string).into())
        }

        // If we reach this, return the amount of extracted files.
        Ok(files_extracted)
    }

    /// This function enables/disables compression in all `PackedFiles` inside the `PackFile`. Partial compression is not supported.
    pub fn toggle_compression(&mut self, enable: bool) {
        self.packed_files.par_iter_mut().for_each(|x| x.get_ref_mut_raw().set_should_be_compressed(enable));
    }

    /// This function returns the notes contained within the provided `PackFile`.
    pub fn get_notes(&self) -> &Option<String> {
        &self.notes
    }

    /// This function saves your notes within the provided `PackFile`.
    pub fn set_notes(&mut self, notes: &Option<String>) {
        self.notes = notes.clone();
    }

    /// This function returns the settings contained within the provided `PackFile`.
    pub fn get_settings(&self) -> &PackFileSettings {
        &self.settings
    }

    /// This function saves your settings within the provided `PackFile`.
    pub fn set_settings(&mut self, settings: &PackFileSettings) {
        self.settings = settings.clone();
    }

    /// This function returns the timestamp of the provided `PackFile`.
    pub fn get_timestamp(&self) -> i64 {
        self.timestamp
    }

    /// This function sets the timestamp of the provided `PackFile`.
    pub fn set_timestamp(&mut self, timestamp: i64) {
        self.timestamp = timestamp;
    }

    /// This function returns the `PFHVersion` of the provided `PackFile`.
    pub fn get_pfh_version(&self) -> PFHVersion {
        self.pfh_version
    }

    /// This function sets the `PFHVersion` of the provided `PackFile`.
    pub fn set_pfh_version(&mut self, pfh_version: PFHVersion) {
        self.pfh_version = pfh_version;
    }

    /// This function returns the `PFHFileType` of the provided `PackFile`.
    pub fn get_pfh_file_type(&self) -> PFHFileType {
        self.pfh_file_type
    }

    /// This function sets the `PFHFileType` of the provided `PackFile`.
    ///
    /// NOTE: This may change the PFHVersion of this PackFile too.
    pub fn set_pfh_file_type(&mut self, pfh_file_type: PFHFileType) {
        self.pfh_file_type = pfh_file_type;

        // Make sure the current PFHVersion of this PackFile is compatible with the new PFHFileType.
        let pfh_version = GAME_SELECTED.read().unwrap().get_pfh_version_by_file_type(self.get_pfh_file_type());
        if pfh_version != self.get_pfh_version() {
            self.set_pfh_version(pfh_version);
        }
    }

    /// This function returns the `Bitmask` of the provided `PackFile`.
    pub fn get_bitmask(&self) -> PFHFlags {
        self.bitmask
    }

    /// This function returns a reference to the `Bitmask` of the provided `PackFile`.
    pub fn get_ref_bitmask(&self) -> &PFHFlags {
        &self.bitmask
    }

    /// This function returns a mutable reference to the `Bitmask` of the provided `PackFile`.
    pub fn get_ref_mut_bitmask(&mut self) -> &mut PFHFlags {
        &mut self.bitmask
    }

    /// This function sets the `Bitmask` of the provided `PackFile`.
    pub fn set_bitmask(&mut self, bitmask: PFHFlags) {
        self.bitmask = bitmask;
    }

    /// This function remove all `PackedFiles` from a `PackFile`.
    pub fn remove_all_packedfiles(&mut self) {
        self.packed_files = vec![];
    }

    /// This function checks if a `PackedFile` with a certain path exists in a `PackFile`.
    pub fn packedfile_exists(&self, path: &[String]) -> bool {
        self.packed_files.par_iter().any(|x| x.get_path() == path)
    }

    /// This function checks if a folder with `PackedFiles` in it exists in a `PackFile`.
    pub fn folder_exists(&self, path: &[String]) -> bool {
        if path.is_empty() {
           false
        } else {
            self.packed_files.par_iter().any(|x| x.get_path().starts_with(path) && x.get_path().len() > path.len())
        }
    }

    /// This function takes an slice of PathTypes and turns it into a vector of individual PackedFile's paths.
    ///
    /// This is intended to be done after a dedup. Otherwise, you'll probably get duplicated paths at the end.
    pub fn get_paths_from_path_types(&self, path_types: &[PathType]) -> Vec<Vec<String>> {

        // If we have a PackFile, just get the paths of all PackedFiles in the PackFile.
        let we_have_packfile = path_types.par_iter().any(|item| {
            matches!(item, PathType::PackFile)
        });

        if we_have_packfile {
            return self.get_packed_files_all_paths();
        }

        // If we don't have folders, the rest are files, so we just get the path stored in each PathType.
        let we_have_folder = path_types.par_iter().any(|item| {
            matches!(item, PathType::Folder(_))
        });

        if !we_have_folder {
            return path_types.par_iter().filter_map(|x| if let PathType::File(path) = x { Some(path.to_vec()) } else { None }).collect();
        }

        // If we have a mix of files and folders... then we get the paths according to the type.
        let mut paths: Vec<Vec<String>> = path_types.par_iter()
            .filter_map(|path_type| if let PathType::Folder(path) = path_type { Some(self.get_packed_files_paths_by_path_start(path)) } else { None })
            .flatten()
            .collect::<Vec<Vec<String>>>();

        paths.append(&mut path_types.par_iter()
            .filter_map(|x| if let PathType::File(path) = x { Some(path.to_vec()) } else { None })
            .collect::<Vec<Vec<String>>>());

        paths
    }

    /// This function removes all not-in-memory-already PackedFiles from the PackFile. Used for removing possibly corrupted PackedFiles from the PackFile in order to sanitize it.
    ///
    /// BE CAREFUL WITH USING THIS. IT MAY (PROBABLY WILL) CAUSE DATA LOSSES.
    pub fn clean_packfile(&mut self) {
        self.packed_files.retain(|x| x.is_in_memory())
    }

    /// This function allows you to change the path of a `PackedFile` inside a `PackFile`.
    ///
    /// By default this append a `_number` to the file name in case of collision. If you want it to overwrite instead,
    /// pass `overwrite` as `true`. This can fail if you pass it an empty or reserved path, so make sure you check the result.
    ///
    /// We return the final destination path of the PackedFile, if it worked, or an error.
    pub fn move_packedfile(
        &mut self,
        source_path: &[String],
        destination_path: &[String],
        overwrite: bool,
    ) -> Result<Vec<String>> {

        // First, ensure we can move between the paths.
        let reserved_names = Self::get_reserved_packed_file_names();
        if destination_path.is_empty() { return Err(ErrorKind::EmptyInput.into()) }
        if source_path == destination_path { return Err(ErrorKind::PathsAreEqual.into()) }
        if reserved_names.contains(&destination_path.to_vec()) { return Err(ErrorKind::ReservedFiles.into()) }

        // We may need to modify his destination path if we're not overwriting so...
        let mut destination_path = destination_path.to_vec();

        // First, we check if BOTH, the source and destination, exist.
        let source_exists = self.packedfile_exists(source_path);
        let destination_exists = self.packedfile_exists(&destination_path);

        // If both exists, we do some name resolving:
        // - If we want to overwrite the destination file, we simply remove it.
        // - If not, we check until we find a free path using "_X". This also takes into account extensions, so "m.loc" will become "m_1.loc".
        if source_exists && destination_exists {
            if overwrite { self.remove_packed_file_by_path(&destination_path); }
            else {
                let name_current = destination_path.last().unwrap().to_owned();
                let name_split = name_current.split('.').collect::<Vec<&str>>();
                let name = name_split[0];
                let extension = if name_split.len() > 1 { name_split[1..].join(".") } else { "".to_owned() };
                for number in 0.. {
                    let name = if extension.is_empty() { format!("{}_{}", name, number) } else { format!("{}_{}.{}", name, number, extension) };
                    *destination_path.last_mut().unwrap() = name;
                    if !self.packedfile_exists(&destination_path) && !reserved_names.contains(&destination_path) {
                        break;
                    }
                }
            }
        }

        // Then just change the path of the `PackedFile` if exists. Return error if it doesn't.
        match self.get_ref_mut_packed_file_by_path(source_path) {
            Some(packed_file) => {
                packed_file.get_ref_mut_raw().set_path(&destination_path)?;
                Ok(destination_path)
            },
            None => Err(ErrorKind::PackedFileNotFound.into())
        }
    }

    /// This function allows you to change the name of a folder inside a `PackFile`.
    ///
    /// By default this append a `_number` to the file names in case of collision. If you want it to overwrite instead,
    /// pass `overwrite` as `true`. This can fail if you pass it an empty or reserved path, so make sure you check the result.
    ///
    /// We return the list of source/final paths of each moved PackedFile, if it worked, or an error.
    pub fn move_folder(
        &mut self,
        source_path: &[String],
        destination_path: &[String],
        overwrite: bool,
    ) -> Result<Vec<(Vec<String>, Vec<String>)>> {

        // First, ensure we can move between the paths.
        if source_path.is_empty() || destination_path.is_empty() { return Err(ErrorKind::EmptyInput.into()) }
        if source_path == destination_path { return Err(ErrorKind::PathsAreEqual.into()) }

        // Next... just get all the PackedFiles to move, and move them one by one.
        let mut successes = vec![];
        for packed_file_current_path in self.get_ref_packed_files_by_path_start(source_path).iter().map(|x| x.get_path().to_vec()).collect::<Vec<Vec<String>>>() {
            let mut new_path = packed_file_current_path.to_vec();
            new_path.splice(..source_path.len(), destination_path.iter().cloned());
            if let Ok(new_path) = self.move_packedfile(&packed_file_current_path, &new_path, overwrite) {
                successes.push((packed_file_current_path, new_path))
            }
        }

        Ok(successes)
    }

    /// This function is used to rename one or more `PackedFile`/Folder inside a `PackFile`.
    ///
    /// It returns the list of "Original Path/New Path" of each renamed PackedFile.
    ///
    /// This doesn't stop on failure. Instead, if a rename fails, it skips that PackedFile from the rename process.
    ///
    /// If `overwrite` is set to `true`, in case of destination `PackedFile` already existing, it'll be overwritten.
    /// If set to `false`, the file will be renamed to 'xxx_1', or the first number available. Extensions are taken
    /// into account when doing this, so 'x.loc' will become 'x_1.loc'.
    pub fn rename_packedfiles(
        &mut self,
        renaming_data: &[(PathType, String)],
        overwrite: bool
    ) -> Vec<(PathType, Vec<String>)> {

        let mut successes = vec![];
        for (item_type, new_name) in renaming_data {

            // Skip items with empty new names.
            if new_name.is_empty() { continue; }

            // We only allow to rename files and folders.
            match item_type {
                PathType::File(ref path) => {
                    let mut new_path = path.to_vec();
                    *new_path.last_mut().unwrap() = new_name.to_owned();
                    if let Ok(destination_path) = self.move_packedfile(path, &new_path, overwrite) {
                        successes.push((item_type.clone(), destination_path));
                    }
                }

                PathType::Folder(ref path) => {
                    let mut new_path = path.to_vec();
                    *new_path.last_mut().unwrap() = new_name.to_owned();
                    if let Ok(result) = self.move_folder(path, &new_path, overwrite) {
                        result.iter().map(|(x, y)| (PathType::File(x.to_vec()), y.to_vec())).for_each(|x| successes.push(x));
                    }
                }

                // PackFiles and errors are skipped.
                PathType::PackFile | PathType::None => continue,
            }
        }

        // Return the list of successes.
        successes
    }

    /// This function merges (if possible) the provided DB and LOC tables into one with the provided name.
    ///
    /// NOTE: The merged table will be created in the folder of the first provided file.
    pub fn merge_tables(
        &mut self,
        paths: &[Vec<String>],
        name: &str,
        delete_source_paths: bool,
    ) -> Result<Vec<String>> {

        // Get the schema, as we'll need it unlocked to decode all the files fast.
        let schema = SCHEMA.read().unwrap();
        let schema = if let Some(ref schema) = *schema { schema } else { return Err(ErrorKind::SchemaNotFound.into()) };

        let mut db_files = vec![];
        let mut loc_files = vec![];

        // Decode the files and put them in their respective list.
        for path in paths {
            if let Some(packed_file) = self.get_ref_mut_packed_file_by_path(path) {
                match packed_file.decode_return_ref_no_locks(schema)? {
                    DecodedPackedFile::DB(table) => db_files.push(table.clone()),
                    DecodedPackedFile::Loc(table) => loc_files.push(table.clone()),
                    _ => return Err(ErrorKind::InvalidFilesForMerging.into())
                }
            }
        }

        // If we have no tables, or we have both, db and loc, return an error. If we have only tables, but different tables, we also return an error.
        if (!db_files.is_empty() && !loc_files.is_empty()) || (db_files.is_empty() && loc_files.is_empty()) ||
        (!db_files.is_empty() && !db_files.iter().all(|x| x.name == db_files[0].name)) { return Err(ErrorKind::InvalidFilesForMerging.into()) }

        // If we have db tables, get their newest definition, update all the tables to that definition if needed,
        // and then merge all their data in one table.
        let merged_table = if !db_files.is_empty() {
            let db_files = if db_files.iter().all(|x| x.get_definition().get_version() == db_files[0].get_definition().get_version()) { db_files }
            else {
                let definition = db_files.iter().map(|x| x.get_definition()).max_by_key(|x| x.get_version()).unwrap();
                for table in &mut db_files { table.set_definition(&definition); }
                db_files
            };
            let mut new_table = DB::new(&db_files[0].name, None, &db_files[0].get_definition());
            let mut entries = vec![];
            db_files.iter().for_each(|x| entries.extend_from_slice(x.get_ref_table_data()));
            new_table.set_table_data(&entries)?;
            DecodedPackedFile::DB(new_table)
        }

        // Same thing for locs.
        else if !loc_files.is_empty() {
            let loc_files = if loc_files.iter().all(|x| x.get_definition().get_version() == loc_files[0].get_definition().get_version()) { loc_files }
            else {
                let definition = loc_files.iter().map(|x| x.get_definition()).max_by_key(|x| x.get_version()).unwrap();
                for table in &mut loc_files { table.set_definition(&definition); }
                loc_files
            };
            let mut new_table = Loc::new(&loc_files[0].get_definition());
            let mut entries = vec![];
            loc_files.iter().for_each(|x| entries.extend_from_slice(x.get_ref_table_data()));
            new_table.set_table_data(&entries)?;
            DecodedPackedFile::Loc(new_table)
        } else { unimplemented!() };

        // And then, we save the newly created table to a `PackedFile`.
        let mut path = paths[0].to_vec();
        path.pop();
        path.push(name.to_owned());
        let packed_file = PackedFile::new_from_decoded(&merged_table, &path);

        // If we want to remove the source files, this is the moment.
        if delete_source_paths { paths.iter().for_each(|x| self.remove_packed_file_by_path(x)); }

        // Prepare the paths to return.
        self.add_packed_file(&packed_file, true)
    }

    /// This function is used to optimize a `PackFile` by removing extra useless data from it.
    ///
    /// Currently, this function does the following:
    /// - Sort tables by their first key.
    /// - Remove duplicated rows on DB tables.
    /// - Remove duplicated rows on Loc tables.
    /// - Remove default rows from DB tables (except if the table has the same name as his vanilla counterpart and certain setting is enabled).
    /// - Remove default rows from Loc tables (except if the table has the same name as his vanilla counterpart and certain setting is enabled).
    /// - Remove unchanged data from DB tables (except if the table has the same name as his vanilla counterpart and certain setting is enabled).
    /// - Remove unchanged data from Loc tables (except if the table has the same name as his vanilla counterpart and certain setting is enabled).
    /// - Remove empty DB tables (except if the table has the same name as his vanilla counterpart and certain setting is enabled).
    /// - Remove empty Loc tables (except if the table has the same name as his vanilla counterpart and certain setting is enabled).
    /// - Remove XML files in map folders.
    /// - Remove files identical to Parent/Vanilla files (if is identical to vanilla, but a parent mod overwrites it, it ignores it).
    pub fn optimize(&mut self, dependencies: &Dependencies) -> Result<Vec<Vec<String>>> {

        // We can only optimize if we have vanilla data available.
        if !dependencies.game_has_vanilla_data_loaded(true) {
            return Err(ErrorKind::DependenciesCacheNotGeneratedorOutOfDate.into());
        }

        // List of PackedFiles to delete.
        let mut files_to_delete: Vec<Vec<String>> = vec![];

        // First, do a hash pass over all the files, and mark for removal those that match by path and hash with vanilla/parent ones.
        let packedfiles_paths = self.get_ref_packed_files_all_paths().iter().map(|x| PathType::File(x.to_vec())).collect::<Vec<PathType>>();
        let mut dependencies_overwritten_files = dependencies.get_most_relevant_files_by_paths(&packedfiles_paths);
        files_to_delete.append(&mut dependencies_overwritten_files.iter_mut().filter_map(|dep_packed_file| {
            if let Some(packed_file) = self.get_ref_mut_packed_file_by_path(dep_packed_file.get_path()) {
                if let Ok(local_hash) = packed_file.get_hash_from_data() {
                    if let Ok(dependency_hash) = dep_packed_file.get_hash_from_data() {
                        if local_hash == dependency_hash {
                            Some(packed_file.get_path().to_vec())
                        } else { None }
                    } else { None }
                } else { None }
            } else { None }
        }).collect());

        // Then, do a second pass, this time over the decodeable files that we can optimize.
        if let Some(ref schema) = *SCHEMA.read().unwrap() {
            let dependencies = dependencies.get_db_and_loc_tables_from_cache(true, true, true, true)?;

            // Get a list of every Loc and DB PackedFiles in our dependency's files.
            let (game_dbs, game_locs): (Vec<&DB>, Vec<&Loc>) = dependencies.iter()
                .filter_map(|packed_file| packed_file.get_decoded_from_memory().ok())
                .partition_map(|x| match x {
                    DecodedPackedFile::DB(db) => Either::Left(db),
                    DecodedPackedFile::Loc(loc) => Either::Right(loc),
                    _ => unreachable!()
                }
            );

            // Then, we optimize the data inside the `PackedFiles`.
            let packed_files_to_optimize = self.get_ref_mut_packed_files_by_types(&[PackedFileType::DB, PackedFileType::Loc, PackedFileType::Text(TextType::Plain)], false);
            files_to_delete.append(&mut packed_files_to_optimize.into_par_iter().filter_map(|packed_file|{

                // Only check it if it's not already marked for deletion.
                if files_to_delete.iter().all(|path| packed_file.get_path() != path) {
                    let path = packed_file.get_path().to_vec();

                    // Unless we specifically wanted to, ignore the same-name-as-vanilla files,
                    // as those are probably intended to overwrite vanilla files, not to be optimized.
                    let can_be_optimized = SETTINGS.read().unwrap().settings_bool["optimize_not_renamed_packedfiles"] || (
                        !SETTINGS.read().unwrap().settings_bool["optimize_not_renamed_packedfiles"] &&
                        dependencies.iter().map(|x| x.get_path()).all(|x| x != path)
                    );

                    if can_be_optimized {
                        match PackedFileType::get_packed_file_type(packed_file.get_ref_raw(), false) {
                            PackedFileType::DB => {
                                if let Ok(DecodedPackedFile::DB(db)) = packed_file.decode_return_ref_mut_no_locks(schema) {
                                    let is_empty = db.optimize_table(&game_dbs);
                                    if is_empty {
                                        return Some(path.to_vec());
                                    }
                                }
                            }

                            PackedFileType::Loc => {
                                if let Ok(DecodedPackedFile::Loc(loc)) = packed_file.decode_return_ref_mut_no_locks(schema) {
                                    let is_empty = loc.optimize_table(&game_locs);
                                    if is_empty {
                                        return Some(path.to_vec());
                                    }
                                }
                            }

                            PackedFileType::Text(text_type) => {
                                if !path.is_empty() && path.starts_with(&Self::get_terry_map_path()) && text_type == TextType::Xml {
                                    return Some(path.to_vec());
                                }
                            }

                            // Ignore the rest.
                            _ => {}
                        }
                    }
                }

                None
            }).collect());
        }

        // Delete all the files marked for deletion.
        files_to_delete.iter().for_each(|x| self.remove_packed_file_by_path(x));

        // Return the deleted files, so the caller can know what got removed.
        Ok(files_to_delete)
    }

    /// This function is used to generate all loc entries missing from a PackFile into a missing.loc file.
    pub fn generate_missing_loc_data(&mut self, schema: &Schema) -> Result<Vec<String>> {

        let db_tables = self.get_ref_packed_files_by_type(PackedFileType::DB, false);
        let loc_tables = self.get_ref_packed_files_by_type(PackedFileType::Loc, false);
        let mut missing_trads_file = Loc::new(schema.get_ref_last_definition_loc().unwrap());

        db_tables.iter().for_each(|packed_file| {
            if let DecodedPackedFile::DB(table) = packed_file.get_decoded_from_memory().unwrap() {
                let definition = table.get_ref_definition();
                if !definition.get_localised_fields().is_empty() && definition.get_fields_processed().iter().filter(|x| x.get_is_key()).count() > 1 {
                    println!("{}, keys: {}", table.get_table_name_without_tables(), definition.get_fields_processed().iter().filter(|x| x.get_is_key()).count());
                }
            }
        });

        let loc_keys_from_memory = loc_tables.par_iter().filter_map(|packed_file| {
            if let DecodedPackedFile::Loc(table) = packed_file.get_decoded_from_memory().unwrap() {
                Some(table.get_ref_table_data().iter().filter_map(|x| {
                    if let DecodedData::StringU16(data) = &x[0] {
                        Some(data.as_str())
                    } else {
                        None
                    }
                }).collect::<HashSet<&str>>())
            } else { None }
        }).flatten().collect::<HashSet<&str>>();

        let missing_trads_file_table_data = db_tables.par_iter().filter_map(|packed_file| {
            if let DecodedPackedFile::DB(table) = packed_file.get_decoded_from_memory().unwrap() {
                let definition = table.get_ref_definition();
                let loc_fields = definition.get_localised_fields();
                let processed_fields = definition.get_fields_processed();
                if !loc_fields.is_empty() {
                    let table_data = table.get_ref_table_data();
                    let table_name = table.get_table_name_without_tables();

                    // Get the keys, which may be concatenated. We get them IN THE ORDER THEY ARE IN THE BINARY FILE.
                    let key_field_names = definition.get_ref_fields().iter().filter_map(|field| if field.get_is_key() { Some(field.get_name()) } else { None }).collect::<Vec<&str>>();
                    let key_field_positions = key_field_names.iter().filter_map(|name| processed_fields.iter().position(|field| field.get_name() == *name)).collect::<Vec<usize>>();

                    let mut new_rows = vec![];

                    for row in table_data {
                        for loc_field in loc_fields {
                            let key = key_field_positions.iter().map(|pos| row[*pos].data_to_string()).join("");
                            let loc_key = format!("{}_{}_{}", table_name, loc_field.get_name(), key);

                            if loc_keys_from_memory.get(&*loc_key).is_none() {
                                let mut new_row = missing_trads_file.get_new_row();
                                new_row[0] = DecodedData::StringU16(loc_key);
                                new_row[1] = DecodedData::StringU16("PLACEHOLDER".to_owned());
                                new_rows.push(new_row);
                            }
                        }
                    }

                    return Some(new_rows)
                }
            }
            None
        }).flatten().collect::<Vec<Vec<DecodedData>>>();

        // Save the missing translations to a missing_locs.loc file.
        let _ = missing_trads_file.set_table_data(&missing_trads_file_table_data);
        if !missing_trads_file_table_data.is_empty() {
            let packed_file = PackedFile::new_from_decoded(&DecodedPackedFile::Loc(missing_trads_file), &["text".to_owned(), "missing_locs.loc".to_owned()]);
            self.add_packed_file(&packed_file, true)
        } else {
            Ok(vec![])
        }
    }

    /// This function is used to patch Warhammer Siege map packs so their AI actually works.
    ///
    /// This also removes the useless xml files left by Terry in the `PackFile`.
    pub fn patch_siege_ai(&mut self) -> Result<(String, Vec<Vec<String>>)> {

        // If there are no files, directly return an error.
        if self.packed_files.is_empty() {
            return Err(ErrorKind::PatchSiegeAIEmptyPackFile.into())
        }

        let mut files_patched = 0;
        let mut files_to_delete: Vec<Vec<String>> = vec![];
        let mut multiple_defensive_hill_hints = false;

        // We only need to change stuff inside the map folder, so we only check the maps in that folder.
        for packed_file in self.get_ref_mut_packed_files_by_path_start(&Self::get_terry_map_path()) {
            let path = packed_file.get_path();
            let name = path.last().unwrap().clone();

            // The files we need to process are `bmd_data.bin` and all the `catchment_` files the map has.
            if name == DEFAULT_BMD_DATA || (name.starts_with("catchment_") && name.ends_with(".bin")) {
                let data = packed_file.get_ref_mut_raw().get_ref_mut_data_and_keep_it()?;

                // The patching process it's simple. First, we check if there is SiegeAI stuff in the file by checking if there is an Area Node.
                // If we find one, we check if there is a defensive hill hint in the same file, and patch it if there is one.
                if data.windows(19).any(|window: &[u8]|window == SIEGE_AREA_NODE_HINT) {
                    if let Some(index) = data.windows(18).position(|window: &[u8]|window == DEFENSIVE_HILL_HINT) {
                        data.splice(index..index + 18, FORT_PERIMETER_HINT.iter().cloned());
                        files_patched += 1;
                    }

                    // If there is more than one defensive hill in one file, is a valid file, but we want to warn the user about it.
                    if data.windows(18).any(|window: &[u8]|window == DEFENSIVE_HILL_HINT) {
                        multiple_defensive_hill_hints = true;
                    }
                }
            }

            // All xml in this folder are useless, so we mark them all for deletion.
            else if name.ends_with(".xml") {
                files_to_delete.push(packed_file.get_path().to_vec());
            }
        }

        // If there are files to delete, we delete them.
        files_to_delete.iter().for_each(|x| self.remove_packed_file_by_path(x));

        // If we didn't found any file to patch or delete, return an error.
        if files_patched == 0 && files_to_delete.is_empty() { Err(ErrorKind::PatchSiegeAINoPatchableFiles.into()) }

        // TODO: make this more.... `fluent`.
        // If we found files to delete, but not to patch, return a message reporting it.
        else if files_patched == 0 {
            Ok((format!("No file suitable for patching has been found.\n{} files deleted.", files_to_delete.len()), files_to_delete))
        }

        // If we found multiple defensive hill hints... it's ok, but we return a warning.
        else if multiple_defensive_hill_hints {

            // The message is different depending on the amount of files deleted.
            if files_to_delete.is_empty() {
                Ok((format!("{} files patched.\nNo file suitable for deleting has been found.\
                \n\n\
                WARNING: Multiple Defensive Hints have been found and we only patched the first one.\
                 If you are using SiegeAI, you should only have one Defensive Hill in the map (the \
                 one acting as the perimeter of your fort/city/castle). Due to SiegeAI being present, \
                 in the map, normal Defensive Hills will not work anyways, and the only thing they do \
                 is interfere with the patching process. So, if your map doesn't work properly after \
                 patching, delete all the extra Defensive Hill Hints. They are the culprit.",
                 files_patched), files_to_delete))
            }
            else {
                Ok((format!("{} files patched.\n{} files deleted.\
                \n\n\
                WARNING: Multiple Defensive Hints have been found and we only patched the first one.\
                 If you are using SiegeAI, you should only have one Defensive Hill in the map (the \
                 one acting as the perimeter of your fort/city/castle). Due to SiegeAI being present, \
                 in the map, normal Defensive Hills will not work anyways, and the only thing they do \
                 is interfere with the patching process. So, if your map doesn't work properly after \
                 patching, delete all the extra Defensive Hill Hints. They are the culprit.",
                files_patched, files_to_delete.len()), files_to_delete))
            }
        }

        // If no files to delete were found, but we got files patched, report it.
        else if files_to_delete.is_empty() {
            Ok((format!("{} files patched.\nNo file suitable for deleting has been found.", files_patched), files_to_delete))
        }

        // And finally, if we got some files patched and some deleted, report it too.
        else {
            Ok((format!("{} files patched.\n{} files deleted.", files_patched, files_to_delete.len()), files_to_delete))
        }
    }


    /// This function is used to Mass-Import TSV files into a PackFile.
    pub fn mass_import_tsv(
        &mut self,
        tsv_paths: &[PathBuf],
        name: Option<String>,
        overwrite: bool
    ) -> Result<(Vec<Vec<String>>, Vec<Vec<String>>)> {

        // Create the following lists:
        // - PackedFiles to add.
        // - PackedFiles to remove.
        // - Paths with errors.
        let mut packed_files: Vec<PackedFile> = vec![];
        let mut packed_files_to_remove = vec![];
        let mut error_files = vec![];

        // If there is not a schema, don't do anything.
        if let Some(ref schema) = *SCHEMA.read().unwrap() {
            for path in tsv_paths {

                // The first row has the PackedFile Type (or name, in case of tables) and version.
                // The second row contains the column names, and it can be ignored.
                let mut tsv = String::new();
                BufReader::new(File::open(&path)?).read_to_string(&mut tsv)?;

                // We get his second line, if it has it. Otherwise, we return an error in this file.
                let mut iter = tsv.lines();
                iter.next();
                if let Some(line) = iter.next() {

                    // Split the first line by \t so we can get the info of the table.
                    // We expect to have 2 or 3 items here. If we have more or less, stop.
                    let tsv_info = line.split(';').collect::<Vec<&str>>();
                    if tsv_info.len() == 2 || tsv_info.len() == 3 {

                        // Get the type of the table.
                        let mut table_type = tsv_info[0].to_owned();
                        if table_type.starts_with("#") {
                            table_type.remove(0);
                        }

                        // Get the definition, depending on the table type and version.
                        // If the name is not specific for a type of file, we treat it as a DB Table.
                        match &*table_type {
                            TSV_NAME_LOC => {
                                if let Ok((table, file_path)) = Loc::import_tsv(schema, path) {

                                    // Depending on the name received, call it one thing or another.
                                    let mut path = match name {
                                        Some(ref name) => vec!["text".to_owned(), "db".to_owned(), if name.ends_with(".loc") { name.to_string() } else { format!("{}.loc", name) }],
                                        None => match file_path {
                                            Some(file_path) => file_path,
                                            None => vec!["text".to_owned(), "db".to_owned(), format!("{}.loc", path.file_stem().unwrap().to_str().unwrap())],
                                        },
                                    };

                                    let name = path.last().unwrap().to_owned();

                                    // If that path already exists in the list of new PackedFiles to add, change it using the index.
                                    if !overwrite {
                                        let mut index = 1;
                                        while packed_files.iter().any(|x| x.get_path() == &*path) {
                                            path[2] = format!("{}_{}.loc", name, index);
                                            index += 1;
                                        }
                                    }

                                    // If that path already exist in the PackFile, add it to the "remove" list.
                                    if self.packedfile_exists(&path) { packed_files_to_remove.push(path.to_vec()) }

                                    // Create and add the new PackedFile to the list of PackedFiles to add.
                                    let mut packed_file = PackedFile::new(path, self.get_file_name());
                                    packed_file.set_decoded(&DecodedPackedFile::Loc(table));
                                    packed_files.push(packed_file);
                                }
                                else { error_files.push(path.to_string_lossy().to_string()); }
                            }
                            _ => {
                                if let Ok((table, file_name)) = DB::import_tsv(schema, path) {

                                    // Depending on the name received, call it one thing or another.
                                    let mut path = match name {
                                        Some(ref name) => vec!["db".to_owned(), table_type.to_owned(), name.to_owned()],
                                        None => match file_name {
                                            Some(name) => name,
                                            None => vec!["db".to_owned(), table_type.to_owned(), path.file_stem().unwrap().to_str().unwrap().to_string()],
                                        },
                                    };

                                    let name = path.last().unwrap().to_owned();

                                    // If that path already exists in the list of new PackedFiles to add, change it using the index.
                                    if !overwrite {
                                        let mut index = 1;
                                        while packed_files.iter().any(|x| x.get_path() == &*path) {
                                            path[2] = format!("{}_{}", name, index);
                                            index += 1;
                                        }
                                    }

                                    // If that path already exists in the PackFile, add it to the "remove" list.
                                    if self.packedfile_exists(&path) { packed_files_to_remove.push(path.to_vec()) }

                                    // Create and add the new PackedFile to the list of PackedFiles to add.
                                    let mut packed_file = PackedFile::new(path, self.get_file_name());
                                    packed_file.set_decoded(&DecodedPackedFile::DB(table));
                                    packed_files.push(packed_file);
                                }
                                else { error_files.push(path.to_string_lossy().to_string()); }
                            }
                        }
                    }
                    else { error_files.push(path.to_string_lossy().to_string()); }
                }
                else { error_files.push(path.to_string_lossy().to_string()); }
            }

            // If any of the files returned error, return error.
            if !error_files.is_empty() {
                let error_files_string = error_files.iter().map(|x| format!("<li>{}</li>", x)).collect::<String>();
                return Err(ErrorKind::MassImport(error_files_string).into())
            }

            // Get the "TreePath" of the new PackFiles to return them.
            let tree_path = packed_files.iter().map(|x| x.get_path().to_vec()).collect::<Vec<Vec<String>>>();

            // Remove all the "conflicting" PackedFiles from the PackFile, before adding the new ones.
            for packed_file_to_remove in &packed_files_to_remove {
                self.remove_packed_file_by_path(packed_file_to_remove);
            }

            // We add all the files to the PackFile, and return success.
            let packed_files_to_add = packed_files.iter().collect::<Vec<&PackedFile>>();
            self.add_packed_files(&packed_files_to_add, true, true)?;
            Ok((packed_files_to_remove, tree_path))
        }
        else {
            Err(ErrorKind::SchemaNotFound.into())
        }
    }

    /// This function is used to Mass-Export TSV files from a PackFile.
    ///
    /// NOTE: this will OVERWRITE any existing file that has a name conflict with the TSV files provided.
    pub fn mass_export_tsv(&mut self, path_types: &[PathType], export_path: &Path) -> Result<String> {

        // Lists of PackedFiles that couldn't be exported for one thing or another and exported PackedFile names,
        // so we make sure we don't overwrite those with the following ones.
        let mut error_list = vec![];
        let mut exported_files = vec![];

        // We need the schema to export. If there is no schema, return an error.
        match *SCHEMA.read().unwrap() {
            Some(ref schema) => {

                // Keep the PathTypes added so we can return them to the UI easily.
                let path_types = PathType::dedup(path_types);
                let paths = self.get_paths_from_path_types(&path_types);
                let paths_ref = paths.par_iter().map(|x| (*x).as_ref()).collect::<Vec<&[String]>>();

                let mut packed_files = self.get_ref_mut_packed_files_by_paths(paths_ref);

                // Decode the entire set of PackedFiles we want to export.
                packed_files.iter_mut().for_each(|packed_file| {
                    let path = packed_file.get_path().to_vec();
                    match packed_file.decode_return_ref_no_locks(schema) {
                        Ok(data) => match data {
                            DecodedPackedFile::DB(data) => {

                                // His name will be "db_name_file_name.tsv". If that's taken, we'll add an index until we find one available.
                                let mut name = format!("{}_{}.tsv", path[1], path.last().unwrap().to_owned());
                                let mut export_path = export_path.to_path_buf();

                                // Checks to avoid overwriting exported files go here, in an infinite loop of life and death.
                                let mut index = 1;
                                while exported_files.contains(&name) {
                                    name = format!("{}_{}_{}.tsv", path[1], path.last().unwrap().to_owned(), index);
                                    index += 1;
                                }

                                export_path.push(name.to_owned());
                                match data.export_tsv(&export_path, &path[1], &path) {
                                    Ok(_) => exported_files.push(name),
                                    Err(error) => error_list.push((packed_file.get_path().join("\\"), error)),
                                }

                            }
                            DecodedPackedFile::Loc(data) => {

                                // His name will be "db_name_file_name.tsv". If that's taken, we'll add an index until we find one available.
                                let mut name = format!("{}.tsv", path.last().unwrap().to_owned());
                                let mut export_path = export_path.to_path_buf();

                                // Checks to avoid overwriting exported files go here, in an infinite loop of life and death.
                                let mut index = 1;
                                while exported_files.contains(&name) {
                                    name = format!("{}_{}.tsv", path.last().unwrap().to_owned(), index);
                                    index += 1;
                                }

                                export_path.push(name.to_owned());
                                match data.export_tsv(&export_path, TSV_NAME_LOC, &path) {
                                    Ok(_) => exported_files.push(name),
                                    Err(error) => error_list.push((packed_file.get_path().join("\\"), error)),
                                }
                            }

                            // Ignore any other PackedFiles.
                            _ => {}
                        }
                        Err(error) => error_list.push((packed_file.get_path().join("\\"), error)),
                    }
                });
            }
            None => return Err(Error::from(ErrorKind::SchemaNotFound)),
        }

        // If there has been errors, return ok with the list of errors.
        if !error_list.is_empty() {
            let error_files_string = error_list.iter().map(|x| format!("<li>{}</li>", x.0)).collect::<String>();
            Ok(format!("<p>All exportable files have been exported, except the following ones:</p><ul>{}</ul>", error_files_string))
        }

        // Otherwise, just return success and an empty error list.
        else { Ok("<p>All exportable files have been exported.</p>".to_owned()) }
    }

    /// This function loads a `PackFile` as dependency, loading all his dependencies in the process.
    fn load_single_dependency_packfile(
        packed_files: &mut HashMap<String, PackedFile>,
        cached_packed_files: &mut HashMap<String, CachedPackedFile>,
        packfile_name: &str,
        already_loaded_dependencies: &mut Vec<String>,
        data_paths: &Option<Vec<PathBuf>>,
        contents_paths: &Option<Vec<PathBuf>>,
    ) {
        // Do not process already processed packfiles.
        if !already_loaded_dependencies.contains(&packfile_name.to_owned()) {

            // First we load the content `PackFiles`.
            if let Some(ref paths) = contents_paths {
                if let Some(path) = paths.iter().find(|x| x.file_name().unwrap().to_string_lossy() == packfile_name) {
                    if let Ok(pack_file) = PackFile::open_packfiles(&[path.to_path_buf()], true, false, false) {

                        // Add the current `PackFile` to the done list, so we don't get into cyclic dependencies.
                        already_loaded_dependencies.push(packfile_name.to_owned());
                        pack_file.get_packfiles_list().iter().for_each(|x| Self::load_single_dependency_packfile(packed_files, cached_packed_files, x, already_loaded_dependencies, data_paths, contents_paths));
                        for packed_file in pack_file.get_ref_packed_files_by_types(&[PackedFileType::DB, PackedFileType::Loc], false) {
                            packed_files.insert(packed_file.get_path().join("/"), packed_file.clone());
                        }

                        for packed_file in pack_file.get_ref_packed_files_all() {
                            if let Ok(cached_packed_file) = CachedPackedFile::new_from_packed_file(packed_file) {
                                cached_packed_files.insert(cached_packed_file.get_ref_packed_file_path().to_owned(), cached_packed_file);
                            }
                        }
                    }
                }
            }

            // Then we load the data `PackFiles`.
            if let Some(ref paths) = data_paths {
                if let Some(path) = paths.iter().find(|x| x.file_name().unwrap().to_string_lossy() == packfile_name) {
                    if let Ok(pack_file) = PackFile::open_packfiles(&[path.to_path_buf()], true, false, false) {

                        // Add the current `PackFile` to the done list, so we don't get into cyclic dependencies.
                        already_loaded_dependencies.push(packfile_name.to_owned());
                        pack_file.get_packfiles_list().iter().for_each(|x| Self::load_single_dependency_packfile(packed_files, cached_packed_files, x, already_loaded_dependencies, data_paths, contents_paths));
                        for packed_file in pack_file.get_ref_packed_files_by_types(&[PackedFileType::DB, PackedFileType::Loc], false) {
                            packed_files.insert(packed_file.get_path().join("/"), packed_file.clone());
                        }

                        for packed_file in pack_file.get_ref_packed_files_all() {
                            if let Ok(cached_packed_file) = CachedPackedFile::new_from_packed_file(packed_file) {
                                cached_packed_files.insert(cached_packed_file.get_ref_packed_file_path().to_owned(), cached_packed_file);
                            }
                        }
                    }
                }
            }
        }
    }

    /// This function loads to memory the custom (made by modders) dependencies of a `PackFile`.
    /// The packed_files Vec is for the PackedFiles that needs decoding.
    /// The cached one is for all files, to set up quickaccess to them.
    ///
    /// To avoid entering into an infinite loop while calling this recursively, we have to pass the
    /// list of loaded `PackFiles` each time we execute this.
    pub fn load_custom_dependency_packfiles(
        packed_files: &mut HashMap<String, PackedFile>,
        cached_packed_files: &mut HashMap<String, CachedPackedFile>,
        pack_file_names: &[String],
    ) {

        let data_packs_paths = GAME_SELECTED.read().unwrap().get_data_packfiles_paths();
        let content_packs_paths = GAME_SELECTED.read().unwrap().get_content_packfiles_paths();
        let mut loaded_packfiles = vec![];

        pack_file_names.iter().for_each(|x| Self::load_single_dependency_packfile(packed_files, cached_packed_files, x, &mut loaded_packfiles, &data_packs_paths, &content_packs_paths));
    }

    /// This function allows you to open all CA PackFiles as one for the currently selected Game.
    ///
    /// This function tries to get the list of CA PackFile of the currently selected game from the manifest.txt on /data,
    /// then it tries to open them all as one. Simple and effective.
    pub fn open_all_ca_packfiles() -> Result<Self> {
        let pack_file_paths = GAME_SELECTED.read().unwrap().get_all_ca_packfiles_paths()?;
        Self::open_packfiles(&pack_file_paths, true, true, true)
    }

    /// This function allows you to open one or more `PackFiles`.
    ///
    /// The way it works:
    /// - If you open just one `PackFile`, it just calls the `PackFile::read()` function on it.
    /// - If you open multiple `PackFiles`, it merges them into one, taking care of conflicts the same way the game does.
    ///
    /// You can also make it ignore mod PackFiles, so it only open `PackFiles` released by CA, and can choose to lock it,
    /// so the user cannot save it (avoiding the *"I tried to save and got an out-of-memory error!!!"* problem).
    pub fn open_packfiles(
        packs_paths: &[PathBuf],
        use_lazy_loading: bool,
        ignore_mods: bool,
        lock_packfile: bool
    ) -> Result<Self> {

        // If we just have one `PackFile`, just read it. No fancy logic needed. If you're an asshole and tried to break this
        // by passing it no paths, enjoy the error.
        if packs_paths.is_empty() { return Err(ErrorKind::PackFileNoPathProvided.into()) }
        if packs_paths.len() == 1 { Self::read(&packs_paths[0], use_lazy_loading) }

        // Otherwise, read all of them into a *fake* `PackFile` and take care of the duplicated files like the game will do.
        else {

            // We have to ensure the paths are sorted and valid. Otherwise, this can go to hell.
            let mut packs_paths = packs_paths.iter().filter(|x| x.is_file()).collect::<Vec<&PathBuf>>();
            packs_paths.sort_by_key(|x| x.file_name().unwrap().to_string_lossy().to_string());

            let pfh_version = GAME_SELECTED.read().unwrap().get_pfh_version_by_file_type(PFHFileType::Mod);
            let pfh_name = if ignore_mods { GAME_SELECTED.read().unwrap().get_game_key_name() } else { String::from("merged_mod.pack")};
            let mut pack_file = Self::new_with_name(&pfh_name, pfh_version);

            // Read all the `PackFiles`, one by one, and separate their files by `PFHFileType`.
            let mut boot_files = vec![];
            let mut release_files = vec![];
            let mut patch_files = vec![];
            let mut mod_files = vec![];
            let mut movie_files = vec![];
            for path in packs_paths {
                match Self::read(path, use_lazy_loading) {
                    Ok(mut pack) => match pack.get_pfh_file_type() {
                        PFHFileType::Boot => boot_files.append(&mut pack.packed_files),
                        PFHFileType::Release => release_files.append(&mut pack.packed_files),
                        PFHFileType::Patch => patch_files.append(&mut pack.packed_files),
                        PFHFileType::Mod => if !ignore_mods {
                            mod_files.append(&mut pack.packed_files)
                        },
                        PFHFileType::Movie => movie_files.append(&mut pack.packed_files),

                        // If we find an unknown one, return an error.
                        PFHFileType::Other(_) => return Err(ErrorKind::PackFileTypeUnknown.into()),
                    },
                    Err(error) => return Err(error)
                }
            }

            // The priority in case of collision is:
            // - Same Type: Last to come is the valid one.
            // - Different Type: Last to come is the valid one.
            boot_files.sort_by(|x, y| x.get_path().cmp(y.get_path()));
            boot_files.reverse();
            boot_files.dedup_by(|x, y| x.get_path() == y.get_path());

            release_files.sort_by(|x, y| x.get_path().cmp(y.get_path()));
            release_files.reverse();
            release_files.dedup_by(|x, y| x.get_path() == y.get_path());

            patch_files.sort_by(|x, y| x.get_path().cmp(y.get_path()));
            patch_files.reverse();
            patch_files.dedup_by(|x, y| x.get_path() == y.get_path());

            if !ignore_mods {
                mod_files.sort_by(|x, y| x.get_path().cmp(y.get_path()));
                mod_files.reverse();
                mod_files.dedup_by(|x, y| x.get_path() == y.get_path());
            }

            movie_files.sort_by(|x, y| x.get_path().cmp(y.get_path()));
            movie_files.reverse();
            movie_files.dedup_by(|x, y| x.get_path() == y.get_path());

            pack_file.add_packed_files(&(boot_files.iter().collect::<Vec<&PackedFile>>()), true, false)?;
            pack_file.add_packed_files(&(release_files.iter().collect::<Vec<&PackedFile>>()), true, false)?;
            pack_file.add_packed_files(&(patch_files.iter().collect::<Vec<&PackedFile>>()), true, false)?;

            if !ignore_mods {
                pack_file.add_packed_files(&(mod_files.iter().collect::<Vec<&PackedFile>>()), true, false)?;
            }

            pack_file.add_packed_files(&(movie_files.iter().collect::<Vec<&PackedFile>>()), true, false)?;

            // Set it as type "Other(200)", so we can easily identify it as fake in other places.
            // Used to lock the CA Files.
            if lock_packfile {
                pack_file.set_pfh_file_type(PFHFileType::Other(200));
            }

            // Return the new PackedFiles list.
            Ok(pack_file)
        }
    }

    /// This function reads the content of a PackFile into a `PackFile` struct.
    pub fn read(
        file_path: &Path,
        use_lazy_loading: bool
    ) -> Result<Self> {

        // Check if what we received is even a `PackFile`.
        if !file_path.file_name().unwrap().to_string_lossy().to_string().ends_with(".pack") { return Err(ErrorKind::OpenPackFileInvalidExtension.into()) }

        // Prepare the PackFile to be read and the virtual PackFile to be written.
        let mut pack_file = BufReader::new(File::open(&file_path)?);
        let pack_file_name = file_path.file_name().unwrap().to_string_lossy().to_string();
        let mut pack_file_decoded = Self::new();

        // First, we do some quick checkings to ensure it's a valid PackFile.
        // 24 is the bare minimum that we need to check how a PackFile should be internally, so any file with less than that is not a valid PackFile.
        let pack_file_len = pack_file.get_ref().metadata()?.len();
        if pack_file_len < 24 { return Err(ErrorKind::PackFileHeaderNotComplete.into()) }

        // Check if it has the weird steam-only header, and skip it if found.
        let mut buffer = vec![0; 8];
        pack_file.read_exact(&mut buffer)?;
        let start_index = if buffer.decode_string_u8(0, 3)? == MFH_PREAMBLE { 8 } else { 0 };

        // Create a little buffer to read the basic data from the header of the PackFile.
        let mut buffer = vec![0; 24];
        pack_file.seek(SeekFrom::Start(start_index))?;
        pack_file.read_exact(&mut buffer)?;

        // Start populating our decoded PackFile struct.
        pack_file_decoded.file_path = file_path.to_path_buf();
        pack_file_decoded.pfh_version = PFHVersion::get_version(&buffer.decode_string_u8(0, 4)?)?;
        pack_file_decoded.pfh_file_type = PFHFileType::get_type(buffer.decode_integer_u32(4)? & 15);
        pack_file_decoded.bitmask = PFHFlags::from_bits_truncate(buffer.decode_integer_u32(4)? & !15);

        // Read the data about the indexes to use it later.
        let pack_file_count = buffer.decode_integer_u32(8)?;
        let pack_file_index_size = buffer.decode_integer_u32(12)?;
        let packed_file_count = buffer.decode_integer_u32(16)?;
        let packed_file_index_size = buffer.decode_integer_u32(20)?;

        // Depending on the data we got, prepare to read the header and ensure we have all the bytes we need.
        match pack_file_decoded.pfh_version {

            // PFH6 contains a subheader with some extra data we want to keep.
            PFHVersion::PFH6 => buffer = vec![0; 308],

            PFHVersion::PFH5 | PFHVersion::PFH4 => {
                if (pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) && pack_file_len < 48) ||
                    (!pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) && pack_file_len < 28) { return Err(ErrorKind::PackFileHeaderNotComplete.into()) }

                if pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) { buffer = vec![0; 48]; }
                else { buffer = vec![0; 28]; }
            }

            PFHVersion::PFH3 | PFHVersion::PFH2 => buffer = vec![0; 32],
            PFHVersion::PFH0 => buffer = vec![0; 24],
        }

        // Restore the cursor of the BufReader to 0, so we can read the full header in one go. The first 24 bytes are
        // already decoded but, for the sake of clarity in the positions of the rest of the header stuff, we do this.
        pack_file.seek(SeekFrom::Start(start_index))?;
        pack_file.read_exact(&mut buffer)?;

        // The creation time is a bit of an asshole. Depending on the PackFile Version/Id/Preamble, it uses a type, another or it doesn't exists.
        // Keep in mind that we store his raw value. If you want his legible value, you have to convert it yourself. PFH0 doesn't have it.
        pack_file_decoded.timestamp = match pack_file_decoded.pfh_version {
            PFHVersion::PFH6 | PFHVersion::PFH5 | PFHVersion::PFH4 => i64::from(buffer.decode_integer_u32(24)?),
            PFHVersion::PFH3 | PFHVersion::PFH2 => (buffer.decode_integer_i64(24)? / WINDOWS_TICK) - SEC_TO_UNIX_EPOCH,
            PFHVersion::PFH0 => 0
        };

        if let PFHVersion::PFH6 = pack_file_decoded.pfh_version {
            pack_file_decoded.game_version = buffer.decode_integer_u32(36)?;
            pack_file_decoded.build_number = buffer.decode_integer_u32(40)?;
            pack_file_decoded.authoring_tool = buffer.decode_string_u8_0padded(44, AUTHORING_TOOL_SIZE as usize)?.0;
            pack_file_decoded.extra_subheader_data = buffer[52..].to_vec();
        }

        // Ensure the PackFile has all the data needed for the index. If the PackFile's data is encrypted
        // and the PackFile is PFH5, due to how the encryption works, the data should start in a multiple of 8.
        let mut data_position = u64::from(start_index as u32 + buffer.len() as u32 + pack_file_index_size + packed_file_index_size);
        if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) &&
            pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) &&
            pack_file_decoded.pfh_version == PFHVersion::PFH5 {
            data_position = if (data_position % 8) > 0 { data_position + 8 - (data_position % 8) } else { data_position };
        }
        if pack_file_len < data_position { return Err(ErrorKind::PackFileIndexesNotComplete.into()) }

        // Create the buffers for the indexes data.
        let mut pack_file_index = vec![0; pack_file_index_size as usize];
        let mut packed_file_index = vec![0; packed_file_index_size as usize];

        // Get the data from both indexes to their buffers.
        pack_file.read_exact(&mut pack_file_index)?;
        pack_file.read_exact(&mut packed_file_index)?;

        // Read the PackFile Index.
        let mut pack_file_index_position: usize = 0;

        // First, we decode every entry in the PackFile index and store it. It's encoded in StringU8 terminated in 00,
        // so we just read them char by char until hitting 0, then decode the next one and so on.
        // NOTE: This doesn't deal with encryption, as we haven't seen any encrypted PackFile with data in this index.
        for _ in 0..pack_file_count {
            let pack_file_name = pack_file_index.decode_packedfile_string_u8_0terminated(pack_file_index_position, &mut pack_file_index_position)?;
            pack_file_decoded.pack_files.push(pack_file_name);
        }

        // Depending on the version of the PackFile and his bitmask, the PackedFile index has one format or another.
        let packed_file_index_path_offset = match pack_file_decoded.pfh_version {
            PFHVersion::PFH6 | PFHVersion::PFH5 => {

                // If it has the extended header bit, is an Arena PackFile. These ones use a normal PFH4 index format for some reason.
                if pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) {
                    if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 8 } else { 4 }
                }

                // Otherwise, it's a Warhammer 2 PackFile. These ones have 4 bytes for the size, 4 for the timestamp and 1 for the compression.
                else if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 9 } else { 5 }
            }

            // If it has the last modified date of the PackedFiles, we default to 8. Otherwise, we default to 4.
            PFHVersion::PFH4 => if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 8 } else { 4 }

            // These are like PFH4, but the timestamp has 8 bytes instead of 4.
            PFHVersion::PFH3 | PFHVersion::PFH2 => if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { 12 } else { 4 }

            // There isn't seem to be a bitmask in ANY PFH0 PackFile, so we will assume they didn't even use it back then.
            PFHVersion::PFH0 => 4
        };

        // Prepare the needed stuff to read the PackedFiles.
        let mut index_position: usize = 0;
        let pack_file = Arc::new(Mutex::new(pack_file));
        for packed_files_to_decode in (0..packed_file_count).rev() {

            // Get his size. If it's encrypted, decrypt it first.
            let size = if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) {
                let encrypted_size = packed_file_index.decode_integer_u32(index_position)?;
                decrypt_index_item_file_length(encrypted_size, packed_files_to_decode as u32)
            } else {
                packed_file_index.decode_integer_u32(index_position)?
            };

            // If we have the last modified date of the PackedFiles in the Index, get it. Otherwise, default to 0,
            // so we have something to write in case we want to enable them for our PackFile.
            let timestamp = if pack_file_decoded.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) {
                match pack_file_decoded.pfh_version {
                    PFHVersion::PFH6 | PFHVersion::PFH5 | PFHVersion::PFH4 => {
                        let timestamp = i64::from(packed_file_index.decode_integer_u32(index_position + 4)?);
                        if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) {
                            i64::from(decrypt_index_item_file_length(timestamp as u32, packed_files_to_decode as u32))
                        } else { timestamp }
                    }

                    // We haven't found a single encrypted PFH3/PFH0 PackFile to test, so always assume these are unencrypted. Also, PFH0 doesn't seem to have a timestamp.
                    PFHVersion::PFH3 | PFHVersion::PFH2 => (packed_file_index.decode_integer_i64(index_position + 4)? / WINDOWS_TICK) - SEC_TO_UNIX_EPOCH,
                    PFHVersion::PFH0 => 0,
                }
            } else { 0 };

            // Update his offset, and get his compression data if it has it.
            index_position += packed_file_index_path_offset;
            let is_compressed = if let PFHVersion::PFH5 = pack_file_decoded.pfh_version {
                matches!(packed_file_index.decode_bool(index_position - 1), Ok(true))
            } else { false };

            // Get his path. Like the PackFile index, it's a StringU8 terminated in 00. We get it and split it in folders for easy use.
            let path = if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_INDEX) {
                decrypt_index_item_filename(&packed_file_index[index_position..], size as u8, &mut index_position)
            }
            else { packed_file_index.decode_packedfile_string_u8_0terminated(index_position, &mut index_position)? };
            let path = path.split('\\').map(|x| x.to_owned()).collect::<Vec<String>>();

            // Once we are done, we create the PackedFile and add it to the PackedFile list.
            let raw_data = RawPackedFile::read_from_data(
                path,
                pack_file_name.to_string(),
                timestamp,
                is_compressed,
                if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) { Some(pack_file_decoded.pfh_version) } else { None },
                PackedFileData::OnDisk(RawOnDisk::new(
                    pack_file.clone(),
                    data_position,
                    size,
                    is_compressed,
                    if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) { Some(pack_file_decoded.pfh_version) } else { None },
                ))
            );

            let mut packed_file = PackedFile::new_from_raw(&raw_data);

            // If this is a notes PackedFile, save the notes and forget about the PackedFile. Otherwise, save the PackedFile.
            if packed_file.get_path() == [RESERVED_NAME_NOTES] {
                if let Ok(data) = packed_file.get_raw_data_and_keep_it() {
                    if let Ok(data) = data.decode_string_u8(0, data.len()) {
                        pack_file_decoded.notes = Some(data);
                    }
                }
            }

            else if packed_file.get_path() == [RESERVED_NAME_SETTINGS] {
                if let Ok(data) = packed_file.get_raw_data_and_keep_it() {
                    pack_file_decoded.settings = if let Ok(settings) = PackFileSettings::load(&data) {
                        settings
                    } else {
                        PackFileSettings::default()
                    };
                }
            }
            else {
                pack_file_decoded.packed_files.push(packed_file);
            }

            // Then we move our data position. For encrypted files in PFH5 PackFiles (only ARENA) we have to start the next one in a multiple of 8.
            if pack_file_decoded.bitmask.contains(PFHFlags::HAS_ENCRYPTED_DATA) &&
                pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) &&
                pack_file_decoded.pfh_version == PFHVersion::PFH5 {
                let padding = 8 - (size % 8);
                let padded_size = if padding < 8 { size + padding } else { size };
                data_position += u64::from(padded_size);
            }
            else { data_position += u64::from(size); }
        }

        // If at this point we have not reached the end of the PackFile, there is something wrong with it.
        // NOTE: Arena PackFiles have extra data at the end. If we detect one of those PackFiles, take that into account.
        if pack_file_decoded.pfh_version == PFHVersion::PFH5 && pack_file_decoded.bitmask.contains(PFHFlags::HAS_EXTENDED_HEADER) {
            if data_position + 256 != pack_file_len { return Err(ErrorKind::PackFileSizeIsNotWhatWeExpect(pack_file_len, data_position).into()) }
        }
        else if data_position != pack_file_len { return Err(ErrorKind::PackFileSizeIsNotWhatWeExpect(pack_file_len, data_position).into()) }

        // If we disabled lazy-loading, load every PackedFile to memory.
        if !use_lazy_loading { for packed_file in &mut pack_file_decoded.packed_files { packed_file.get_ref_mut_raw().load_data()?; }}

        // Return our PackFile.
        Ok(pack_file_decoded)
    }

    /// This function tries to save a `PackFile` to a file in the filesystem.
    ///
    /// If no path is passed, the `PackFile` will be saved in his current path.
    /// If a path is passed as `new_path` the `PackFile` will be saved in that path.
    pub fn save(&mut self, new_path: Option<PathBuf>) -> Result<()> {

        // If any of the problematic masks in the header is set or is one of CA's, return an error.
        if !self.is_editable(*SETTINGS.read().unwrap().settings_bool.get("allow_editing_of_ca_packfiles").unwrap()) { return Err(ErrorKind::PackFileIsNonEditable.into()) }

        // If we receive a new path, update it. Otherwise, ensure the file actually exists on disk.
        if let Some(path) = new_path { self.set_file_path(&path)?; }
        else if !self.get_file_path().is_file() { return Err(ErrorKind::PackFileIsNotAFile.into()) }

        // We ensure that all the data is loaded and in his right form (compressed/encrypted) before attempting to save.
        // We need to do this here because we need later on their compressed size.
        for packed_file in &mut self.packed_files {

            // If we decoded it, re-encode it. Otherwise, just load it.
            packed_file.encode()?;

            // Remember: first compress (only PFH5), then encrypt.
            let is_compressible = !matches!(PackedFileType::get_packed_file_type(packed_file.get_ref_raw(), false), PackedFileType::DB | PackedFileType::Loc);
            let (_, data, is_compressed, is_encrypted, should_be_compressed, should_be_encrypted) = packed_file.get_ref_mut_raw().get_data_and_info_from_memory()?;

            // If, in any moment, we enabled/disabled the PackFile compression, compress/decompress the PackedFile. EXCEPT FOR TABLES. NEVER COMPRESS TABLES.
            if !is_compressible {
                *should_be_compressed = false;
            }

            if *should_be_compressed && !*is_compressed {
                *data = compress_data(data)?;
                *is_compressed = true;
            }
            else if !*should_be_compressed && *is_compressed {
                *data = decompress_data(data)?;
                *is_compressed = false;
            }

            // Encryption is not yet supported. Decrypt everything.
            if is_encrypted.is_some() {
                *data = decrypt_packed_file(data);
                *is_encrypted = None;
                *should_be_encrypted = None;
            }
        }

        // Only do this in non-vanilla files.
        if self.pfh_file_type == PFHFileType::Mod || self.pfh_file_type == PFHFileType::Movie {

            // Save notes, if needed.
            if let Some(note) = &self.notes {
                let mut data = vec![];
                data.encode_string_u8(note);
                let raw_data = RawPackedFile::read_from_vec(vec![RESERVED_NAME_NOTES.to_owned()], self.get_file_name(), 0, false, data);
                let packed_file = PackedFile::new_from_raw(&raw_data);
                self.packed_files.push(packed_file);
            }

            // Saving PackFile settings.
            let mut data = vec![];
            data.write_all(to_string_pretty(&self.settings)?.as_bytes())?;
            let raw_data = RawPackedFile::read_from_vec(vec![RESERVED_NAME_SETTINGS.to_owned()], self.get_file_name(), 0, false, data);
            let packed_file = PackedFile::new_from_raw(&raw_data);
            self.packed_files.push(packed_file);
        }

        // For some bizarre reason, if the PackedFiles are not alphabetically sorted they may or may not crash the game for particular people.
        // So, to fix it, we have to sort all the PackedFiles here by path.
        // NOTE: This sorting has to be CASE INSENSITIVE. This means for "ac", "Ab" and "aa" it'll be "aa", "Ab", "ac".
        self.packed_files.sort_unstable_by_key(|a| a.get_path().join("\\").to_lowercase());

        // First we encode the indexes and the data (just in case we compressed it).
        let mut pack_file_index = vec![];
        let mut packed_file_index = vec![];

        for pack_file in &self.pack_files {
            pack_file_index.extend_from_slice(pack_file.as_bytes());
            pack_file_index.push(0);
        }

        for packed_file in &self.packed_files {
            packed_file_index.encode_integer_u32(packed_file.get_ref_raw().get_size());

            // Depending on the version of the PackFile and his bitmask, the PackedFile index has one format or another.
            // In PFH5 case, we don't support saving encrypted PackFiles for Arena. So we'll default to Warhammer 2 format.
            match self.pfh_version {
                PFHVersion::PFH6 | PFHVersion::PFH5 => {
                    if self.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { packed_file_index.encode_integer_u32(packed_file.get_ref_raw().get_timestamp() as u32); }
                    if packed_file.get_ref_raw().get_should_be_compressed() { packed_file_index.push(1); } else { packed_file_index.push(0); }
                }
                PFHVersion::PFH4 => {
                    if self.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { packed_file_index.encode_integer_u32(packed_file.get_ref_raw().get_timestamp() as u32); }
                }
                PFHVersion::PFH3 | PFHVersion::PFH2 => {
                    if self.bitmask.contains(PFHFlags::HAS_INDEX_WITH_TIMESTAMPS) { packed_file_index.encode_integer_i64(packed_file.get_ref_raw().get_timestamp()); }
                }

                // This one doesn't have timestamps, so we just skip this step.
                PFHVersion::PFH0 => {}
            }

            packed_file_index.append(&mut packed_file.get_path().join("\\").as_bytes().to_vec());
            packed_file_index.push(0);
        }

        // Create the file to save to, and save the header and the indexes.
        let mut file = BufWriter::new(File::create(&self.file_path)?);

        // Write the entire header.
        let mut header = vec![];
        header.encode_string_u8(self.pfh_version.get_value());
        header.encode_integer_u32(self.bitmask.bits | self.pfh_file_type.get_value());
        header.encode_integer_u32(self.pack_files.len() as u32);
        header.encode_integer_u32(pack_file_index.len() as u32);
        header.encode_integer_u32(self.packed_files.len() as u32);
        header.encode_integer_u32(packed_file_index.len() as u32);

        // Update the creation time, then save it. PFH0 files don't have timestamp in the headers.
        self.timestamp = get_current_time();
        match self.pfh_version {
            PFHVersion::PFH6 | PFHVersion::PFH5 | PFHVersion::PFH4 => header.encode_integer_u32(self.timestamp as u32),
            PFHVersion::PFH3 | PFHVersion::PFH2 => header.encode_integer_i64((self.timestamp + SEC_TO_UNIX_EPOCH) * WINDOWS_TICK),
            PFHVersion::PFH0 => {}
        };

        if let PFHVersion::PFH6 = self.pfh_version {
            header.encode_integer_u32(SUBHEADER_MARK);
            header.encode_integer_u32(SUBHEADER_VERSION);

            // Just in case the PackFile is not up-to-date, we update it.
            if let Ok(version_number) = GAME_SELECTED.read().unwrap().get_game_selected_exe_version_number() {
                self.set_game_version(version_number);
            }

            header.encode_integer_u32(self.game_version);
            header.encode_integer_u32(self.build_number);

            // Save it as "Made By CA" if the debug setting for it is enabled.
            if SETTINGS.read().unwrap().settings_bool["spoof_ca_authoring_tool"] {
                self.set_authoring_tool(AUTHORING_TOOL_CA)?;
            }

            header.encode_string_u8_0padded(&(self.authoring_tool.to_owned(), 8))?;
            header.extend_from_slice(&self.extra_subheader_data);
        }

        // Write the indexes and the data of the PackedFiles. No need to keep the data, as it has been preloaded before.
        file.write_all(&header)?;
        file.write_all(&pack_file_index)?;
        file.write_all(&packed_file_index)?;
        for packed_file in &self.packed_files {
            let data = packed_file.get_ref_raw().get_raw_data()?;
            file.write_all(&data)?;
        }

        // Remove again the reserved PackedFiles.
        self.remove_packed_file_by_path(&[RESERVED_NAME_NOTES.to_owned()]);
        self.remove_packed_file_by_path(&[RESERVED_NAME_SETTINGS.to_owned()]);

        // If nothing has failed, return success.
        Ok(())
    }
}

/// Implementation of trait `Default` for `PackFile`.
impl Default for PackFile {

    /// This function creates a new empty `PackFile`.
    ///
    /// In reality, this just calls the `new()` function. It's just here for completeness.
    fn default() -> Self {
        Self::new()
    }
}

/// Implementation to create a `PackFileInfo` from a `PackFile`.
impl From<&PackFile> for PackFileInfo {
    fn from(packfile: &PackFile) -> Self {
        Self {
            file_name: packfile.get_file_name(),
            file_path: packfile.file_path.to_path_buf(),
            pfh_version: packfile.pfh_version,
            pfh_file_type: packfile.pfh_file_type,
            bitmask: packfile.bitmask,
            timestamp: packfile.timestamp,
            compression_state: packfile.get_compression_state(),
        }
    }
}

/// Implementation of `Manifest`.
impl Manifest {

    /// This function returns a parsed version of the `manifest.txt` of the Game Selected, if exists and is parsable.
    pub fn read_from_game_selected() -> Result<Self> {
        let mut manifest_path = GAME_SELECTED.read().unwrap().get_data_path().map_err(|_| Error::from(ErrorKind::GameManifestNotFound))?;
        manifest_path.push("manifest.txt");

        let mut reader = ReaderBuilder::new()
            .delimiter(b'\t')
            .quoting(false)
            .has_headers(false)
            .flexible(true)
            .from_path(&manifest_path)
            .map_err(|_| Error::from(ErrorKind::GameManifestNotFound))?;

        // Due to "flexible" not actually working when doing serde-backed deserialization (took some time to figure this out)
        // the deserialization has to be done manually.
        let mut entries = vec![];
        for record in reader.records() {
            let record = record?;

            // We only know these manifest formats.
            if record.len() != 2 && record.len() != 3 {
                return Err(ErrorKind::ManifestError.into());
            } else {
                let mut manifest_entry = ManifestEntry {
                    relative_path: record.get(0).ok_or_else(|| Error::from(ErrorKind::ManifestError))?.to_owned(),
                    size: record.get(1).ok_or_else(|| Error::from(ErrorKind::ManifestError))?.parse()?,
                    ..Default::default()
                };

                // In newer games, a third field has been added.
                if record.len() == 3 {
                    manifest_entry.belongs_to_base_game = record.get(2).ok_or_else(|| Error::from(ErrorKind::ManifestError))?.parse().ok();
                }
                else {
                    manifest_entry.belongs_to_base_game = None;
                }

                entries.push(manifest_entry);
            }
        }

        let manifest = Self(entries);
        Ok(manifest)
    }

    /// This function returns a parsed version of the `manifest.txt` in the folder you provided, if exists and is parsable.
    pub fn read_from_folder(path: &Path) -> Result<Self> {
        let manifest_path = path.join("manifest.txt");

        let mut reader = ReaderBuilder::new()
            .delimiter(b'\t')
            .quoting(false)
            .has_headers(false)
            .flexible(true)
            .from_path(&manifest_path)?;

        // Due to "flexible" not actually working when doing serde-backed deserialization (took some time to figure this out)
        // the deserialization has to be done manually.
        let mut entries = vec![];
        for record in reader.records() {
            let record = record?;

            // We only know these manifest formats.
            if record.len() != 2 && record.len() != 3 {
                return Err(ErrorKind::ManifestError.into());
            } else {
                let mut manifest_entry = ManifestEntry {
                    relative_path: record.get(0).ok_or_else(|| Error::from(ErrorKind::ManifestError))?.to_owned(),
                    size: record.get(1).ok_or_else(|| Error::from(ErrorKind::ManifestError))?.parse()?,
                    ..Default::default()
                };

                // In newer games, a third field has been added.
                if record.len() == 3 {
                    manifest_entry.belongs_to_base_game = record.get(2).ok_or_else(|| Error::from(ErrorKind::ManifestError))?.parse().ok();
                }
                else {
                    manifest_entry.belongs_to_base_game = None;
                }

                entries.push(manifest_entry);
            }
        }

        let manifest = Self(entries);
        Ok(manifest)
    }

    pub fn is_path_in_manifest(&self, path: &Path) -> bool {
        let insensitivized_path = path.to_str().unwrap().to_lowercase().replace("\\", "/");
        self.0.iter().any(|x| insensitivized_path.ends_with(&x.relative_path.to_lowercase()))
    }
}

/// Default implementation for PackFileSettings.
impl Default for PackFileSettings {

    fn default() -> Self {
        let mut settings_text = BTreeMap::new();
        let settings_string = BTreeMap::new();
        let mut settings_bool = BTreeMap::new();
        let settings_number = BTreeMap::new();

        settings_text.insert("diagnostics_files_to_ignore".to_owned(), "".to_owned());
        settings_text.insert("import_files_to_ignore".to_owned(), "".to_owned());
        settings_bool.insert("disable_autosaves".to_owned(), false);

        Self {
            settings_text,
            settings_string,
            settings_bool,
            settings_number,
        }
    }
}

/// Implementation of PackFileSettings.
impl PackFileSettings {

    /// This function tries to load the settings from the current PackFile and return them.
    pub fn load(data: &[u8]) -> Result<Self> {
        let mut settings: Self = from_slice(data)?;

        // Add/Remove settings missing/no-longer-needed for keeping it update friendly. First, remove the outdated ones, then add the new ones.
        let defaults = Self::default();
        {
            let mut keys_to_delete = vec![];
            for (key, _) in settings.settings_text.clone() { if defaults.settings_text.get(&*key).is_none() { keys_to_delete.push(key); } }
            for key in &keys_to_delete { settings.settings_text.remove(key); }

            let mut keys_to_delete = vec![];
            for (key, _) in settings.settings_string.clone() { if defaults.settings_string.get(&*key).is_none() { keys_to_delete.push(key); } }
            for key in &keys_to_delete { settings.settings_string.remove(key); }

            let mut keys_to_delete = vec![];
            for (key, _) in settings.settings_bool.clone() { if defaults.settings_bool.get(&*key).is_none() { keys_to_delete.push(key); } }
            for key in &keys_to_delete { settings.settings_bool.remove(key); }

            let mut keys_to_delete = vec![];
            for (key, _) in settings.settings_number.clone() { if defaults.settings_number.get(&*key).is_none() { keys_to_delete.push(key); } }
            for key in &keys_to_delete { settings.settings_number.remove(key); }
        }

        {
            for (key, value) in defaults.settings_text { if settings.settings_text.get(&*key).is_none() { settings.settings_text.insert(key, value);  } }
            for (key, value) in defaults.settings_string { if settings.settings_string.get(&*key).is_none() { settings.settings_string.insert(key, value);  } }
            for (key, value) in defaults.settings_bool { if settings.settings_bool.get(&*key).is_none() { settings.settings_bool.insert(key, value);  } }
            for (key, value) in defaults.settings_number { if settings.settings_number.get(&*key).is_none() { settings.settings_number.insert(key, value);  } }
        }

        Ok(settings)
    }

    pub fn get_diagnostics_files_to_ignore(&self) -> Option<Vec<(Vec<String>, Vec<String>, Vec<String>)>> {
        self.settings_text.get("diagnostics_files_to_ignore").map(|files_to_ignore| {
            let files = files_to_ignore.split('\n').collect::<Vec<&str>>();

            // Ignore commented out rows.
            files.iter().filter_map(|x| {
                if !x.starts_with('#') {
                    let path = x.splitn(3, ';').collect::<Vec<&str>>();
                    if path.len() == 3 {
                        Some((path[0].split('/').filter_map(|y| if !y.is_empty() { Some(y.to_owned()) } else { None }).collect::<Vec<String>>(), path[1].split(',').filter_map(|y| if !y.is_empty() { Some(y.to_owned()) } else { None }).collect::<Vec<String>>(), path[2].split(',').filter_map(|y| if !y.is_empty() { Some(y.to_owned()) } else { None }).collect::<Vec<String>>()))
                    } else if path.len() == 2 {
                        Some((path[0].split('/').filter_map(|y| if !y.is_empty() { Some(y.to_owned()) } else { None }).collect::<Vec<String>>(), path[1].split(',').filter_map(|y| if !y.is_empty() { Some(y.to_owned()) } else { None }).collect::<Vec<String>>(), vec![]))
                    } else if path.len() == 1 {
                        Some((path[0].split('/').filter_map(|y| if !y.is_empty() { Some(y.to_owned()) } else { None }).collect::<Vec<String>>(), vec![], vec![]))
                    } else {
                        None
                    }
                } else {
                    None
                }
            }).collect::<Vec<(Vec<String>, Vec<String>, Vec<String>)>>()
        })
    }
}
*/

/// Implementation of trait `Default` for `PFHFlags`.
impl Default for PFHFlags {
    fn default() -> Self {
        Self::empty()
    }
}
